# file name: check_neo4j_connection_fixed.py
import asyncio
import os
import sys
import json
import time
import math
import re
from datetime import datetime, timedelta
from typing import List, Dict, Any
from dotenv import load_dotenv
# 1
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)
# å…³é”®ï¼šå°† src ç›®å½•ç½®äºæœ€å‰ï¼Œç¡®ä¿ä»å½“å‰ä»“åº“åŠ è½½æ¨¡å—
src_dir = os.path.join(project_root, "src")
if src_dir not in sys.path:
    sys.path.insert(0, src_dir)

load_dotenv()

# é¦–å…ˆå®šä¹‰ _loc_normalize å‡½æ•°ï¼Œå› ä¸ºå…¶ä»–å‡½æ•°ä¾èµ–å®ƒ
def _loc_normalize(text: str) -> str:
    text = str(text) if text is not None else ""
    text = text.lower()
    text = re.sub(r"[\,]", " ", text)
    text = re.sub(r"\b(a|an|the|and)\b", " ", text)
    text = re.sub(r"[^\w\s]", " ", text)
    text = " ".join(text.split())
    return text

# å°è¯•ä» metrics.py å¯¼å…¥åŸºç¡€æŒ‡æ ‡
try:
    from common.metrics import f1_score, bleu1, jaccard
    print("âœ… ä» metrics.py å¯¼å…¥åŸºç¡€æŒ‡æ ‡æˆåŠŸ")
except ImportError as e:
    print(f"âŒ ä» metrics.py å¯¼å…¥å¤±è´¥: {e}")
    # å›é€€åˆ°æœ¬åœ°å®ç°
    def f1_score(pred: str, ref: str) -> float:
        pred_str = str(pred) if pred is not None else ""
        ref_str = str(ref) if ref is not None else ""

        p_tokens = _loc_normalize(pred_str).split()
        r_tokens = _loc_normalize(ref_str).split()
        if not p_tokens and not r_tokens:
            return 1.0
        if not p_tokens or not r_tokens:
            return 0.0
        p_set = set(p_tokens)
        r_set = set(r_tokens)
        tp = len(p_set & r_set)
        precision = tp / len(p_set) if p_set else 0.0
        recall = tp / len(r_set) if r_set else 0.0
        if precision + recall == 0:
            return 0.0
        return 2 * precision * recall / (precision + recall)

    def bleu1(pred: str, ref: str) -> float:
        pred_str = str(pred) if pred is not None else ""
        ref_str = str(ref) if ref is not None else ""

        p_tokens = _loc_normalize(pred_str).split()
        r_tokens = _loc_normalize(ref_str).split()
        if not p_tokens:
            return 0.0

        r_counts = {}
        for t in r_tokens:
            r_counts[t] = r_counts.get(t, 0) + 1

        clipped = 0
        p_counts = {}
        for t in p_tokens:
            p_counts[t] = p_counts.get(t, 0) + 1

        for t, c in p_counts.items():
            clipped += min(c, r_counts.get(t, 0))

        precision = clipped / max(len(p_tokens), 1)
        ref_len = len(r_tokens)
        pred_len = len(p_tokens)

        if pred_len > ref_len or pred_len == 0:
            bp = 1.0
        else:
            bp = math.exp(1 - ref_len / max(pred_len, 1))

        return bp * precision

    def jaccard(pred: str, ref: str) -> float:
        pred_str = str(pred) if pred is not None else ""
        ref_str = str(ref) if ref is not None else ""

        p = set(_loc_normalize(pred_str).split())
        r = set(_loc_normalize(ref_str).split())
        if not p and not r:
            return 1.0
        if not p or not r:
            return 0.0
        return len(p & r) / len(p | r)

# å°è¯•ä» qwen_search_eval.py å¯¼å…¥ LoCoMo ç‰¹å®šæŒ‡æ ‡
try:
    # æ·»åŠ  evaluation ç›®å½•è·¯å¾„
    evaluation_dir = os.path.join(project_root, "evaluation")
    if evaluation_dir not in sys.path:
        sys.path.insert(0, evaluation_dir)

    # å°è¯•ä»ä¸åŒä½ç½®å¯¼å…¥
    try:
        from locomo.qwen_search_eval import loc_f1_score, loc_multi_f1, _resolve_relative_times
        print("âœ… ä» locomo.qwen_search_eval å¯¼å…¥ LoCoMo ç‰¹å®šæŒ‡æ ‡æˆåŠŸ")
    except ImportError:
        from qwen_search_eval import loc_f1_score, loc_multi_f1, _resolve_relative_times
        print("âœ… ä» qwen_search_eval å¯¼å…¥ LoCoMo ç‰¹å®šæŒ‡æ ‡æˆåŠŸ")

except ImportError as e:
    print(f"âŒ ä» qwen_search_eval.py å¯¼å…¥å¤±è´¥: {e}")
    # å›é€€åˆ°æœ¬åœ°å®ç° LoCoMo ç‰¹å®šå‡½æ•°
    def _resolve_relative_times(text: str, anchor: datetime) -> str:
        t = str(text) if text is not None else ""
        t = re.sub(r"\btoday\b", anchor.date().isoformat(), t, flags=re.IGNORECASE)
        t = re.sub(r"\byesterday\b", (anchor - timedelta(days=1)).date().isoformat(), t, flags=re.IGNORECASE)
        t = re.sub(r"\btomorrow\b", (anchor + timedelta(days=1)).date().isoformat(), t, flags=re.IGNORECASE)

        def _ago_repl(m: re.Match[str]) -> str:
            n = int(m.group(1))
            return (anchor - timedelta(days=n)).date().isoformat()
        def _in_repl(m: re.Match[str]) -> str:
            n = int(m.group(1))
            return (anchor + timedelta(days=n)).date().isoformat()

        t = re.sub(r"\b(\d+)\s+days\s+ago\b", _ago_repl, t, flags=re.IGNORECASE)
        t = re.sub(r"\bin\s+(\d+)\s+days\b", _in_repl, t, flags=re.IGNORECASE)
        t = re.sub(r"\blast\s+week\b", (anchor - timedelta(days=7)).date().isoformat(), t, flags=re.IGNORECASE)
        t = re.sub(r"\bnext\s+week\b", (anchor + timedelta(days=7)).date().isoformat(), t, flags=re.IGNORECASE)
        return t

    def loc_f1_score(prediction: str, ground_truth: str) -> float:
        p_tokens = _loc_normalize(prediction).split()
        g_tokens = _loc_normalize(ground_truth).split()
        if not p_tokens or not g_tokens:
            return 0.0
        p = set(p_tokens)
        g = set(g_tokens)
        tp = len(p & g)
        precision = tp / len(p) if p else 0.0
        recall = tp / len(g) if g else 0.0
        return (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0

    def loc_multi_f1(prediction: str, ground_truth: str) -> float:
        predictions = [p.strip() for p in str(prediction).split(',') if p.strip()]
        ground_truths = [g.strip() for g in str(ground_truth).split(',') if g.strip()]
        if not predictions or not ground_truths:
            return 0.0
        def _f1(a: str, b: str) -> float:
            return loc_f1_score(a, b)
        vals = []
        for gt in ground_truths:
            vals.append(max(_f1(pred, gt) for pred in predictions))
        return sum(vals) / len(vals)


def smart_context_selection(contexts: List[str], question: str, max_chars: int = 8000) -> str:
    """åŸºäºé—®é¢˜å…³é”®è¯æ™ºèƒ½é€‰æ‹©ä¸Šä¸‹æ–‡"""
    if not contexts:
        return ""

    # æå–é—®é¢˜å…³é”®è¯ï¼ˆåªä¿ç•™æœ‰æ„ä¹‰çš„è¯ï¼‰
    question_lower = question.lower()
    stop_words = {'what', 'when', 'where', 'who', 'why', 'how', 'did', 'do', 'does', 'is', 'are', 'was', 'were', 'the', 'a', 'an', 'and', 'or', 'but'}
    question_words = set(re.findall(r'\b\w+\b', question_lower))
    question_words = {word for word in question_words if word not in stop_words and len(word) > 2}

    print(f"ğŸ” é—®é¢˜å…³é”®è¯: {question_words}")

    # ç»™æ¯ä¸ªä¸Šä¸‹æ–‡æ‰“åˆ†
    scored_contexts = []
    for i, context in enumerate(contexts):
        context_lower = context.lower()
        score = 0

        # å…³é”®è¯åŒ¹é…å¾—åˆ†
        keyword_matches = 0
        for word in question_words:
            if word in context_lower:
                keyword_matches += 1
                # å…³é”®è¯å‡ºç°æ¬¡æ•°è¶Šå¤šï¼Œå¾—åˆ†è¶Šé«˜
                score += context_lower.count(word) * 2

        # ä¸Šä¸‹æ–‡é•¿åº¦å¾—åˆ†ï¼ˆé€‚ä¸­çš„é•¿åº¦æ›´å¥½ï¼‰
        context_len = len(context)
        if 100 < context_len < 2000:  # ç†æƒ³é•¿åº¦èŒƒå›´
            score += 5
        elif context_len >= 2000:  # å¤ªé•¿å¯èƒ½åŒ…å«æ— å…³ä¿¡æ¯
            score += 2

        # å¦‚æœæ˜¯å‰å‡ ä¸ªä¸Šä¸‹æ–‡ï¼Œç»™äºˆé¢å¤–åˆ†æ•°ï¼ˆé€šå¸¸ç›¸å…³æ€§æ›´é«˜ï¼‰
        if i < 3:
            score += 3

        scored_contexts.append((score, context, keyword_matches))

    # æŒ‰å¾—åˆ†æ’åº
    scored_contexts.sort(key=lambda x: x[0], reverse=True)

    # é€‰æ‹©é«˜å¾—åˆ†çš„ä¸Šä¸‹æ–‡ï¼Œç›´åˆ°è¾¾åˆ°å­—ç¬¦é™åˆ¶
    selected = []
    total_chars = 0
    selected_count = 0

    print("ğŸ“Š ä¸Šä¸‹æ–‡ç›¸å…³æ€§åˆ†æ:")
    for score, context, matches in scored_contexts[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
        print(f"  - å¾—åˆ†: {score}, å…³é”®è¯åŒ¹é…: {matches}, é•¿åº¦: {len(context)}")

    for score, context, matches in scored_contexts:
        if total_chars + len(context) <= max_chars:
            selected.append(context)
            total_chars += len(context)
            selected_count += 1
        else:
            # å¦‚æœè¿™ä¸ªä¸Šä¸‹æ–‡å¾—åˆ†å¾ˆé«˜ä½†æ”¾ä¸ä¸‹ï¼Œå°è¯•æˆªå–
            if score > 10 and total_chars < max_chars - 500:
                remaining = max_chars - total_chars
                # æ‰¾åˆ°åŒ…å«å…³é”®è¯çš„éƒ¨åˆ†
                lines = context.split('\n')
                relevant_lines = []
                current_chars = 0

                for line in lines:
                    line_lower = line.lower()
                    line_relevance = any(word in line_lower for word in question_words)

                    if line_relevance and current_chars < remaining - 100:
                        relevant_lines.append(line)
                        current_chars += len(line)

                if relevant_lines:
                    truncated = '\n'.join(relevant_lines)
                    if len(truncated) > 100:  # ç¡®ä¿æœ‰è¶³å¤Ÿå†…å®¹
                        selected.append(truncated + "\n[ç›¸å…³å†…å®¹æˆªæ–­...]")
                        total_chars += len(truncated)
                        selected_count += 1
            break  # ä¸å†å°è¯•æ·»åŠ æ›´å¤šä¸Šä¸‹æ–‡

    result = "\n\n".join(selected)
    print(f"âœ… æ™ºèƒ½é€‰æ‹©: {selected_count}ä¸ªä¸Šä¸‹æ–‡, æ€»é•¿åº¦: {total_chars}å­—ç¬¦")
    return result


def get_dynamic_search_params(question: str, question_index: int, total_questions: int):
    """æ ¹æ®é—®é¢˜å¤æ‚åº¦å’Œè¿›åº¦åŠ¨æ€è°ƒæ•´æ£€ç´¢å‚æ•°"""

    # åˆ†æé—®é¢˜å¤æ‚åº¦
    word_count = len(question.split())
    has_temporal = any(word in question.lower() for word in ['when', 'date', 'time', 'ago'])
    has_multi_hop = any(word in question.lower() for word in ['and', 'both', 'also', 'while'])

    # æ ¹æ®è¿›åº¦è°ƒæ•´ - åæœŸé—®é¢˜å¯èƒ½éœ€è¦æ›´ç²¾ç¡®çš„æ£€ç´¢
    progress_factor = question_index / total_questions

    base_limit = 12
    if has_temporal and has_multi_hop:
        base_limit = 20
    elif word_count > 8:
        base_limit = 16

    # éšç€æµ‹è¯•è¿›è¡Œï¼Œé€æ¸æ”¶ç´§æ£€ç´¢èŒƒå›´
    adjusted_limit = max(8, int(base_limit * (1 - progress_factor * 0.3)))

    # åŠ¨æ€è°ƒæ•´æœ€å¤§å­—ç¬¦æ•°
    max_chars = 8000 + 4000 * (1 - progress_factor)

    return {
        "limit": adjusted_limit,
        "max_chars": int(max_chars)
    }


class EnhancedEvaluationMonitor:
    def __init__(self, reset_interval=5, performance_threshold=0.6):
        self.question_count = 0
        self.reset_interval = reset_interval
        self.performance_threshold = performance_threshold
        self.consecutive_low_scores = 0
        self.performance_history = []
        self.recent_f1_scores = []

    def should_reset_connections(self, current_f1=None):
        """åŸºäºè®¡æ•°å’Œæ€§èƒ½åŒé‡åˆ¤æ–­"""
        # å®šæœŸé‡ç½®
        if self.question_count % self.reset_interval == 0:
            return True

        # æ€§èƒ½é©±åŠ¨çš„é‡ç½®
        if current_f1 is not None and current_f1 < self.performance_threshold:
            self.consecutive_low_scores += 1
            if self.consecutive_low_scores >= 2:  # è¿ç»­2ä¸ªä½åˆ†å°±é‡ç½®
                print("ğŸš¨ è¿ç»­ä½åˆ†ï¼Œè§¦å‘ç´§æ€¥é‡ç½®")
                self.consecutive_low_scores = 0
                return True
        else:
            self.consecutive_low_scores = 0

        return False

    def record_performance(self, question_index, metrics, context_length, retrieved_docs):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡ï¼Œæ£€æµ‹è¡°å‡"""
        self.performance_history.append({
            'index': question_index,
            'metrics': metrics,
            'context_length': context_length,
            'retrieved_docs': retrieved_docs,
            'timestamp': time.time()
        })

        # è®°å½•æœ€è¿‘çš„F1åˆ†æ•°
        self.recent_f1_scores.append(metrics['f1'])
        if len(self.recent_f1_scores) > 5:
            self.recent_f1_scores.pop(0)

    def get_recent_performance(self):
        """è·å–è¿‘æœŸå¹³å‡æ€§èƒ½"""
        if not self.recent_f1_scores:
            return 0.5
        return sum(self.recent_f1_scores) / len(self.recent_f1_scores)

    def get_performance_trend(self):
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        if len(self.performance_history) < 2:
            return "stable"

        recent_metrics = [item['metrics']['f1'] for item in self.performance_history[-5:]]
        earlier_metrics = [item['metrics']['f1'] for item in self.performance_history[-10:-5]]

        if len(recent_metrics) < 2 or len(earlier_metrics) < 2:
            return "stable"

        recent_avg = sum(recent_metrics) / len(recent_metrics)
        earlier_avg = sum(earlier_metrics) / len(earlier_metrics)

        if recent_avg < earlier_avg * 0.8:
            return "degrading"
        elif recent_avg > earlier_avg * 1.1:
            return "improving"
        else:
            return "stable"


def get_enhanced_search_params(question: str, question_index: int, total_questions: int, recent_performance: float):
    """åŸºäºé—®é¢˜å¤æ‚åº¦å’Œè¿‘æœŸæ€§èƒ½åŠ¨æ€è°ƒæ•´æ£€ç´¢å‚æ•°"""

    # åŸºç¡€å‚æ•°
    base_params = get_dynamic_search_params(question, question_index, total_questions)

    # æ€§èƒ½è‡ªé€‚åº”è°ƒæ•´
    if recent_performance < 0.5:  # è¿‘æœŸè¡¨ç°å·®
        # å¢åŠ æ£€ç´¢èŒƒå›´ï¼Œå°è¯•è·å–æ›´å¤šä¸Šä¸‹æ–‡
        base_params["limit"] = min(base_params["limit"] + 5, 25)
        base_params["max_chars"] = min(base_params["max_chars"] + 2000, 12000)
        print(f"ğŸ“ˆ æ€§èƒ½è‡ªé€‚åº”ï¼šå¢åŠ æ£€ç´¢èŒƒå›´ (limit={base_params['limit']}, max_chars={base_params['max_chars']})")

    elif recent_performance > 0.8:  # è¿‘æœŸè¡¨ç°å¥½
        # æ”¶ç´§æ£€ç´¢ï¼Œæé«˜ç²¾åº¦
        base_params["limit"] = max(base_params["limit"] - 2, 8)
        base_params["max_chars"] = max(base_params["max_chars"] - 1000, 6000)
        print(f"ğŸ¯ æ€§èƒ½è‡ªé€‚åº”ï¼šæé«˜æ£€ç´¢ç²¾åº¦ (limit={base_params['limit']}, max_chars={base_params['max_chars']})")

    # ä¸­é—´é˜¶æ®µç‰¹æ®Šå¤„ç†
    mid_sequence_factor = abs(question_index / total_questions - 0.5)
    if mid_sequence_factor < 0.2:  # åœ¨ä¸­é—´30%çš„é—®é¢˜
        print("ğŸ¯ ä¸­é—´é˜¶æ®µï¼šä½¿ç”¨æ›´ç²¾ç¡®çš„æ£€ç´¢ç­–ç•¥")
        base_params["limit"] = max(base_params["limit"] - 2, 10)  # å‡å°‘æ•°é‡ï¼Œæé«˜è´¨é‡
        base_params["max_chars"] = max(base_params["max_chars"] - 1000, 7000)

    return base_params


def enhanced_context_selection(contexts: List[str], question: str, question_index: int, total_questions: int, max_chars: int = 8000) -> str:
    """è€ƒè™‘é—®é¢˜åºåˆ—ä½ç½®çš„æ™ºèƒ½é€‰æ‹©"""

    if not contexts:
        return ""

    # åœ¨åºåˆ—ä¸­é—´é˜¶æ®µä½¿ç”¨æ›´ä¸¥æ ¼çš„ç­›é€‰
    mid_sequence_factor = abs(question_index / total_questions - 0.5)  # è·ç¦»ä¸­å¿ƒçš„è·ç¦»

    if mid_sequence_factor < 0.2:  # åœ¨ä¸­é—´30%çš„é—®é¢˜
        print("ğŸ¯ ä¸­é—´é˜¶æ®µï¼šä½¿ç”¨ä¸¥æ ¼ä¸Šä¸‹æ–‡ç­›é€‰")

        # æå–é—®é¢˜å…³é”®è¯
        question_lower = question.lower()
        stop_words = {'what', 'when', 'where', 'who', 'why', 'how', 'did', 'do', 'does', 'is', 'are', 'was', 'were', 'the', 'a', 'an', 'and', 'or', 'but'}
        question_words = set(re.findall(r'\b\w+\b', question_lower))
        question_words = {word for word in question_words if word not in stop_words and len(word) > 2}

        # åªä¿ç•™é«˜åº¦ç›¸å…³çš„ä¸Šä¸‹æ–‡
        filtered_contexts = []
        for context in contexts:
            context_lower = context.lower()
            relevance_score = sum(3 if word in context_lower else 0 for word in question_words)

            # é¢å¤–åŠ åˆ†ç»™åŒ…å«æ•°å­—ã€æ—¥æœŸçš„ä¸Šä¸‹æ–‡ï¼ˆå¯¹äº‹å®æ€§é—®é¢˜æ›´é‡è¦ï¼‰
            if any(char.isdigit() for char in context):
                relevance_score += 2

            # æé«˜é˜ˆå€¼ï¼šåªæœ‰å¾—åˆ†>=3çš„ä¸Šä¸‹æ–‡æ‰ä¿ç•™
            if relevance_score >= 3:
                filtered_contexts.append(context)
            else:
                print(f"  - è¿‡æ»¤ä½åˆ†ä¸Šä¸‹æ–‡: å¾—åˆ†={relevance_score}")

        contexts = filtered_contexts
        print(f"ğŸ” ä¸¥æ ¼ç­›é€‰åä¿ç•™ {len(contexts)} ä¸ªä¸Šä¸‹æ–‡")

    # ä½¿ç”¨åŸæœ‰çš„æ™ºèƒ½é€‰æ‹©é€»è¾‘
    return smart_context_selection(contexts, question, max_chars)


async def run_enhanced_evaluation():
    """ä½¿ç”¨å¢å¼ºæ–¹æ³•è¿›è¡Œå®Œæ•´è¯„ä¼° - è§£å†³ä¸­é—´æ€§èƒ½è¡°å‡é—®é¢˜"""
    try:
        from dotenv import load_dotenv
    except Exception:
        def load_dotenv():
            return None
     
    # ä¿®æ­£å¯¼å…¥è·¯å¾„ï¼šä½¿ç”¨ app.core.memory.src å‰ç¼€
    from app.repositories.neo4j.neo4j_connector import Neo4jConnector
    from app.repositories.neo4j.graph_search import search_graph_by_embedding
    from app.core.memory.llm_tools.openai_embedder import OpenAIEmbedderClient
    from app.core.models.base import RedBearModelConfig
    from app.core.memory.utils.llm.llm_utils import get_llm_client
    from app.core.memory.utils.config.config_utils import get_embedder_config
    from app.core.memory.utils.config.definitions import SELECTED_LLM_ID, SELECTED_EMBEDDING_ID

    # åŠ è½½æ•°æ®
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    current_file = os.path.abspath(__file__)
    evaluation_dir = os.path.dirname(os.path.dirname(current_file))  # evaluationç›®å½•
    memory_dir = os.path.dirname(evaluation_dir)  # memoryç›®å½•
    data_path = os.path.join(memory_dir, "data", "locomo10.json")
    with open(data_path, "r", encoding="utf-8") as f:
        raw = json.load(f)

    qa_items = []
    if isinstance(raw, list):
        for entry in raw:
            qa_items.extend(entry.get("qa", []))
    else:
        qa_items.extend(raw.get("qa", []))
    
    items = qa_items[:20]  # æµ‹è¯•å¤šå°‘ä¸ªé—®é¢˜
    
    # åˆå§‹åŒ–å¢å¼ºç›‘æ§å™¨
    monitor = EnhancedEvaluationMonitor(reset_interval=5, performance_threshold=0.6)
    
    llm = get_llm_client(SELECTED_LLM_ID)
    
    # åˆå§‹åŒ–embedder
    cfg_dict = get_embedder_config(SELECTED_EMBEDDING_ID)
    embedder = OpenAIEmbedderClient(
        model_config=RedBearModelConfig.model_validate(cfg_dict)
    )
    
    # åˆå§‹åŒ–è¿æ¥å™¨
    connector = Neo4jConnector()

    # åˆå§‹åŒ–ç»“æœå­—å…¸
    results = {
        "questions": [],
        "overall_metrics": {"f1": 0.0, "b1": 0.0, "j": 0.0, "loc_f1": 0.0},
        "category_metrics": {},
        "retrieval_stats": {"total_questions": len(items), "avg_context_length": 0, "avg_retrieved_docs": 0},
        "performance_trend": "stable",
        "timestamp": datetime.now().isoformat(),
        "enhanced_strategy": True
    }

    total_f1 = 0.0
    total_bleu1 = 0.0
    total_jaccard = 0.0
    total_loc_f1 = 0.0
    total_context_length = 0
    total_retrieved_docs = 0
    category_stats = {}

    try:
        for i, item in enumerate(items):
            monitor.question_count += 1

            # è·å–è¿‘æœŸæ€§èƒ½ç”¨äºé‡ç½®åˆ¤æ–­
            recent_performance = monitor.get_recent_performance()

            # å¢å¼ºçš„é‡ç½®åˆ¤æ–­
            should_reset = monitor.should_reset_connections(current_f1=recent_performance)
            if should_reset and i > 0:
                print(f"ğŸ”„ é‡ç½®Neo4jè¿æ¥ (é—®é¢˜ {i+1}/{len(items)}, è¿‘æœŸæ€§èƒ½: {recent_performance:.3f})...")
                await connector.close()
                connector = Neo4jConnector()  # åˆ›å»ºæ–°è¿æ¥
                print("âœ… è¿æ¥é‡ç½®å®Œæˆ")

            q = item.get("question", "")
            ref = item.get("answer", "")
            ref_str = str(ref) if ref is not None else ""

            print(f"\nğŸ” [{i+1}/{len(items)}] é—®é¢˜: {q}")
            print(f"âœ… çœŸå®ç­”æ¡ˆ: {ref_str}")

            # åˆ†ç±»åˆ«ç»Ÿè®¡
            category = "Unknown"
            if item.get("category") == 1:
                category = "Multi-Hop"
            elif item.get("category") == 2:
                category = "Temporal"
            elif item.get("category") == 3:
                category = "Open Domain"
            elif item.get("category") == 4:
                category = "Single-Hop"

            # å¢å¼ºçš„æ£€ç´¢å‚æ•°
            search_params = get_enhanced_search_params(q, i, len(items), recent_performance)
            search_limit = search_params["limit"]
            max_chars = search_params["max_chars"]

            print(f"ğŸ·ï¸ ç±»åˆ«: {category}, æ£€ç´¢å‚æ•°: limit={search_limit}, max_chars={max_chars}")
            
            # ä½¿ç”¨é¡¹ç›®æ ‡å‡†çš„æ··åˆæ£€ç´¢æ–¹æ³•
            t0 = time.time()
            contexts_all = []

            try:
                # ä½¿ç”¨ç»Ÿä¸€çš„æœç´¢æœåŠ¡
                from app.core.memory.storage_services.search import run_hybrid_search
                
                print("ğŸ”€ ä½¿ç”¨æ··åˆæœç´¢æœåŠ¡...")
                
                search_results = await run_hybrid_search(
                    query_text=q,
                    search_type="hybrid",
                    group_id="locomo_sk",
                    limit=20,
                    include=["statements", "chunks", "entities", "summaries"],
                    alpha=0.6,  # BM25æƒé‡
                    embedding_id=SELECTED_EMBEDDING_ID
                )
                
                # å¤„ç†æœç´¢ç»“æœ - æ–°çš„æœç´¢æœåŠ¡è¿”å›ç»Ÿä¸€çš„ç»“æ„
                chunks = search_results.get("chunks", [])
                statements = search_results.get("statements", [])
                entities = search_results.get("entities", [])
                summaries = search_results.get("summaries", [])
                
                print(f"âœ… æ··åˆæ£€ç´¢æˆåŠŸ: {len(chunks)} chunks, {len(statements)} æ¡é™ˆè¿°, {len(entities)} ä¸ªå®ä½“, {len(summaries)} ä¸ªæ‘˜è¦")

                # æ„å»ºä¸Šä¸‹æ–‡ï¼šä¼˜å…ˆä½¿ç”¨ chunksã€statements å’Œ summaries
                for c in chunks:
                    content = str(c.get("content", "")).strip()
                    if content:
                        contexts_all.append(content)

                for s in statements:
                    stmt_text = str(s.get("statement", "")).strip()
                    if stmt_text:
                        contexts_all.append(stmt_text)
                
                for sm in summaries:
                    summary_text = str(sm.get("summary", "")).strip()
                    if summary_text:
                        contexts_all.append(summary_text)

                # å®ä½“æ‘˜è¦ï¼šæœ€å¤šåŠ å…¥å‰3ä¸ªé«˜åˆ†å®ä½“ï¼Œé¿å…å™ªå£°
                scored = [e for e in entities if e.get("score") is not None]
                top_entities = sorted(scored, key=lambda x: x.get("score", 0), reverse=True)[:3] if scored else entities[:3]
                if top_entities:
                    summary_lines = []
                    for e in top_entities:
                        name = str(e.get("name", "")).strip()
                        etype = str(e.get("entity_type", "")).strip()
                        score = e.get("score")
                        if name:
                            meta = []
                            if etype:
                                meta.append(f"type={etype}")
                            if isinstance(score, (int, float)):
                                meta.append(f"score={score:.3f}")
                            summary_lines.append(f"EntitySummary: {name}{(' [' + ' '.join(meta) + ']') if meta else ''}")
                    if summary_lines:
                        contexts_all.append("\n".join(summary_lines))

                print(f"ğŸ“Š æœ‰æ•ˆä¸Šä¸‹æ–‡æ•°é‡: {len(contexts_all)}")
            except Exception as e:
                print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
                contexts_all = []

            t1 = time.time()
            search_time = (t1 - t0) * 1000

            # å¢å¼ºçš„ä¸Šä¸‹æ–‡é€‰æ‹©
            context_text = ""
            if contexts_all:
                # ä½¿ç”¨å¢å¼ºçš„ä¸Šä¸‹æ–‡é€‰æ‹©
                context_text = enhanced_context_selection(contexts_all, q, i, len(items), max_chars=max_chars)

                # å¦‚æœæ™ºèƒ½é€‰æ‹©åä»ç„¶è¿‡é•¿ï¼Œè¿›è¡Œæœ€ç»ˆä¿æŠ¤æ€§æˆªæ–­
                if len(context_text) > max_chars:
                    print(f"âš ï¸ æ™ºèƒ½é€‰æ‹©åä»ç„¶è¿‡é•¿ ({len(context_text)}å­—ç¬¦)ï¼Œè¿›è¡Œæœ€ç»ˆæˆªæ–­")
                    context_text = context_text[:max_chars] + "\n\n[æœ€ç»ˆæˆªæ–­...]"

                # æ—¶é—´è§£æ
                anchor_date = datetime(2023, 5, 8)  # ä½¿ç”¨å›ºå®šæ—¥æœŸç¡®ä¿ä¸€è‡´æ€§
                context_text = _resolve_relative_times(context_text, anchor_date)

                context_text = f"Reference date: {anchor_date.date().isoformat()}\n\n" + context_text

                print(f"ğŸ“ æœ€ç»ˆä¸Šä¸‹æ–‡é•¿åº¦: {len(context_text)} å­—ç¬¦")

                # æ˜¾ç¤ºä¸åŒä¸Šä¸‹æ–‡çš„é¢„è§ˆï¼ˆä¸åªæ˜¯ç¬¬ä¸€æ¡ï¼‰
                print("ğŸ” ä¸Šä¸‹æ–‡é¢„è§ˆ:")
                for j, context in enumerate(contexts_all[:3]):  # æ˜¾ç¤ºå‰3ä¸ªä¸Šä¸‹æ–‡
                    preview = context[:150].replace('\n', ' ')
                    print(f"  ä¸Šä¸‹æ–‡{j+1}: {preview}...")
                
                # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­
                if ref_str and ref_str.strip():
                    answer_found = any(ref_str.lower() in ctx.lower() for ctx in contexts_all)
                    print(f"ğŸ” è°ƒè¯•ï¼šç­”æ¡ˆ '{ref_str}' æ˜¯å¦åœ¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸­ï¼Ÿ {'âœ… æ˜¯' if answer_found else 'âŒ å¦'}")
                
            else:
                print("âŒ æ²¡æœ‰æ£€ç´¢åˆ°æœ‰æ•ˆä¸Šä¸‹æ–‡")
                context_text = "No relevant context found."

            # LLM å›ç­”
            messages = [
                {"role": "system", "content": (
                    "You are a precise QA assistant. Answer following these rules:\n"
                    "1) Extract the EXACT information mentioned in the context\n"
                    "2) For time questions: calculate actual dates from relative times\n"
                    "3) Return ONLY the answer text in simplest form\n"
                    "4) For dates, use format 'DD Month YYYY' (e.g., '7 May 2023')\n"
                    "5) If no clear answer found, respond with 'Unknown'"
                )},
                {"role": "user", "content": f"Question: {q}\n\nContext:\n{context_text}"},
            ]

            t2 = time.time()
            try:
                # ä½¿ç”¨å¼‚æ­¥è°ƒç”¨
                resp = await llm.chat(messages=messages)
                # å…¼å®¹ä¸åŒçš„å“åº”æ ¼å¼
                pred = resp.content.strip() if hasattr(resp, 'content') else (resp["choices"][0]["message"]["content"].strip() if isinstance(resp, dict) else "Unknown")
            except Exception as e:
                print(f"âŒ LLM ç”Ÿæˆå¤±è´¥: {e}")
                pred = "Unknown"
            t3 = time.time()
            llm_time = (t3 - t2) * 1000

            # è®¡ç®—æŒ‡æ ‡ - ä½¿ç”¨å¯¼å…¥çš„æŒ‡æ ‡å‡½æ•°
            f1_val = f1_score(pred, ref_str)
            bleu1_val = bleu1(pred, ref_str)
            jaccard_val = jaccard(pred, ref_str)
            loc_f1_val = loc_f1_score(pred, ref_str)

            print(f"ğŸ¤– LLM å›ç­”: {pred}")
            print(f"ğŸ“ˆ æŒ‡æ ‡ - F1: {f1_val:.3f}, BLEU-1: {bleu1_val:.3f}, Jaccard: {jaccard_val:.3f}, LoCoMo F1: {loc_f1_val:.3f}")
            print(f"â±ï¸ æ—¶é—´ - æ£€ç´¢: {search_time:.1f}ms, LLM: {llm_time:.1f}ms")

            # æ›´æ–°ç»Ÿè®¡
            total_f1 += f1_val
            total_bleu1 += bleu1_val
            total_jaccard += jaccard_val
            total_loc_f1 += loc_f1_val
            total_context_length += len(context_text)
            total_retrieved_docs += len(contexts_all)

            if category not in category_stats:
                category_stats[category] = {"count": 0, "f1_sum": 0.0, "b1_sum": 0.0, "j_sum": 0.0, "loc_f1_sum": 0.0}

            category_stats[category]["count"] += 1
            category_stats[category]["f1_sum"] += f1_val
            category_stats[category]["b1_sum"] += bleu1_val
            category_stats[category]["j_sum"] += jaccard_val
            category_stats[category]["loc_f1_sum"] += loc_f1_val

            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            metrics = {"f1": f1_val, "bleu1": bleu1_val, "jaccard": jaccard_val, "loc_f1": loc_f1_val}
            monitor.record_performance(i, metrics, len(context_text), len(contexts_all))

            # ä¿å­˜ç»“æœ
            question_result = {
                "question": q,
                "ground_truth": ref_str,
                "prediction": pred,
                "category": category,
                "metrics": metrics,
                "retrieval": {
                    "retrieved_documents": len(contexts_all),
                    "context_length": len(context_text),
                    "search_limit": search_limit,
                    "max_chars": max_chars,
                    "recent_performance": recent_performance
                },
                "timing": {
                    "search_ms": search_time,
                    "llm_ms": llm_time
                }
            }

            results["questions"].append(question_result)

            print("="*60)

    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        # å³ä½¿å‡ºé”™ï¼Œä¹Ÿè¿”å›å·²æœ‰çš„ç»“æœ
        import traceback
        traceback.print_exc()

    finally:
        await connector.close()

    # è®¡ç®—æ€»ä½“æŒ‡æ ‡
    n = len(items)
    if n > 0:
        results["overall_metrics"] = {
            "f1": total_f1 / n,
            "b1": total_bleu1 / n,
            "j": total_jaccard / n,
            "loc_f1": total_loc_f1 / n
        }

        for category, stats in category_stats.items():
            count = stats["count"]
            results["category_metrics"][category] = {
                "count": count,
                "f1": stats["f1_sum"] / count,
                "bleu1": stats["b1_sum"] / count,
                "jaccard": stats["j_sum"] / count,
                "loc_f1": stats["loc_f1_sum"] / count
            }

        results["retrieval_stats"]["avg_context_length"] = total_context_length / n
        results["retrieval_stats"]["avg_retrieved_docs"] = total_retrieved_docs / n

        # åˆ†ææ€§èƒ½è¶‹åŠ¿
        results["performance_trend"] = monitor.get_performance_trend()
        results["reset_interval"] = monitor.reset_interval
        results["total_questions_processed"] = monitor.question_count

    return results


if __name__ == "__main__":
    print("ğŸš€ è¿è¡Œå¢å¼ºç‰ˆå®Œæ•´è¯„ä¼°ï¼ˆè§£å†³ä¸­é—´æ€§èƒ½è¡°å‡é—®é¢˜ï¼‰...")
    print("ğŸ“‹ å¢å¼ºç‰¹æ€§:")
    print("  - åŒé‡é‡ç½®ç­–ç•¥ï¼šå®šæœŸé‡ç½® + æ€§èƒ½é©±åŠ¨é‡ç½®")
    print("  - åŠ¨æ€æ£€ç´¢å‚æ•°ï¼šåŸºäºè¿‘æœŸæ€§èƒ½è‡ªé€‚åº”è°ƒæ•´")
    print("  - ä¸­é—´é˜¶æ®µä¸¥æ ¼ç­›é€‰ï¼šæé«˜ä¸Šä¸‹æ–‡è´¨é‡è¦æ±‚")
    print("  - è¿ç»­æ€§èƒ½ç›‘æ§ï¼šå®æ—¶æ£€æµ‹æ€§èƒ½è¡°å‡")

    result = asyncio.run(run_enhanced_evaluation())

    print("\nğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ:")
    print("æ€»ä½“æŒ‡æ ‡:")
    print(f"  F1: {result['overall_metrics']['f1']:.4f}")
    print(f"  BLEU-1: {result['overall_metrics']['b1']:.4f}")
    print(f"  Jaccard: {result['overall_metrics']['j']:.4f}")
    print(f"  LoCoMo F1: {result['overall_metrics']['loc_f1']:.4f}")

    print("\nåˆ†ç±»åˆ«æŒ‡æ ‡:")
    for category, metrics in result['category_metrics'].items():
        print(f"  {category}: F1={metrics['f1']:.4f}, BLEU-1={metrics['bleu1']:.4f}, Jaccard={metrics['jaccard']:.4f}, LoCoMo F1={metrics['loc_f1']:.4f} (æ ·æœ¬æ•°: {metrics['count']})")

    print("\næ£€ç´¢ç»Ÿè®¡:")
    stats = result['retrieval_stats']
    print(f"  å¹³å‡ä¸Šä¸‹æ–‡é•¿åº¦: {stats['avg_context_length']:.0f} å­—ç¬¦")
    print(f"  å¹³å‡æ£€ç´¢æ–‡æ¡£æ•°: {stats['avg_retrieved_docs']:.1f}")

    print(f"\næ€§èƒ½è¶‹åŠ¿: {result['performance_trend']}")
    print(f"é‡ç½®é—´éš”: æ¯{result['reset_interval']}ä¸ªé—®é¢˜")
    print(f"å¤„ç†é—®é¢˜æ€»æ•°: {result['total_questions_processed']}")
    print(f"å¢å¼ºç­–ç•¥: {'å¯ç”¨' if result.get('enhanced_strategy', False) else 'æœªå¯ç”¨'}")


    # ä¿å­˜ç»“æœåˆ°æŒ‡å®šç›®å½•
    # ä½¿ç”¨ä»£ç æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
    current_file_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(current_file_dir, "results")
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, "enhanced_evaluation_results.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
