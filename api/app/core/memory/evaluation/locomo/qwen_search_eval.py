import argparse
import asyncio
import json
import os
import statistics
import time
from datetime import datetime, timedelta
from typing import Any, Dict, List

try:
    from dotenv import load_dotenv
except Exception:
    def load_dotenv():
        return None

import re

from app.core.memory.evaluation.common.metrics import (
    avg_context_tokens,
    bleu1,
    jaccard,
    latency_stats,
)
from app.core.memory.evaluation.common.metrics import f1_score as common_f1
from app.core.memory.evaluation.extraction_utils import (
    ingest_contexts_via_full_pipeline,
)
from app.core.memory.llm_tools.openai_embedder import OpenAIEmbedderClient
from app.core.memory.storage_services.search import run_hybrid_search
from app.core.memory.utils.config.definitions import (
    PROJECT_ROOT,
    SELECTED_EMBEDDING_ID,
    SELECTED_GROUP_ID,
    SELECTED_LLM_ID,
)
from app.core.memory.utils.llm.llm_utils import MemoryClientFactory
from app.core.models.base import RedBearModelConfig
from app.db import get_db_context
from app.repositories.neo4j.graph_search import search_graph, search_graph_by_embedding
from app.repositories.neo4j.neo4j_connector import Neo4jConnector
from app.services.memory_config_service import MemoryConfigService


# å‚è€ƒ evaluation/locomo/evaluation.py çš„ F1 è®¡ç®—é€»è¾‘ï¼ˆç§»é™¤å¤–éƒ¨ä¾èµ–ï¼Œå†…è”å®ç°ï¼‰
def _loc_normalize(text: str) -> str:
    import re
    # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
    text = str(text) if text is not None else ""
    text = text.lower()
    text = re.sub(r"[\,]", " ", text)  # å»æ‰é€—å·
    text = re.sub(r"\b(a|an|the|and)\b", " ", text)
    text = re.sub(r"[^\w\s]", " ", text)
    text = " ".join(text.split())
    return text

# è¿½åŠ ï¼šç›¸å¯¹æ—¶é—´å½’ä¸€åŒ–ä¸ºç»å¯¹æ—¥æœŸï¼ˆæœ‰é™æ”¯æŒï¼štoday/yesterday/tomorrow/X days ago/in X days/last week/next weekï¼‰
def _resolve_relative_times(text: str, anchor: datetime) -> str:
    import re
    # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
    t = str(text) if text is not None else ""
    # today / yesterday / tomorrow
    t = re.sub(r"\btoday\b", anchor.date().isoformat(), t, flags=re.IGNORECASE)
    t = re.sub(r"\byesterday\b", (anchor - timedelta(days=1)).date().isoformat(), t, flags=re.IGNORECASE)
    t = re.sub(r"\btomorrow\b", (anchor + timedelta(days=1)).date().isoformat(), t, flags=re.IGNORECASE)
    # X days ago / in X days
    def _ago_repl(m: re.Match[str]) -> str:
        n = int(m.group(1))
        return (anchor - timedelta(days=n)).date().isoformat()
    def _in_repl(m: re.Match[str]) -> str:
        n = int(m.group(1))
        return (anchor + timedelta(days=n)).date().isoformat()
    t = re.sub(r"\b(\d+)\s+days\s+ago\b", _ago_repl, t, flags=re.IGNORECASE)
    t = re.sub(r"\bin\s+(\d+)\s+days\b", _in_repl, t, flags=re.IGNORECASE)
    # last week / next weekï¼ˆä»¥7å¤©è¿‘ä¼¼ï¼‰
    t = re.sub(r"\blast\s+week\b", (anchor - timedelta(days=7)).date().isoformat(), t, flags=re.IGNORECASE)
    t = re.sub(r"\bnext\s+week\b", (anchor + timedelta(days=7)).date().isoformat(), t, flags=re.IGNORECASE)
    return t

def loc_f1_score(prediction: str, ground_truth: str) -> float:
    # å•ç­”æ¡ˆ F1ï¼šæŒ‰è¯é›†åˆè®¡ç®—ï¼ˆè¿‘ä¼¼åŸå§‹å®ç°ï¼Œå»é™¤è¯å¹²ä¾èµ–ï¼‰
    # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
    pred_str = str(prediction) if prediction is not None else ""
    truth_str = str(ground_truth) if ground_truth is not None else ""

    p_tokens = _loc_normalize(pred_str).split()
    g_tokens = _loc_normalize(truth_str).split()
    if not p_tokens or not g_tokens:
        return 0.0
    p = set(p_tokens)
    g = set(g_tokens)
    tp = len(p & g)
    precision = tp / len(p) if p else 0.0
    recall = tp / len(g) if g else 0.0
    return (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0

def loc_multi_f1(prediction: str, ground_truth: str) -> float:
    # å¤šç­”æ¡ˆ F1ï¼šprediction ä¸ ground_truth ä»¥é€—å·åˆ†éš”ï¼Œé€ä¸€åŒ¹é…å–æœ€å¤§ï¼Œå†å¯¹å¤šä¸ª GT å–å¹³å‡
    # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
    pred_str = str(prediction) if prediction is not None else ""
    truth_str = str(ground_truth) if ground_truth is not None else ""

    predictions = [p.strip() for p in str(pred_str).split(',') if p.strip()]
    ground_truths = [g.strip() for g in str(truth_str).split(',') if g.strip()]
    if not predictions or not ground_truths:
        return 0.0
    def _f1(a: str, b: str) -> float:
        return loc_f1_score(a, b)
    vals = []
    for gt in ground_truths:
        vals.append(max(_f1(pred, gt) for pred in predictions))
    return sum(vals) / len(vals)

# æ ‡å‡†åŒ– LoCoMo ç±»åˆ«åï¼šæ”¯æŒæ•°å­— category ä¸å­—ç¬¦ä¸² cat/type
CATEGORY_MAP_NUM_TO_NAME = {
    4: "Single-Hop",
    1: "Multi-Hop",
    3: "Open Domain",
    2: "Temporal",
}

_TYPE_ALIASES = {
    "single-hop": "Single-Hop",
    "singlehop": "Single-Hop",
    "single hop": "Single-Hop",
    "multi-hop": "Multi-Hop",
    "multihop": "Multi-Hop",
    "multi hop": "Multi-Hop",
    "open domain": "Open Domain",
    "opendomain": "Open Domain",
    "temporal": "Temporal",
}

def get_category_label(item: Dict[str, Any]) -> str:
    # 1) ç›´æ¥ç”¨å­—ç¬¦ä¸² cat
    cat = item.get("cat")
    if isinstance(cat, str) and cat.strip():
        name = cat.strip()
        lower = name.lower()
        return _TYPE_ALIASES.get(lower, name)
    # 2) æ•°å­— category è½¬åç§°
    cat_num = item.get("category")
    if isinstance(cat_num, int):
        return CATEGORY_MAP_NUM_TO_NAME.get(cat_num, "unknown")
    # 3) å¤‡ç”¨ type å­—æ®µ
    t = item.get("type")
    if isinstance(t, str) and t.strip():
        lower = t.strip().lower()
        return _TYPE_ALIASES.get(lower, t.strip())
    return "unknown"


def smart_context_selection(contexts: List[str], question: str, max_chars: int = 12000) -> str:
    """åŸºäºé—®é¢˜å…³é”®è¯æ™ºèƒ½é€‰æ‹©ä¸Šä¸‹æ–‡"""
    if not contexts:
        return ""

    # æå–é—®é¢˜å…³é”®è¯ï¼ˆåªä¿ç•™æœ‰æ„ä¹‰çš„è¯ï¼‰
    question_lower = question.lower()
    stop_words = {'what', 'when', 'where', 'who', 'why', 'how', 'did', 'do', 'does', 'is', 'are', 'was', 'were', 'the', 'a', 'an', 'and', 'or', 'but'}
    question_words = set(re.findall(r'\b\w+\b', question_lower))
    question_words = {word for word in question_words if word not in stop_words and len(word) > 2}

    print(f"ğŸ” é—®é¢˜å…³é”®è¯: {question_words}")

    # ç»™æ¯ä¸ªä¸Šä¸‹æ–‡æ‰“åˆ†
    scored_contexts = []
    for i, context in enumerate(contexts):
        context_lower = context.lower()
        score = 0

        # å…³é”®è¯åŒ¹é…å¾—åˆ†
        keyword_matches = 0
        for word in question_words:
            if word in context_lower:
                keyword_matches += 1
                # å…³é”®è¯å‡ºç°æ¬¡æ•°è¶Šå¤šï¼Œå¾—åˆ†è¶Šé«˜
                score += context_lower.count(word) * 2

        # ä¸Šä¸‹æ–‡é•¿åº¦å¾—åˆ†ï¼ˆé€‚ä¸­çš„é•¿åº¦æ›´å¥½ï¼‰
        context_len = len(context)
        if 100 < context_len < 2000:  # ç†æƒ³é•¿åº¦èŒƒå›´
            score += 5
        elif context_len >= 2000:  # å¤ªé•¿å¯èƒ½åŒ…å«æ— å…³ä¿¡æ¯
            score += 2

        # å¦‚æœæ˜¯å‰å‡ ä¸ªä¸Šä¸‹æ–‡ï¼Œç»™äºˆé¢å¤–åˆ†æ•°ï¼ˆé€šå¸¸ç›¸å…³æ€§æ›´é«˜ï¼‰
        if i < 3:
            score += 3

        scored_contexts.append((score, context, keyword_matches))

    # æŒ‰å¾—åˆ†æ’åº
    scored_contexts.sort(key=lambda x: x[0], reverse=True)

    # é€‰æ‹©é«˜å¾—åˆ†çš„ä¸Šä¸‹æ–‡ï¼Œç›´åˆ°è¾¾åˆ°å­—ç¬¦é™åˆ¶
    selected = []
    total_chars = 0
    selected_count = 0

    print("ğŸ“Š ä¸Šä¸‹æ–‡ç›¸å…³æ€§åˆ†æ:")
    for score, context, matches in scored_contexts[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
        print(f"  - å¾—åˆ†: {score}, å…³é”®è¯åŒ¹é…: {matches}, é•¿åº¦: {len(context)}")

    for score, context, matches in scored_contexts:
        if total_chars + len(context) <= max_chars:
            selected.append(context)
            total_chars += len(context)
            selected_count += 1
        else:
            # å¦‚æœè¿™ä¸ªä¸Šä¸‹æ–‡å¾—åˆ†å¾ˆé«˜ä½†æ”¾ä¸ä¸‹ï¼Œå°è¯•æˆªå–
            if score > 10 and total_chars < max_chars - 500:
                remaining = max_chars - total_chars
                # æ‰¾åˆ°åŒ…å«å…³é”®è¯çš„éƒ¨åˆ†
                lines = context.split('\n')
                relevant_lines = []
                current_chars = 0

                for line in lines:
                    line_lower = line.lower()
                    line_relevance = any(word in line_lower for word in question_words)

                    if line_relevance and current_chars < remaining - 100:
                        relevant_lines.append(line)
                        current_chars += len(line)

                if relevant_lines:
                    truncated = '\n'.join(relevant_lines)
                    if len(truncated) > 100:  # ç¡®ä¿æœ‰è¶³å¤Ÿå†…å®¹
                        selected.append(truncated + "\n[ç›¸å…³å†…å®¹æˆªæ–­...]")
                        total_chars += len(truncated)
                        selected_count += 1
            break  # ä¸å†å°è¯•æ·»åŠ æ›´å¤šä¸Šä¸‹æ–‡

    result = "\n\n".join(selected)
    print(f"âœ… æ™ºèƒ½é€‰æ‹©: {selected_count}ä¸ªä¸Šä¸‹æ–‡, æ€»é•¿åº¦: {total_chars}å­—ç¬¦")
    return result


def get_search_params_by_category(category: str):
    """æ ¹æ®é—®é¢˜ç±»åˆ«è°ƒæ•´æ£€ç´¢å‚æ•°"""
    params_map = {
        "Multi-Hop": {"limit": 20, "max_chars": 15000},
        "Temporal": {"limit": 16, "max_chars": 10000},
        "Open Domain": {"limit": 24, "max_chars": 18000},
        "Single-Hop": {"limit": 12, "max_chars": 8000},
    }
    return params_map.get(category, {"limit": 16, "max_chars": 12000})


async def run_locomo_eval(
    sample_size: int = 1,
    group_id: str | None = None,
    search_limit: int = 8,
    context_char_budget: int = 4000,  # ä¿æŒé»˜è®¤å€¼ä¸å˜
    llm_temperature: float = 0.0,
    llm_max_tokens: int = 32,
    search_type: str = "hybrid",  # ä¿æŒé»˜è®¤å€¼ä¸å˜
    output_path: str | None = None,
    skip_ingest_if_exists: bool = True,
    llm_timeout: float = 10.0,
    llm_max_retries: int = 1
) -> Dict[str, Any]:

    # å‡½æ•°å†…éƒ¨ä½¿ç”¨ä¸‰è·¯æ£€ç´¢é€»è¾‘ï¼Œä½†ä¿æŒå‚æ•°ç­¾åä¸å˜
    group_id = group_id or SELECTED_GROUP_ID
    data_path = os.path.join(PROJECT_ROOT, "data", "locomo10.json")
    if not os.path.exists(data_path):
        data_path = os.path.join(os.getcwd(), "data", "locomo10.json")
    with open(data_path, "r", encoding="utf-8") as f:
        raw = json.load(f)
    # LoCoMo æ•°æ®ç»“æ„ï¼šé¡¶å±‚ä¸ºè‹¥å¹²å¯¹è±¡ï¼Œæ¯ä¸ªå¯¹è±¡ä¸‹æœ‰ qa åˆ—è¡¨
    qa_items: List[Dict[str, Any]] = []
    if isinstance(raw, list):
        for entry in raw:
            qa_items.extend(entry.get("qa", []))
    else:
        qa_items.extend(raw.get("qa", []))
    items: List[Dict[str, Any]] = qa_items[:sample_size]

    # === ä¿æŒåŸæ¥çš„æ•°æ®æ‘„å…¥é€»è¾‘ ===
    entries = raw if isinstance(raw, list) else [raw]

    # åªæ‘„å…¥å‰1æ¡å¯¹è¯ï¼ˆä¿æŒåŸæ ·ï¼‰
    max_dialogues_to_ingest = 1
    contents: List[str] = []
    print(f"ğŸ“Š æ‰¾åˆ° {len(entries)} ä¸ªå¯¹è¯å¯¹è±¡ï¼Œåªæ‘„å…¥å‰ {max_dialogues_to_ingest} æ¡")

    for i, entry in enumerate(entries[:max_dialogues_to_ingest]):
        if not isinstance(entry, dict):
            continue

        conv = entry.get("conversation", {})
        sample_id = entry.get("sample_id", f"unknown_{i}")

        print(f"ğŸ” å¤„ç†å¯¹è¯ {i+1}: {sample_id}")

        lines: List[str] = []
        if isinstance(conv, dict):
            # æ”¶é›†æ‰€æœ‰ session_* çš„æ¶ˆæ¯
            session_count = 0
            for key, val in conv.items():
                if isinstance(val, list) and key.startswith("session_"):
                    session_count += 1
                    for msg in val:
                        role = msg.get("speaker") or "ç”¨æˆ·"
                        text = msg.get("text") or ""
                        text = str(text).strip()
                        if not text:
                            continue
                        lines.append(f"{role}: {text}")

            print(f"  - åŒ…å« {session_count} ä¸ªsession, {len(lines)} æ¡æ¶ˆæ¯")

        if not lines:
            print(f"âš ï¸  è­¦å‘Š: å¯¹è¯ {sample_id} æ²¡æœ‰å¯¹è¯å†…å®¹ï¼Œè·³è¿‡æ‘„å…¥")
            continue

        contents.append("\n".join(lines))

    print(f"ğŸ“¥ æ€»å…±æ‘„å…¥ {len(contents)} ä¸ªå¯¹è¯çš„conversationå†…å®¹")

    # é€‰æ‹©è¦è¯„æµ‹çš„QAå¯¹ï¼ˆä»æ‰€æœ‰å¯¹è¯ä¸­é€‰å–ï¼‰
    indexed_items: List[tuple[int, Dict[str, Any]]] = []
    if isinstance(raw, list):
        for e_idx, entry in enumerate(raw):
            for qa in entry.get("qa", []):
                indexed_items.append((e_idx, qa))
    else:
        for qa in raw.get("qa", []):
            indexed_items.append((0, qa))

    # è¿™é‡Œä½¿ç”¨sample_sizeæ¥é™åˆ¶è¯„æµ‹çš„QAæ•°é‡
    selected = indexed_items[:sample_size]
    items: List[Dict[str, Any]] = [qa for _, qa in selected]

    print(f"ğŸ¯ å°†è¯„æµ‹ {len(items)} ä¸ªQAå¯¹ï¼Œæ•°æ®åº“ä¸­åªåŒ…å« {len(contents)} ä¸ªå¯¹è¯")
    # === ä¿®æ”¹ç»“æŸ ===

    connector = Neo4jConnector()

    # å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶é‡æ–°æ‘„å…¥çº¯å‡€çš„å¯¹è¯æ•°æ®
    print("ğŸ”„ å¼ºåˆ¶é‡æ–°æ‘„å…¥çº¯å‡€çš„å¯¹è¯æ•°æ®...")
    await ingest_contexts_via_full_pipeline(contents, group_id, save_chunk_output=True)

    # ä½¿ç”¨å¼‚æ­¥LLMå®¢æˆ·ç«¯
    with get_db_context() as db:
        factory = MemoryClientFactory(db)
        llm_client = factory.get_llm_client(SELECTED_LLM_ID)
    # åˆå§‹åŒ–embedderç”¨äºç›´æ¥è°ƒç”¨
    with get_db_context() as db:
        config_service = MemoryConfigService(db)
        cfg_dict = config_service.get_embedder_config(SELECTED_EMBEDDING_ID)
    embedder = OpenAIEmbedderClient(
        model_config=RedBearModelConfig.model_validate(cfg_dict)
    )
    
    # connector initialized above
    latencies_llm: List[float] = []
    latencies_search: List[float] = []
    # ä¸Šä¸‹æ–‡è¯Šæ–­æ”¶é›†
    per_query_context_counts: List[int] = []
    per_query_context_avg_tokens: List[float] = []
    per_query_context_chars: List[int] = []
    per_query_context_tokens_total: List[int] = []
    # è¯¦ç»†æ ·æœ¬è°ƒè¯•ä¿¡æ¯
    samples: List[Dict[str, Any]] = []
    # é€šç”¨æŒ‡æ ‡
    f1s: List[float] = []
    b1s: List[float] = []
    jss: List[float] = []
    # å‚è€ƒ LoCoMo è¯„æµ‹çš„ç±»åˆ«ä¸“ç”¨ F1ï¼ˆmulti-hop ä½¿ç”¨å¤šç­”æ¡ˆ F1ï¼‰
    loc_f1s: List[float] = []
    # Per-category aggregation
    cat_counts: Dict[str, int] = {}
    cat_f1s: Dict[str, List[float]] = {}
    cat_b1s: Dict[str, List[float]] = {}
    cat_jss: Dict[str, List[float]] = {}
    cat_loc_f1s: Dict[str, List[float]] = {}
    try:
        for item in items:
            q = item.get("question", "")
            ref = item.get("answer", "")
            # ç¡®ä¿ç­”æ¡ˆæ˜¯å­—ç¬¦ä¸²
            ref_str = str(ref) if ref is not None else ""
            cat = get_category_label(item)

            print(f"\n=== å¤„ç†é—®é¢˜: {q} ===")

            # æ ¹æ®ç±»åˆ«è°ƒæ•´æ£€ç´¢å‚æ•°
            search_params = get_search_params_by_category(cat)
            adjusted_limit = search_params["limit"]
            max_chars = search_params["max_chars"]

            print(f"ğŸ·ï¸ ç±»åˆ«: {cat}, æ£€ç´¢å‚æ•°: limit={adjusted_limit}, max_chars={max_chars}")

            # æ”¹è¿›çš„æ£€ç´¢é€»è¾‘ï¼šä½¿ç”¨ä¸‰è·¯æ£€ç´¢ï¼ˆstatements, dialogues, entitiesï¼‰
            t0 = time.time()
            contexts_all: List[str] = []
            search_results = None  # ä¿å­˜å®Œæ•´çš„æ£€ç´¢ç»“æœ

            try:
                if search_type == "embedding":
                    # ç›´æ¥è°ƒç”¨åµŒå…¥æ£€ç´¢ï¼ŒåŒ…å«ä¸‰è·¯æ•°æ®
                    search_results = await search_graph_by_embedding(
                        connector=connector,
                        embedder_client=embedder,
                        query_text=q,
                        group_id=group_id,
                        limit=adjusted_limit,
                        include=["chunks", "statements", "entities", "summaries"],  # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„ç±»å‹
                    )
                    chunks = search_results.get("chunks", [])
                    statements = search_results.get("statements", [])
                    entities = search_results.get("entities", [])
                    summaries = search_results.get("summaries", [])
                    
                    print(f"âœ… åµŒå…¥æ£€ç´¢æˆåŠŸ: {len(chunks)} chunks, {len(statements)} æ¡é™ˆè¿°, {len(entities)} ä¸ªå®ä½“, {len(summaries)} ä¸ªæ‘˜è¦")
                    
                    # æ„å»ºä¸Šä¸‹æ–‡ï¼šä¼˜å…ˆä½¿ç”¨ chunksã€statements å’Œ summaries
                    for c in chunks:
                        content = str(c.get("content", "")).strip()
                        if content:
                            contexts_all.append(content)

                    for s in statements:
                        stmt_text = str(s.get("statement", "")).strip()
                        if stmt_text:
                            contexts_all.append(stmt_text)
                    
                    for sm in summaries:
                        summary_text = str(sm.get("summary", "")).strip()
                        if summary_text:
                            contexts_all.append(summary_text)

                    # å®ä½“æ‘˜è¦ï¼šæœ€å¤šåŠ å…¥å‰3ä¸ªé«˜åˆ†å®ä½“ï¼Œé¿å…å™ªå£°
                    scored = [e for e in entities if e.get("score") is not None]
                    top_entities = sorted(scored, key=lambda x: x.get("score", 0), reverse=True)[:3] if scored else entities[:3]
                    if top_entities:
                        summary_lines = []
                        for e in top_entities:
                            name = str(e.get("name", "")).strip()
                            etype = str(e.get("entity_type", "")).strip()
                            score = e.get("score")
                            if name:
                                meta = []
                                if etype:
                                    meta.append(f"type={etype}")
                                if isinstance(score, (int, float)):
                                    meta.append(f"score={score:.3f}")
                                summary_lines.append(f"EntitySummary: {name}{(' [' + '; '.join(meta) + ']') if meta else ''}")
                        if summary_lines:
                            contexts_all.append("\n".join(summary_lines))

                elif search_type == "keyword":
                    # ç›´æ¥è°ƒç”¨å…³é”®è¯æ£€ç´¢
                    search_results = await search_graph(
                        connector=connector,
                        q=q,
                        group_id=group_id,
                        limit=adjusted_limit
                    )
                    dialogs = search_results.get("dialogues", [])
                    statements = search_results.get("statements", [])
                    entities = search_results.get("entities", [])
                    print(f"ğŸ”¤ å…³é”®è¯æ£€ç´¢æ‰¾åˆ° {len(dialogs)} æ¡å¯¹è¯, {len(statements)} æ¡é™ˆè¿°, {len(entities)} ä¸ªå®ä½“")

                    # æ„å»ºä¸Šä¸‹æ–‡
                    for d in dialogs:
                        content = str(d.get("content", "")).strip()
                        if content:
                            contexts_all.append(content)
                    for s in statements:
                        stmt_text = str(s.get("statement", "")).strip()
                        if stmt_text:
                            contexts_all.append(stmt_text)
                    # å®ä½“å¤„ç†ï¼ˆå…³é”®è¯æ£€ç´¢çš„å®ä½“å¯èƒ½æ²¡æœ‰åˆ†æ•°ï¼‰
                    if entities:
                        entity_names = [str(e.get("name", "")).strip() for e in entities[:5] if e.get("name")]
                        if entity_names:
                            contexts_all.append(f"EntitySummary: {', '.join(entity_names)}")

                else:  # hybrid
                    # ğŸ¯ å…³é”®ä¿®å¤ï¼šæ··åˆæ£€ç´¢ä½¿ç”¨æ›´ä¸¥æ ¼çš„å›é€€æœºåˆ¶
                    print("ğŸ”€ ä½¿ç”¨æ··åˆæ£€ç´¢ï¼ˆå¸¦å›é€€æœºåˆ¶ï¼‰...")
                    try:
                        search_results = await run_hybrid_search(
                            query_text=q,
                            search_type=search_type,
                            group_id=group_id,
                            limit=adjusted_limit,
                            include=["chunks", "statements", "entities", "summaries"],
                            output_path=None,
                        )
                        
                        # ğŸ¯ å…³é”®ä¿®å¤ï¼šæ­£ç¡®å¤„ç†æ··åˆæ£€ç´¢çš„æ‰å¹³ç»“æ„
                        # æ–°çš„APIè¿”å›æ‰å¹³ç»“æ„ï¼Œç›´æ¥ä»é¡¶å±‚è·å–ç»“æœ
                        if search_results and isinstance(search_results, dict):
                            # æ–°APIè¿”å›æ‰å¹³ç»“æ„ï¼šç›´æ¥ä»é¡¶å±‚è·å–
                            chunks = search_results.get("chunks", [])
                            statements = search_results.get("statements", [])
                            entities = search_results.get("entities", [])
                            summaries = search_results.get("summaries", [])
                            
                            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆç»“æœ
                            if chunks or statements or entities or summaries:
                                print(f"âœ… æ··åˆæ£€ç´¢æˆåŠŸ: {len(chunks)} chunks, {len(statements)} é™ˆè¿°, {len(entities)} å®ä½“, {len(summaries)} æ‘˜è¦")
                            else:
                                # å¦‚æœé¡¶å±‚æ²¡æœ‰ç»“æœï¼Œå°è¯•æ—§çš„åµŒå¥—ç»“æ„ï¼ˆå‘åå…¼å®¹ï¼‰
                                reranked = search_results.get("reranked_results", {})
                                if reranked and isinstance(reranked, dict):
                                    chunks = reranked.get("chunks", [])
                                    statements = reranked.get("statements", [])
                                    entities = reranked.get("entities", [])
                                    summaries = reranked.get("summaries", [])
                                    print(f"âœ… æ··åˆæ£€ç´¢æˆåŠŸï¼ˆä½¿ç”¨æ—§æ ¼å¼rerankedç»“æœï¼‰: {len(chunks)} chunks, {len(statements)} é™ˆè¿°")
                                else:
                                    raise ValueError("æ··åˆæ£€ç´¢è¿”å›ç©ºç»“æœ")
                        else:
                            raise ValueError("æ··åˆæ£€ç´¢è¿”å›ç©ºç»“æœ")
                            
                    except Exception as e:
                        print(f"âŒ æ··åˆæ£€ç´¢å¤±è´¥: {e}ï¼Œå›é€€åˆ°åµŒå…¥æ£€ç´¢")
                        search_results = await search_graph_by_embedding(
                            connector=connector,
                            embedder_client=embedder,
                            query_text=q,
                            group_id=group_id,
                            limit=adjusted_limit,
                            include=["chunks", "statements", "entities", "summaries"],
                        )
                        chunks = search_results.get("chunks", [])
                        statements = search_results.get("statements", [])
                        entities = search_results.get("entities", [])
                        summaries = search_results.get("summaries", [])
                        print(f"âœ… å›é€€åµŒå…¥æ£€ç´¢æˆåŠŸ: {len(chunks)} chunks, {len(statements)} é™ˆè¿°")
                    
                    # ğŸ¯ ç»Ÿä¸€å¤„ç†ï¼šæ„å»ºä¸Šä¸‹æ–‡ï¼ˆæ‰€æœ‰æ£€ç´¢ç±»å‹å…±ç”¨ï¼‰
                    for c in chunks:
                        content = str(c.get("content", "")).strip()
                        if content:
                            contexts_all.append(content)
                    
                    for s in statements:
                        stmt_text = str(s.get("statement", "")).strip()
                        if stmt_text:
                            contexts_all.append(stmt_text)
                    
                    for sm in summaries:
                        summary_text = str(sm.get("summary", "")).strip()
                        if summary_text:
                            contexts_all.append(summary_text)
                    
                    # å®ä½“æ‘˜è¦ï¼šæœ€å¤šåŠ å…¥å‰3ä¸ªé«˜åˆ†å®ä½“
                    if entities:
                        scored = [e for e in entities if e.get("score") is not None]
                        top_entities = sorted(scored, key=lambda x: x.get("score", 0), reverse=True)[:3] if scored else entities[:3]
                        if top_entities:
                            summary_lines = []
                            for e in top_entities:
                                name = str(e.get("name", "")).strip()
                                etype = str(e.get("entity_type", "")).strip()
                                score = e.get("score")
                                if name:
                                    meta = []
                                    if etype:
                                        meta.append(f"type={etype}")
                                    if isinstance(score, (int, float)):
                                        meta.append(f"score={score:.3f}")
                                    summary_lines.append(f"EntitySummary: {name}{(' [' + '; '.join(meta) + ']') if meta else ''}")
                            if summary_lines:
                                contexts_all.append("\n".join(summary_lines))
                
                # å…³é”®ä¿®å¤ï¼šè¿‡æ»¤æ‰åŒ…å«å½“å‰é—®é¢˜ç­”æ¡ˆçš„ä¸Šä¸‹æ–‡
                filtered_contexts = []
                for context in contexts_all:
                    content = str(context)
                    # æ’é™¤åŒ…å«å½“å‰é—®é¢˜æ ‡å‡†ç­”æ¡ˆçš„ä¸Šä¸‹æ–‡
                    if ref_str and ref_str.strip() and ref_str.strip() in content:
                        print("ğŸš« è¿‡æ»¤æ‰åŒ…å«æ ‡å‡†ç­”æ¡ˆçš„ä¸Šä¸‹æ–‡")
                        continue
                    filtered_contexts.append(context)

                print(f"ğŸ“Š è¿‡æ»¤åä¿ç•™ {len(filtered_contexts)} ä¸ªä¸Šä¸‹æ–‡ (åŸ {len(contexts_all)} ä¸ª)")
                contexts_all = filtered_contexts

                # è¾“å‡ºå®Œæ•´çš„æ£€ç´¢ç»“æœä¿¡æ¯
                print("ğŸ” æ£€ç´¢ç»“æœè¯¦æƒ…:")
                if search_results:
                    output_data = {
                        "statements": [
                            {
                                "statement": s.get("statement", "")[:200] + "..." if len(s.get("statement", "")) > 200 else s.get("statement", ""),
                                "score": s.get("score", 0.0)
                            }
                            for s in (statements[:2] if 'statements' in locals() else [])
                        ],
                        "dialogues": [
                            {
                                "uuid": d.get("uuid", ""),
                                "group_id": d.get("group_id", ""),
                                "content": d.get("content", "")[:200] + "..." if len(d.get("content", "")) > 200 else d.get("content", ""),
                                "score": d.get("score", 0.0)
                            }
                            for d in (dialogs[:2] if 'dialogs' in locals() else [])
                        ],
                        "entities": [
                            {
                                "name": e.get("name", ""),
                                "entity_type": e.get("entity_type", ""),
                                "score": e.get("score", 0.0)
                            }
                            for e in (entities[:2] if 'entities' in locals() else [])
                        ]
                    }
                    print(json.dumps(output_data, ensure_ascii=False, indent=2))
                else:
                    print("   æ— æ£€ç´¢ç»“æœ")

            except Exception as e:
                print(f"âŒ {search_type}æ£€ç´¢å¤±è´¥: {e}")
                contexts_all = []
                search_results = None

            t1 = time.time()
            latencies_search.append((t1 - t0) * 1000)

            # ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡é€‰æ‹©
            context_text = ""
            if contexts_all:
                context_text = smart_context_selection(contexts_all, q, max_chars=max_chars)

                # å¦‚æœæ™ºèƒ½é€‰æ‹©åä»ç„¶è¿‡é•¿ï¼Œè¿›è¡Œæœ€ç»ˆä¿æŠ¤æ€§æˆªæ–­
                if len(context_text) > max_chars:
                    print(f"âš ï¸ æ™ºèƒ½é€‰æ‹©åä»ç„¶è¿‡é•¿ ({len(context_text)}å­—ç¬¦)ï¼Œè¿›è¡Œæœ€ç»ˆæˆªæ–­")
                    context_text = context_text[:max_chars] + "\n\n[æœ€ç»ˆæˆªæ–­...]"

                # æ—¶é—´è§£æ
                anchor_date = datetime(2023, 5, 8)  # ä½¿ç”¨å›ºå®šæ—¥æœŸç¡®ä¿ä¸€è‡´æ€§
                context_text = _resolve_relative_times(context_text, anchor_date)

                context_text = f"Reference date: {anchor_date.date().isoformat()}\n\n" + context_text

                print(f"ğŸ“ æœ€ç»ˆä¸Šä¸‹æ–‡é•¿åº¦: {len(context_text)} å­—ç¬¦")

                # æ˜¾ç¤ºä¸åŒä¸Šä¸‹æ–‡çš„é¢„è§ˆ
                print("ğŸ” ä¸Šä¸‹æ–‡é¢„è§ˆ:")
                for j, context in enumerate(contexts_all[:3]):  # æ˜¾ç¤ºå‰3ä¸ªä¸Šä¸‹æ–‡
                    preview = context[:150].replace('\n', ' ')
                    print(f"  ä¸Šä¸‹æ–‡{j+1}: {preview}...")

            else:
                print("âŒ æ²¡æœ‰æ£€ç´¢åˆ°æœ‰æ•ˆä¸Šä¸‹æ–‡")
                context_text = "No relevant context found."

            # è®°å½•ä¸Šä¸‹æ–‡è¯Šæ–­ä¿¡æ¯
            per_query_context_counts.append(len(contexts_all))
            per_query_context_avg_tokens.append(avg_context_tokens([context_text]))
            per_query_context_chars.append(len(context_text))
            per_query_context_tokens_total.append(len(_loc_normalize(context_text).split()))

            # LLM æç¤ºè¯
            messages = [
                {"role": "system", "content": (
                    "You are a precise QA assistant. Answer following these rules:\n"
                    "1) Extract the EXACT information mentioned in the context\n"
                    "2) For time questions: calculate actual dates from relative times\n"
                    "3) Return ONLY the answer text in simplest form\n"
                    "4) For dates, use format 'DD Month YYYY' (e.g., '7 May 2023')\n"
                    "5) If no clear answer found, respond with 'Unknown'"
                )},
                {"role": "user", "content": f"Question: {q}\n\nContext:\n{context_text}"},
            ]

            t2 = time.time()
            # ä½¿ç”¨å¼‚æ­¥è°ƒç”¨
            resp = await llm_client.chat(messages=messages)
            t3 = time.time()
            latencies_llm.append((t3 - t2) * 1000)
            
            # å…¼å®¹ä¸åŒçš„å“åº”æ ¼å¼
            pred = resp.content.strip() if hasattr(resp, 'content') else (resp["choices"][0]["message"]["content"].strip() if isinstance(resp, dict) else "Unknown")
            
            # è®¡ç®—æŒ‡æ ‡ï¼ˆç¡®ä¿ä½¿ç”¨å­—ç¬¦ä¸²ï¼‰
            f1_val = common_f1(str(pred), ref_str)
            b1_val = bleu1(str(pred), ref_str)
            j_val = jaccard(str(pred), ref_str)

            f1s.append(f1_val)
            b1s.append(b1_val)
            jss.append(j_val)

            # Accumulate by category
            cat_counts[cat] = cat_counts.get(cat, 0) + 1
            cat_f1s.setdefault(cat, []).append(f1_val)
            cat_b1s.setdefault(cat, []).append(b1_val)
            cat_jss.setdefault(cat, []).append(j_val)

            # LoCoMo ä¸“ç”¨ F1ï¼šmulti-hop(1) ä½¿ç”¨å¤šç­”æ¡ˆ F1ï¼Œå…¶å®ƒ(2/3/4)ä½¿ç”¨å•ç­”æ¡ˆ F1
            if item.get("category") in [2, 3, 4]:
                loc_val = loc_f1_score(str(pred), ref_str)
            elif item.get("category") in [1]:
                loc_val = loc_multi_f1(str(pred), ref_str)
            else:
                loc_val = loc_f1_score(str(pred), ref_str)
            loc_f1s.append(loc_val)
            cat_loc_f1s.setdefault(cat, []).append(loc_val)

            # ä¿å­˜å®Œæ•´çš„æ£€ç´¢ç»“æœä¿¡æ¯
            samples.append({
                "question": q,
                "answer": ref_str,
                "category": cat,
                "prediction": pred,
                "metrics": {
                    "f1": f1_val,
                    "b1": b1_val,
                    "j": j_val,
                    "loc_f1": loc_val
                },
                "retrieval": {
                    "retrieved_documents": len(contexts_all),
                    "context_length": len(context_text),
                    "search_limit": adjusted_limit,
                    "max_chars": max_chars
                },
                "timing": {
                    "search_ms": (t1 - t0) * 1000,
                    "llm_ms": (t3 - t2) * 1000
                }
            })

            print(f"ğŸ¤– LLM å›ç­”: {pred}")
            print(f"âœ… æ­£ç¡®ç­”æ¡ˆ: {ref_str}")
            print(f"ğŸ“ˆ å½“å‰æŒ‡æ ‡ - F1: {f1_val:.3f}, BLEU-1: {b1_val:.3f}, Jaccard: {j_val:.3f}, LoCoMo F1: {loc_val:.3f}")

        # Compute per-category averages and dispersion (std, iqr)
        def _percentile(sorted_vals: List[float], p: float) -> float:
            if not sorted_vals:
                return 0.0
            if len(sorted_vals) == 1:
                return sorted_vals[0]
            k = (len(sorted_vals) - 1) * p
            f = int(k)
            c = f + 1 if f + 1 < len(sorted_vals) else f
            if f == c:
                return sorted_vals[f]
            return sorted_vals[f] + (sorted_vals[c] - sorted_vals[f]) * (k - f)

        by_category: Dict[str, Dict[str, float | int]] = {}
        for c in cat_counts:
            f_list = cat_f1s.get(c, [])
            b_list = cat_b1s.get(c, [])
            j_list = cat_jss.get(c, [])
            lf_list = cat_loc_f1s.get(c, [])
            j_sorted = sorted(j_list)
            j_std = statistics.stdev(j_list) if len(j_list) > 1 else 0.0
            j_q75 = _percentile(j_sorted, 0.75)
            j_q25 = _percentile(j_sorted, 0.25)
            by_category[c] = {
                "count": cat_counts[c],
                "f1": (sum(f_list) / max(len(f_list), 1)) if f_list else 0.0,
                "b1": (sum(b_list) / max(len(b_list), 1)) if b_list else 0.0,
                "j": (sum(j_list) / max(len(j_list), 1)) if j_list else 0.0,
                "j_std": j_std,
                "j_iqr": (j_q75 - j_q25) if j_list else 0.0,
                # å‚è€ƒ LoCoMo è¯„æµ‹çš„ç±»åˆ«ä¸“ç”¨ F1
                "loc_f1": (sum(lf_list) / max(len(lf_list), 1)) if lf_list else 0.0,
            }

        # ç´¯åŠ å‘½ä¸­ï¼ˆcum accuracy by categoryï¼‰ï¼šä¸ evaluation_stats.py è¾“å‡ºå½¢å¼ç›¸ä»¿
        cum_accuracy_by_category = {c: sum(cat_loc_f1s.get(c, [])) for c in cat_counts}

        result = {
            "dataset": "locomo",
            "items": len(items),
            "metrics": {
                "f1": sum(f1s) / max(len(f1s), 1),
                "b1": sum(b1s) / max(len(b1s), 1),
                "j": sum(jss) / max(len(jss), 1),
                # LoCoMo ç±»åˆ«ä¸“ç”¨ F1 çš„æ€»ä½“
                "loc_f1": sum(loc_f1s) / max(len(loc_f1s), 1),
            },
            "by_category": by_category,
            "category_counts": cat_counts,
            "cum_accuracy_by_category": cum_accuracy_by_category,
            "context": {
                "avg_tokens": (sum(per_query_context_avg_tokens) / max(len(per_query_context_avg_tokens), 1)) if per_query_context_avg_tokens else 0.0,
                "avg_chars": (sum(per_query_context_chars) / max(len(per_query_context_chars), 1)) if per_query_context_chars else 0.0,
                "count_avg": (sum(per_query_context_counts) / max(len(per_query_context_counts), 1)) if per_query_context_counts else 0.0,
                "avg_memory_tokens": (sum(per_query_context_tokens_total) / max(len(per_query_context_tokens_total), 1)) if per_query_context_tokens_total else 0.0,
            },
            "latency": {
                "search": latency_stats(latencies_search),
                "llm": latency_stats(latencies_llm),
            },
            "samples": samples,
            "params": {
                "group_id": group_id,
                "search_limit": search_limit,
                "context_char_budget": context_char_budget,
                "search_type": search_type,
                "llm_id": SELECTED_LLM_ID,
                "retrieval_embedding_id": SELECTED_EMBEDDING_ID,
                "skip_ingest_if_exists": skip_ingest_if_exists,
                "llm_timeout": llm_timeout,
                "llm_max_retries": llm_max_retries,
                "llm_temperature": llm_temperature,
                "llm_max_tokens": llm_max_tokens
            },
            "timestamp": datetime.now().isoformat()
        }
        if output_path:
            try:
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                with open(output_path, "w", encoding="utf-8") as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            except Exception as e:
                print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
        return result
    finally:
        await connector.close()


def main():
    parser = argparse.ArgumentParser(description="Run LoCoMo evaluation with Qwen search")
    parser.add_argument("--sample_size", type=int, default=1, help="Number of samples to evaluate")
    parser.add_argument("--group_id", type=str, default=None, help="Group ID for retrieval")
    parser.add_argument("--search_limit", type=int, default=8, help="Search limit per query")
    parser.add_argument("--context_char_budget", type=int, default=12000, help="Max characters for context")
    parser.add_argument("--llm_temperature", type=float, default=0.0, help="LLM temperature")
    parser.add_argument("--llm_max_tokens", type=int, default=32, help="LLM max tokens")
    parser.add_argument("--search_type", type=str, default="embedding", choices=["keyword", "embedding", "hybrid"], help="Search type")
    parser.add_argument("--output_path", type=str, default=None, help="Output path for results")
    parser.add_argument("--skip_ingest_if_exists", action="store_true", help="Skip ingest if group exists")
    parser.add_argument("--llm_timeout", type=float, default=10.0, help="LLM timeout in seconds")
    parser.add_argument("--llm_max_retries", type=int, default=1, help="LLM max retries")
    args = parser.parse_args()

    load_dotenv()

    result = asyncio.run(run_locomo_eval(
        sample_size=args.sample_size,
        group_id=args.group_id,
        search_limit=args.search_limit,
        context_char_budget=args.context_char_budget,
        llm_temperature=args.llm_temperature,
        llm_max_tokens=args.llm_max_tokens,
        search_type=args.search_type,
        output_path=args.output_path,
        skip_ingest_if_exists=args.skip_ingest_if_exists,
        llm_timeout=args.llm_timeout,
        llm_max_retries=args.llm_max_retries
    ))

    print("\n" + "="*50)
    print("ğŸ“Š æœ€ç»ˆè¯„æµ‹ç»“æœ:")
    print(f"   æ ·æœ¬æ•°é‡: {result['items']}")
    print(f"   F1: {result['metrics']['f1']:.3f}")
    print(f"   BLEU-1: {result['metrics']['b1']:.3f}")
    print(f"   Jaccard: {result['metrics']['j']:.3f}")
    print(f"   LoCoMo F1: {result['metrics']['loc_f1']:.3f}")
    print(f"   å¹³å‡ä¸Šä¸‹æ–‡é•¿åº¦: {result['context']['avg_chars']:.0f} å­—ç¬¦")
    print(f"   å¹³å‡æ£€ç´¢å»¶è¿Ÿ: {result['latency']['search']['mean']:.1f}ms")
    print(f"   å¹³å‡LLMå»¶è¿Ÿ: {result['latency']['llm']['mean']:.1f}ms")

    if result['by_category']:
        print("\nğŸ“ˆ æŒ‰ç±»åˆ«ç»†åˆ†:")
        for cat, metrics in result['by_category'].items():
            print(f"   {cat}:")
            print(f"     æ ·æœ¬æ•°: {metrics['count']}")
            print(f"     F1: {metrics['f1']:.3f}")
            print(f"     LoCoMo F1: {metrics['loc_f1']:.3f}")
            print(f"     Jaccard: {metrics['j']:.3f} (Â±{metrics['j_std']:.3f}, IQR={metrics['j_iqr']:.3f})")


if __name__ == "__main__":
    main()
