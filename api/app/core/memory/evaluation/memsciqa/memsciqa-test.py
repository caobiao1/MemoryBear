import argparse
import asyncio
import json
import os
import time
from datetime import datetime
from typing import List, Dict, Any
import re

try:
    from dotenv import load_dotenv
except Exception:
    def load_dotenv():
        return None

# è·¯å¾„ä¸æ¨¡å—å¯¼å…¥ä¿æŒä¸ç°æœ‰è¯„ä¼°è„šæœ¬ä¸€è‡´
import sys
_THIS_DIR = os.path.dirname(os.path.abspath(__file__))
_PROJECT_ROOT = os.path.dirname(os.path.dirname(_THIS_DIR))
_SRC_DIR = os.path.join(_PROJECT_ROOT, "src")
for _p in (_SRC_DIR, _PROJECT_ROOT):
    if _p not in sys.path:
        sys.path.insert(0, _p)

# å¯¹é½ locomo_test çš„æ£€ç´¢é€»è¾‘ï¼šç›´æ¥ä½¿ç”¨ graph_search ä¸ Neo4jConnector/Embedder1
from app.repositories.neo4j.neo4j_connector import Neo4jConnector
from app.repositories.neo4j.graph_search import search_graph, search_graph_by_embedding
from app.core.memory.llm_tools.openai_embedder import OpenAIEmbedderClient
from app.core.models.base import RedBearModelConfig
from app.core.memory.utils.config_utils import get_embedder_config

from app.core.memory.utils.llm.llm_utils import get_llm_client
from app.core.memory.utils.config.definitions import PROJECT_ROOT, SELECTED_GROUP_ID, SELECTED_EMBEDDING_ID, SELECTED_LLM_ID
from app.core.memory.evaluation.common.metrics import exact_match, latency_stats, avg_context_tokens
try:
    from app.core.memory.evaluation.common.metrics import f1_score, bleu1, jaccard
except Exception:
    # å…œåº•ï¼šç®€å•å®ç°ï¼ˆå¿…è¦æ—¶ï¼‰
    def f1_score(pred: str, ref: str) -> float:
        ps = pred.lower().split()
        rs = ref.lower().split()
        if not ps or not rs:
            return 0.0
        tp = len(set(ps) & set(rs))
        if tp == 0:
            return 0.0
        precision = tp / len(ps)
        recall = tp / len(rs)
        if precision + recall == 0:
            return 0.0
        return 2 * precision * recall / (precision + recall)

    def bleu1(pred: str, ref: str) -> float:
        ps = pred.lower().split()
        rs = ref.lower().split()
        if not ps or not rs:
            return 0.0
        overlap = len([w for w in ps if w in rs])
        return overlap / max(len(ps), 1)

    def jaccard(pred: str, ref: str) -> float:
        ps = set(pred.lower().split())
        rs = set(ref.lower().split())
        union = len(ps | rs)
        if union == 0:
            return 0.0
        return len(ps & rs) / union


def smart_context_selection(contexts: List[str], question: str, max_chars: int = 4000) -> str:
    """åŸºäºé—®é¢˜å…³é”®è¯å¯¹ä¸Šä¸‹æ–‡è¿›è¡Œè¯„åˆ†é€‰æ‹©ï¼Œå¹¶åœ¨é¢„ç®—å†…æ‹¼æ¥æ–‡æœ¬ã€‚

    å‚è€ƒ evaluation/memsciqa/evaluate_qa.py çš„å®ç°ï¼Œé¿å…è·¯å¾„å¯¼å…¥å¸¦æ¥çš„ä¸ç¨³å®šã€‚
    """
    if not contexts:
        return ""
    question_lower = (question or "").lower()
    stop_words = {
        'what','when','where','who','why','how','did','do','does','is','are','was','were',
        'the','a','an','and','or','but'
    }
    question_words = set(re.findall(r"\b\w+\b", question_lower))
    question_words = {w for w in question_words if w not in stop_words and len(w) > 2}

    scored = []
    for i, ctx in enumerate(contexts):
        ctx_lower = (ctx or "").lower()
        score = 0
        matches = 0
        for w in question_words:
            if w in ctx_lower:
                matches += 1
                score += ctx_lower.count(w) * 2
        length = len(ctx)
        if 100 < length < 2000:
            score += 5
        elif length >= 2000:
            score += 2
        if i < 3:
            score += 3
        scored.append((score, ctx, matches))

    scored.sort(key=lambda x: x[0], reverse=True)

    selected: List[str] = []
    total = 0
    for score, ctx, _ in scored:
        if total + len(ctx) <= max_chars:
            selected.append(ctx)
            total += len(ctx)
        else:
            if score > 10 and total < max_chars - 200:
                remaining = max_chars - total
                lines = ctx.split('\n')
                rel_lines: List[str] = []
                cur = 0
                for line in lines:
                    l = line.lower()
                    if any(w in l for w in question_words) and cur < remaining - 50:
                        rel_lines.append(line)
                        cur += len(line)
                if rel_lines:
                    truncated = '\n'.join(rel_lines)
                    if len(truncated) > 50:
                        selected.append(truncated + "\n[ç›¸å…³å†…å®¹æˆªæ–­...]")
                        total += len(truncated)
            break
    return "\n\n".join(selected)


def extract_question_keywords(question: str, max_keywords: int = 8) -> List[str]:
    """æå–é—®é¢˜ä¸­çš„å…³é”®è¯ï¼ˆç®€å•è‹±æ–‡åˆ†è¯ï¼Œå»åœç”¨è¯ï¼Œé•¿åº¦>=3ï¼‰ã€‚"""
    ql = (question or "").lower()
    stop_words = {
        'what','when','where','who','why','how','did','do','does','is','are','was','were',
        'the','a','an','and','or','but','of','to','in','on','for','with','from','that','this'
    }
    words = re.findall(r"\b[\w-]+\b", ql)
    kws = [w for w in words if w not in stop_words and len(w) >= 3]
    # å»é‡ä¿åº
    seen = set()
    uniq = []
    for w in kws:
        if w not in seen:
            uniq.append(w)
            seen.add(w)
        if len(uniq) >= max_keywords:
            break
    return uniq


def analyze_contexts_simple(contexts: List[str], keywords: List[str], top_n: int = 5) -> List[Dict[str, int | float]]:
    """å¯¹ä¸Šä¸‹æ–‡è¿›è¡Œç®€å•ç›¸å…³æ€§æ‰“åˆ†ï¼Œä»…ç”¨äºæ§åˆ¶å°å¯è§†åŒ–ã€‚

    è¯„åˆ†: score = match_count*200 + min(len(text), 100000)/100
    """
    results = []
    for ctx in contexts:
        tl = (ctx or "").lower()
        match_count = sum(1 for k in keywords if k in tl)
        length = len(ctx)
        score = match_count * 200 + min(length, 100000) / 100.0
        results.append({"score": float(f"{score:.0f}"), "match": match_count, "length": length})
    results.sort(key=lambda x: (x["score"], x["match"], x["length"]), reverse=True)
    return results[:max(top_n, 0)]


# çº¯æµ‹è¯•è„šæœ¬ä¸è¿›è¡Œæ‘„å…¥ï¼›è‹¥éœ€æ‘„å…¥è¯·ä½¿ç”¨ evaluate_qa.py


def load_dataset_memsciqa(data_path: str) -> List[Dict[str, Any]]:
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°æ•°æ®é›†: {data_path}")
    items: List[Dict[str, Any]] = []
    with open(data_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                items.append(json.loads(line))
            except Exception:
                # è·³è¿‡åè¡Œä½†ä¸ä¸­æ–­
                continue
    return items


async def run_memsciqa_test(
    sample_size: int = 3,
    group_id: str | None = None,
    search_limit: int = 8,
    context_char_budget: int = 4000,
    llm_temperature: float = 0.0,
    llm_max_tokens: int = 64,
    search_type: str = "embedding",
    data_path: str | None = None,
    start_index: int = 0,
    verbose: bool = True,
) -> Dict[str, Any]:
    """memsciqa å¢å¼ºæµ‹è¯•è„šæœ¬ï¼šç»“åˆ evaluate_qa çš„ä¸‰è·¯æ£€ç´¢ä¸æ™ºèƒ½ä¸Šä¸‹æ–‡é€‰æ‹©ã€‚

    - æ”¯æŒä»æŒ‡å®šç´¢å¼•å¼€å§‹ä¸è¯„ä¼°å…¨éƒ¨æ ·æœ¬ï¼ˆsample_size<=0ï¼‰
    - æ”¯æŒåœ¨æ‘„å…¥å‰é‡ç½®ç»„ï¼ˆæ¸…ç©ºå›¾ï¼‰ä¸è·³è¿‡æ‘„å…¥
    - æ”¯æŒ keyword / embedding / hybrid ä¸‰ç§æ£€ç´¢
    """

    # é»˜è®¤ä½¿ç”¨æŒ‡å®šçš„ memsci ç»„ ID
    group_id = group_id or "group_memsci"

    # æ•°æ®è·¯å¾„è§£æï¼ˆé¡¹ç›®æ ¹ä¸å½“å‰å·¥ä½œç›®å½•å…œåº•ï¼‰
    if not data_path:
        proj_path = os.path.join(PROJECT_ROOT, "data", "msc_self_instruct.jsonl")
        cwd_path = os.path.join(os.getcwd(), "data", "msc_self_instruct.jsonl")
        if os.path.exists(proj_path):
            data_path = proj_path
        elif os.path.exists(cwd_path):
            data_path = cwd_path
        else:
            raise FileNotFoundError("æœªæ‰¾åˆ°æ•°æ®é›†: data/msc_self_instruct.jsonlï¼Œè¯·ç¡®ä¿å…¶å­˜åœ¨äºé¡¹ç›®æ ¹ç›®å½•æˆ–å½“å‰å·¥ä½œç›®å½•çš„ data ç›®å½•ä¸‹ã€‚")

    # åŠ è½½æ•°æ®
    all_items = load_dataset_memsciqa(data_path)
    if sample_size is None or sample_size <= 0:
        items = all_items[start_index:]
    else:
        items = all_items[start_index:start_index + sample_size]

    # åˆå§‹åŒ– LLMï¼ˆçº¯æµ‹è¯•ï¼šä¸è¿›è¡Œæ‘„å…¥ï¼‰
    llm = get_llm_client(SELECTED_LLM_ID)

    # åˆå§‹åŒ– Neo4j è¿æ¥ä¸å‘é‡æ£€ç´¢ Embedderï¼ˆå¯¹é½ locomo_testï¼‰
    connector = Neo4jConnector()
    embedder = None
    if search_type in ("embedding", "hybrid"):
        cfg_dict = get_embedder_config(SELECTED_EMBEDDING_ID)
        embedder = OpenAIEmbedderClient(
            model_config=RedBearModelConfig.model_validate(cfg_dict)
        )

    # è¯„ä¼°å¾ªç¯
    latencies_llm: List[float] = []
    latencies_search: List[float] = []
    # å­˜å‚¨å®Œæ•´ä¸Šä¸‹æ–‡æ–‡æœ¬ç”¨äºç»Ÿè®¡
    contexts_used: List[str] = []
    per_query_context_chars: List[int] = []
    per_query_context_counts: List[int] = []
    correct_flags: List[float] = []
    f1s: List[float] = []
    b1s: List[float] = []
    jss: List[float] = []
    samples: List[Dict[str, Any]] = []

    total_items = len(items)
    for idx, item in enumerate(items):
        if verbose:
            print(f"\nğŸ§ª è¯„ä¼°æ ·æœ¬: {idx+1}/{total_items}")
        question = item.get("self_instruct", {}).get("B", "") or item.get("question", "")
        reference = item.get("self_instruct", {}).get("A", "") or item.get("answer", "")

        # ä¸‰è·¯æ£€ç´¢ï¼šchunks/statements/entities/summariesï¼ˆå¯¹é½ qwen_search_eval.pyï¼‰
        t0 = time.time()
        results = None
        try:
            if search_type in ("embedding", "hybrid"):
                # ä½¿ç”¨åµŒå…¥æ£€ç´¢ï¼ˆä¸ qwen_search_eval å¯¹é½ï¼‰
                results = await search_graph_by_embedding(
                    connector=connector,
                    embedder_client=embedder,
                    query_text=question,
                    group_id=group_id,
                    limit=search_limit,
                    include=["chunks", "statements", "entities", "summaries"],  # ä½¿ç”¨ chunks è€Œä¸æ˜¯ dialogues
                )
            elif search_type == "keyword":
                # å…³é”®è¯æ£€ç´¢ï¼ˆç›´æ¥è°ƒç”¨ graph_searchï¼‰
                results = await search_graph(
                    connector=connector,
                    q=question,
                    group_id=group_id,
                    limit=search_limit,
                    include=["chunks", "statements", "entities", "summaries"],  # ä½¿ç”¨ chunks è€Œä¸æ˜¯ dialogues
                )
        except Exception:
            results = None
        t1 = time.time()
        search_ms = (t1 - t0) * 1000
        latencies_search.append(search_ms)

        # æ„å»ºä¸Šä¸‹æ–‡ï¼šåŒ…å« chunksã€é™ˆè¿°ã€æ‘˜è¦å’Œå®ä½“ï¼ˆå¯¹é½ qwen_search_eval.pyï¼‰
        contexts_all: List[str] = []
        retrieved_counts: Dict[str, int] = {}
        if results:
            chunks = results.get("chunks", [])
            statements = results.get("statements", [])
            entities = results.get("entities", [])
            summaries = results.get("summaries", [])
            retrieved_counts = {
                "chunks": len(chunks),
                "statements": len(statements),
                "entities": len(entities),
                "summaries": len(summaries),
            }
            # ä¼˜å…ˆä½¿ç”¨ chunks
            for c in chunks:
                text = str(c.get("content", "")).strip()
                if text:
                    contexts_all.append(text)
            # ç„¶åæ˜¯ statements
            for s in statements:
                text = str(s.get("statement", "")).strip()
                if text:
                    contexts_all.append(text)
            # ç„¶åæ˜¯ summaries
            for sm in summaries:
                text = str(sm.get("summary", "")).strip()
                if text:
                    contexts_all.append(text)
            # å®ä½“æ‘˜è¦ï¼šæœ€å¤šåŠ å…¥å‰3ä¸ªé«˜åˆ†å®ä½“ï¼ˆå¯¹é½ qwen_search_eval.pyï¼‰
            scored = [e for e in entities if e.get("score") is not None]
            top_entities = sorted(scored, key=lambda x: x.get("score", 0), reverse=True)[:3] if scored else entities[:3]
            if top_entities:
                summary_lines = []
                for e in top_entities:
                    name = str(e.get("name", "")).strip()
                    etype = str(e.get("entity_type", "")).strip()
                    score = e.get("score")
                    if name:
                        meta = []
                        if etype:
                            meta.append(f"type={etype}")
                        if isinstance(score, (int, float)):
                            meta.append(f"score={score:.3f}")
                        summary_lines.append(f"EntitySummary: {name}{(' [' + '; '.join(meta) + ']') if meta else ''}")
                if summary_lines:
                    contexts_all.append("\n".join(summary_lines))

        if verbose:
            if retrieved_counts:
                print(f"âœ… æ£€ç´¢æˆåŠŸ: {retrieved_counts.get('chunks',0)} chunks, {retrieved_counts.get('statements',0)} æ¡é™ˆè¿°, {retrieved_counts.get('entities',0)} ä¸ªå®ä½“, {retrieved_counts.get('summaries',0)} ä¸ªæ‘˜è¦")
            print(f"ğŸ“Š æœ‰æ•ˆä¸Šä¸‹æ–‡æ•°é‡: {len(contexts_all)}")
            q_keywords = extract_question_keywords(question, max_keywords=8)
            if q_keywords:
                print(f"ğŸ” é—®é¢˜å…³é”®è¯: {set(q_keywords)}")
            if contexts_all:
                analysis = analyze_contexts_simple(contexts_all, q_keywords, top_n=5)
                if analysis:
                    print("ğŸ“Š ä¸Šä¸‹æ–‡ç›¸å…³æ€§åˆ†æ:")
                    for a in analysis:
                        print(f"  - å¾—åˆ†: {int(a['score'])}, å…³é”®è¯åŒ¹é…: {a['match']}, é•¿åº¦: {a['length']}")
                # æ‰“å°æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡é¢„è§ˆï¼Œä¾¿äºå®šä½ä¸ºä½•ä¸º Unknown
                print("ğŸ” ä¸Šä¸‹æ–‡é¢„è§ˆï¼ˆæœ€å¤šå‰10æ¡ï¼Œæ¯æ¡æˆªæ–­å±•ç¤ºï¼‰:")
                for i, ctx in enumerate(contexts_all[:10]):
                    preview = str(ctx).replace("\n", " ")
                    if len(preview) > 300:
                        preview = preview[:300] + "..."
                    print(f"  [{i+1}] é•¿åº¦: {len(ctx)} | ç‰‡æ®µ: {preview}")
                # æ ‡æ³¨å‚è€ƒç­”æ¡ˆæ˜¯å¦å‡ºç°åœ¨ä»»ä¸€ä¸Šä¸‹æ–‡ä¸­
                ref_lower = (str(reference) or "").lower()
                if ref_lower:
                    hits = []
                    for i, ctx in enumerate(contexts_all):
                        if ref_lower in str(ctx).lower():
                            hits.append(i+1)
                    print(f"ğŸ”— å‚è€ƒç­”æ¡ˆå‘½ä¸­ä¸Šä¸‹æ–‡æ¡æ•°: {len(hits)}" + (f" | å‘½ä¸­ç´¢å¼•: {hits}" if hits else ""))

        context_text = smart_context_selection(contexts_all, question, max_chars=context_char_budget) if contexts_all else ""
        if not context_text:
            context_text = "No relevant context found."
        contexts_used.append(context_text)
        per_query_context_chars.append(len(context_text))
        per_query_context_counts.append(len(contexts_all))

        if verbose:
            selected_count = (context_text.count("\n\n") + 1) if context_text else 0
            print(f"âœ… æ™ºèƒ½é€‰æ‹©: {selected_count}ä¸ªä¸Šä¸‹æ–‡, æ€»é•¿åº¦: {len(context_text)}å­—ç¬¦")
            # å±•ç¤ºæ‹¼æ¥åçš„ä¸Šä¸‹æ–‡ç‰‡æ®µï¼Œä¾¿äºæ ¸æŸ¥æ˜¯å¦åŒ…å«ç­”æ¡ˆ
            concat_preview = context_text.replace("\n", " ")
            if len(concat_preview) > 600:
                concat_preview = concat_preview[:600] + "..."
            print(f"ğŸ§µ æ‹¼æ¥ä¸Šä¸‹æ–‡é¢„è§ˆ: {concat_preview}")

        messages = [
            {
                "role": "system",
                "content": (
                    "You are a QA assistant. Answer in English. Follow these guidelines:\n"
                    "1) If the context contains information to answer the question, provide a concise answer based on the context;\n"
                    "2) If the context does not contain enough information to answer the question, respond with 'Unknown';\n"
                    "3) Keep your answer brief and to the point;\n"
                    "4) Do not add explanations or additional text beyond the answer."
                ),
            },
            {"role": "user", "content": f"Question: {question}\n\nContext:\n{context_text}"},
        ]

        t2 = time.time()
        try:
            # ä½¿ç”¨å¼‚æ­¥è°ƒç”¨
            resp = await llm.chat(messages=messages)
            # æ›´å¥å£®çš„å“åº”è§£æï¼Œå¤„ç†ä¸åŒçš„LLMå“åº”æ ¼å¼
            if hasattr(resp, 'content'):
                pred = resp.content.strip()
            elif isinstance(resp, dict) and "choices" in resp and len(resp["choices"]) > 0:
                pred = resp["choices"][0]["message"]["content"].strip()
            elif isinstance(resp, dict) and "content" in resp:
                pred = resp["content"].strip()
            elif isinstance(resp, str):
                pred = resp.strip()
            else:
                pred = "Unknown"
                print(f"âš ï¸  LLMå“åº”æ ¼å¼å¼‚å¸¸: {type(resp)} - {resp}")

            # æ£€æŸ¥é¢„æµ‹æ˜¯å¦ä¸º"Unknown"æˆ–ç©ºï¼Œå¦‚æœæ˜¯åˆ™æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦çœŸçš„æ²¡æœ‰ç­”æ¡ˆ
            if pred.lower() in ["unknown", ""]:
                # å¦‚æœå‚è€ƒç­”æ¡ˆåœ¨ä¸Šä¸‹æ–‡ä¸­å­˜åœ¨ï¼Œä½†LLMè¿”å›Unknownï¼Œå¯èƒ½æ˜¯æç¤ºè¯é—®é¢˜
                ref_lower = (str(reference) or "").lower()
                if ref_lower and any(ref_lower in ctx.lower() for ctx in contexts_all):
                    print("âš ï¸  å‚è€ƒç­”æ¡ˆåœ¨ä¸Šä¸‹æ–‡ä¸­å­˜åœ¨ä½†LLMè¿”å›Unknownï¼Œæ£€æŸ¥æç¤ºè¯")
        except Exception as e:
            # æ›´è¯¦ç»†çš„é”™è¯¯å¤„ç†
            pred = "Unknown"
            print(f"âš ï¸  LLMè°ƒç”¨å¼‚å¸¸: {e}")
        t3 = time.time()
        llm_ms = (t3 - t2) * 1000
        latencies_llm.append(llm_ms)

        exact = exact_match(pred, reference)
        correct_flags.append(exact)
        f1_val = f1_score(str(pred), str(reference))
        b1_val = bleu1(str(pred), str(reference))
        j_val = jaccard(str(pred), str(reference))
        f1s.append(f1_val)
        b1s.append(b1_val)
        jss.append(j_val)

        if verbose:
            print(f"ğŸ¤– LLM å›ç­”: {pred}")
            print(f"âœ… æ­£ç¡®ç­”æ¡ˆ: {reference}")
            print(f"ğŸ“ˆ å½“å‰æŒ‡æ ‡ - F1: {f1_val:.3f}, BLEU-1: {b1_val:.3f}, Jaccard: {j_val:.3f}")
            print(f"â±ï¸ å»¶è¿Ÿ - æ£€ç´¢: {search_ms:.0f}ms, LLM: {llm_ms:.0f}ms")

        # å¯¹é½ locomo/qwen_search_eval.py çš„æ ·æœ¬è¾“å‡ºç»“æ„
        samples.append({
            "question": str(question),
            "answer": str(reference),
            "prediction": str(pred),
            "metrics": {
                "f1": f1_val,
                "b1": b1_val,
                "j": j_val
            },
            "retrieval": {
                "retrieved_documents": len(contexts_all),
                "context_length": len(context_text),
                "search_limit": search_limit,
                "max_chars": context_char_budget
            },
            "timing": {
                "search_ms": search_ms,
                "llm_ms": llm_ms
            }
        })

    # è®¡ç®—æ€»ä½“æŒ‡æ ‡ä¸èšåˆ
    acc = sum(correct_flags) / max(len(correct_flags), 1)
    ctx_avg_tokens = avg_context_tokens(contexts_used)
    result = {
        "dataset": "memsciqa",
        "items": len(items),
        "metrics": {
            "f1": (sum(f1s) / max(len(f1s), 1)) if f1s else 0.0,
            "b1": (sum(b1s) / max(len(b1s), 1)) if b1s else 0.0,
            "j": (sum(jss) / max(len(jss), 1)) if jss else 0.0,
        },
        "context": {
            "avg_tokens": ctx_avg_tokens,
            "avg_chars": (sum(per_query_context_chars) / max(len(per_query_context_chars), 1)) if per_query_context_chars else 0.0,
            "count_avg": (sum(per_query_context_counts) / max(len(per_query_context_counts), 1)) if per_query_context_counts else 0.0,
            "avg_memory_tokens": 0.0
        },
        "latency": {
            "search": latency_stats(latencies_search),
            "llm": latency_stats(latencies_llm),
        },
        "samples": samples,
        "params": {
            "group_id": group_id,
            "search_limit": search_limit,
            "context_char_budget": context_char_budget,
            "llm_temperature": llm_temperature,
            "llm_max_tokens": llm_max_tokens,
            "search_type": search_type,
            "start_index": start_index,
            "llm_id": SELECTED_LLM_ID,
            "retrieval_embedding_id": SELECTED_EMBEDDING_ID
        },
        "timestamp": datetime.now().isoformat(),
    }
    try:
        await connector.close()
    except Exception:
        pass
    return result


def main():
    load_dotenv()
    parser = argparse.ArgumentParser(description="memsciqa æµ‹è¯•è„šæœ¬ï¼ˆä¸‰è·¯æ£€ç´¢ + æ™ºèƒ½ä¸Šä¸‹æ–‡é€‰æ‹©ï¼‰")
    parser.add_argument("--sample-size", type=int, default=30, help="æ ·æœ¬æ•°é‡ï¼ˆ<=0 è¡¨ç¤ºå…¨éƒ¨ï¼‰")
    parser.add_argument("--all", action="store_true", help="è¯„ä¼°å…¨éƒ¨æ ·æœ¬ï¼ˆè¦†ç›– --sample-sizeï¼‰")
    parser.add_argument("--start-index", type=int, default=0, help="èµ·å§‹æ ·æœ¬ç´¢å¼•")
    parser.add_argument("--group-id", type=str, default="group_memsci", help="å›¾æ•°æ®åº“ Group IDï¼ˆé»˜è®¤ group_memsciï¼‰")
    parser.add_argument("--search-limit", type=int, default=8, help="æ£€ç´¢æ¡æ•°ä¸Šé™")
    parser.add_argument("--context-char-budget", type=int, default=4000, help="ä¸Šä¸‹æ–‡å­—ç¬¦é¢„ç®—")
    parser.add_argument("--llm-temperature", type=float, default=0.0, help="LLM æ¸©åº¦")
    parser.add_argument("--llm-max-tokens", type=int, default=64, help="LLM æœ€å¤§è¾“å‡º token")
    parser.add_argument("--search-type", type=str, default="embedding", choices=["embedding","keyword","hybrid"], help="æ£€ç´¢ç±»å‹ï¼ˆhybrid ç­‰åŒäº embeddingï¼‰")
    parser.add_argument("--data-path", type=str, default=None, help="æ•°æ®é›†è·¯å¾„ï¼ˆé»˜è®¤ data/msc_self_instruct.jsonlï¼‰")
    parser.add_argument("--output", type=str, default=None, help="å°†è¯„ä¼°ç»“æœä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶è·¯å¾„ï¼ˆJSONï¼‰")
    parser.add_argument("--verbose", action="store_true", default=True, help="æ‰“å°è¿‡ç¨‹æ—¥å¿—ï¼ˆé»˜è®¤å¼€å¯ï¼‰")
    parser.add_argument("--quiet", action="store_true", help="å…³é—­è¿‡ç¨‹æ—¥å¿—")
    args = parser.parse_args()

    sample_size = 0 if args.all else args.sample_size

    verbose_flag = False if args.quiet else args.verbose
    result = asyncio.run(
        run_memsciqa_test(
            sample_size=sample_size,
            group_id=args.group_id,
            search_limit=args.search_limit,
            context_char_budget=args.context_char_budget,
            llm_temperature=args.llm_temperature,
            llm_max_tokens=args.llm_max_tokens,
            search_type=args.search_type,
            data_path=args.data_path,
            start_index=args.start_index,
            verbose=verbose_flag,
        )
    )

    print(json.dumps(result, ensure_ascii=False, indent=2))

    # ç»“æœä¿å­˜
    out_path = args.output
    if not out_path:
        eval_dir = os.path.dirname(os.path.abspath(__file__))
        dataset_results_dir = os.path.join(eval_dir, "results")
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        out_path = os.path.join(dataset_results_dir, f"memsciqa_{result['params']['search_type']}_{ts}.json")
    try:
        os.makedirs(os.path.dirname(out_path), exist_ok=True)
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {out_path}")
    except Exception as e:
        print(f"âš ï¸ ç»“æœä¿å­˜å¤±è´¥: {e}")


if __name__ == "__main__":
    main()
