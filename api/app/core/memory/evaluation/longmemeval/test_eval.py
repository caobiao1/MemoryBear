import argparse
import asyncio
import json
import os
import time
import re
import statistics
from datetime import datetime, timedelta
from typing import List, Dict, Any

try:
    from dotenv import load_dotenv
except Exception:
    def load_dotenv():
        return None

# ä¸ç°æœ‰è¯„ä¼°è„šæœ¬ä¿æŒä¸€è‡´çš„å¯¼å…¥æ–¹å¼
from app.repositories.neo4j.neo4j_connector import Neo4jConnector
from app.repositories.neo4j.graph_search import search_graph, search_graph_by_embedding
from app.core.memory.llm_tools.openai_embedder import OpenAIEmbedderClient
from app.core.models.base import RedBearModelConfig
from app.core.memory.utils.config_utils import get_embedder_config
from app.core.memory.utils.llm_utils import get_llm_client
from app.core.memory.evaluation.dialogue_queries import SEARCH_ENTITIES_BY_NAME
from app.core.memory.utils.config.definitions import PROJECT_ROOT, SELECTED_LLM_ID, SELECTED_EMBEDDING_ID
from app.core.memory.evaluation.common.metrics import f1_score as common_f1, jaccard, latency_stats, avg_context_tokens
try:
    from app.core.memory.evaluation.common.metrics import exact_match
except Exception:
    # å…œåº•ï¼šç®€å•çš„å¤§å°å†™ä¸æ•æ„Ÿæ¯”è¾ƒ
    def exact_match(pred: str, ref: str) -> bool:
        return str(pred).strip().lower() == str(ref).strip().lower()


def load_dataset_any(path: str) -> List[Dict[str, Any]]:
    """å¥å£®åœ°åŠ è½½æ•°æ®é›†ï¼ˆå…¼å®¹ list æˆ–å¤šæ®µ JSONï¼‰ã€‚"""
    with open(path, "r", encoding="utf-8") as f:
        s = f.read().strip()
    try:
        obj = json.loads(s)
        if isinstance(obj, list):
            return obj
        elif isinstance(obj, dict):
            return [obj]
    except json.JSONDecodeError:
        pass
    dec = json.JSONDecoder()
    idx = 0
    items: List[Dict[str, Any]] = []
    while idx < len(s):
        while idx < len(s) and s[idx].isspace():
            idx += 1
        if idx >= len(s):
            break
        try:
            obj, end = dec.raw_decode(s, idx)
            if isinstance(obj, list):
                for it in obj:
                    if isinstance(it, dict):
                        items.append(it)
            elif isinstance(obj, dict):
                items.append(obj)
            idx = end
        except json.JSONDecodeError:
            nl = s.find("\n", idx)
            if nl == -1:
                break
            idx = nl + 1
    return items


def is_chinese_text(s: str) -> bool:
    return bool(re.search(r"[\u4e00-\u9fff]", s or ""))


def extract_candidate_options(question: str) -> List[str]:
    """ä»é—®é¢˜ä¸­æå–å€™é€‰é€‰é¡¹ï¼ˆA-or-B ç±»é—®é¢˜ï¼‰ã€‚"""
    q = (question or "").strip()
    options: List[str] = []

    # 1) å¼•å·åŒ…è£¹çš„ç‰‡æ®µ
    for pat in [r"'([^']+)'", r'\"([^\"]+)\"', r'â€œ([^â€]+)â€', r'â€˜([^â€™]+)â€™']:
        for m in re.findall(pat, q):
            val = (m or "").strip()
            if val:
                options.append(val)

    # 2) or/è¿˜æ˜¯/æˆ–è€… è¿æ¥è¯
    if len(options) < 2:
        pats = [
            r"([^,;ï¼Œï¼›]+?)\s+or\s+([^,;ï¼Œï¼›\?\.!.ã€‚ï¼]+)",
            r"([^,;ï¼Œï¼›]+?)\s+è¿˜æ˜¯\s+([^,;ï¼Œï¼›\?\.!.ã€‚ï¼]+)",
            r"([^,;ï¼Œï¼›]+?)\s+æˆ–è€…\s+([^,;ï¼Œï¼›\?\.!.ã€‚ï¼]+)",
        ]
        for pat in pats:
            matches = list(re.finditer(pat, q, flags=re.IGNORECASE))
            if matches:
                m = matches[-1]
                cand1 = m.group(1).strip().strip("?ï¼Ÿ.,ï¼Œ;ï¼› ")
                cand2 = m.group(2).strip().strip("?ï¼Ÿ.,ï¼Œ;ï¼› ")
                options.extend([cand1, cand2])
                break

    # å»é‡
    seen = set()
    uniq: List[str] = []
    for o in options:
        o2 = o.strip()
        key = o2.lower() if not is_chinese_text(o2) else o2
        if o2 and key not in seen:
            uniq.append(o2)
            seen.add(key)
    return uniq


def extract_time_entities(text: str) -> List[Dict[str, Any]]:
    """å¢å¼ºæ—¶é—´å®ä½“æå–ï¼Œä¸“é—¨ç”¨äºæ—¶é—´æ¨ç†é—®é¢˜"""
    time_entities = []

    # æ—¥æœŸæ¨¡å¼
    date_patterns = [
        (r'\b(\d{4})-(\d{1,2})-(\d{1,2})\b', 'date'),  # YYYY-MM-DD
        (r'\b(\d{1,2})æœˆ(\d{1,2})æ—¥\b', 'date'),  # ä¸­æ–‡æ—¥æœŸ
        (r'\b(January|February|March|April|May|June|July|August|September|October|November|December)\s+(\d{1,2}),?\s+(\d{4})?', 'date'),  # è‹±æ–‡æœˆä»½
        (r'\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+(\d{1,2}),?\s+(\d{4})?', 'date'),  # è‹±æ–‡æœˆä»½ç¼©å†™
    ]

    # æ—¶é—´é—´éš”æ¨¡å¼
    duration_patterns = [
        (r'(\d+)\s*å¤©', 'days'),
        (r'(\d+)\s*å‘¨', 'weeks'),
        (r'(\d+)\s*ä¸ªæœˆ', 'months'),
        (r'(\d+)\s*å¹´', 'years'),
        (r'(\d+)\s*days?', 'days'),
        (r'(\d+)\s*weeks?', 'weeks'),
        (r'(\d+)\s*months?', 'months'),
        (r'(\d+)\s*years?', 'years'),
    ]

    # äº‹ä»¶æ—¶é—´å…³ç³»æ¨¡å¼
    temporal_relation_patterns = [
        (r'(ä¹‹å‰|ä»¥å‰|å‰)\s*(\d+)\s*å¤©', 'days_before'),
        (r'(ä¹‹å|ä»¥å|å)\s*(\d+)\s*å¤©', 'days_after'),
        (r'(\d+)\s*å¤©\s*(ä¹‹å‰|ä»¥å‰|å‰)', 'days_before'),
        (r'(\d+)\s*å¤©\s*(ä¹‹å|ä»¥å|å)', 'days_after'),
        (r'(\d+)\s*days?\s*(before|ago)', 'days_before'),
        (r'(\d+)\s*days?\s*(after|later)', 'days_after'),
    ]

    # æå–æ—¥æœŸ
    for pattern, entity_type in date_patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            time_entities.append({
                'text': match.group(),
                'type': entity_type,
                'start': match.start(),
                'end': match.end()
            })

    # æå–æ—¶é—´é—´éš”
    for pattern, entity_type in duration_patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            time_entities.append({
                'text': match.group(),
                'type': entity_type,
                'value': int(match.group(1)),
                'start': match.start(),
                'end': match.end()
            })

    # æå–æ—¶é—´å…³ç³»
    for pattern, entity_type in temporal_relation_patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            time_entities.append({
                'text': match.group(),
                'type': entity_type,
                'value': int(match.group(2)) if match.groups() >= 2 else int(match.group(1)),
                'start': match.start(),
                'end': match.end()
            })

    return time_entities


def calculate_time_difference(date1: str, date2: str) -> int:
    """è®¡ç®—ä¸¤ä¸ªæ—¥æœŸä¹‹é—´çš„å¤©æ•°å·®"""
    try:
        # è§£ææ—¥æœŸæ ¼å¼
        def parse_date(date_str: str) -> datetime:
            # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
            formats = [
                '%Y-%m-%d',
                '%mæœˆ%dæ—¥',
                '%B %d, %Y',
                '%b %d, %Y',
                '%Yå¹´%mæœˆ%dæ—¥'
            ]

            for fmt in formats:
                try:
                    return datetime.strptime(date_str, fmt)
                except ValueError:
                    continue

            # å¦‚æœéƒ½æ— æ³•è§£æï¼Œè¿”å›å½“å‰æ—¥æœŸ
            return datetime.now()

        d1 = parse_date(date1)
        d2 = parse_date(date2)

        # è®¡ç®—å¤©æ•°å·®ï¼ˆç»å¯¹å€¼ï¼‰
        return abs((d2 - d1).days)
    except Exception:
        return -1  # è¡¨ç¤ºè®¡ç®—å¤±è´¥


def _extract_cn_tokens(text: str) -> List[str]:
    """ä¸­æ–‡å…³é”®è¯æå–ï¼ˆçŸ­è¯­çº§ï¼Œå«æ•°è¯/æ—¥æœŸ/å¸¸è§é¢†åŸŸè¯ï¼‰"""
    if not text:
        return []
    t = str(text)
    # å»æ‰å¸¸è§åŠŸèƒ½è¯ï¼ˆç²—ç•¥ï¼Œä¸ä¾èµ–åˆ†è¯åº“ï¼‰
    stop_words = [
        "æˆ‘","æˆ‘ä»¬","ä½ ","ä»–","å¥¹","å®ƒ","è¿™","é‚£","å“ª","ä¸€ä¸ª","ä¸€æ¬¡","ä¸€äº›","ä»€ä¹ˆ","æ€ä¹ˆ","æ˜¯å¦","å—","å‘¢",
        "å¾ˆ","æ›´","æœ€","å·²ç»","æ­£åœ¨","å°†è¦","é©¬ä¸Š","å°½å¿«","æœ€è¿‘","å…³äº","æœ‰å…³","ä»¥åŠ","å¹¶ä¸”","æˆ–è€…","è¿˜æ˜¯",
        "å› ä¸º","æ‰€ä»¥","å¦‚æœ","ä½†æ˜¯","è€Œä¸”","ç„¶å","ä¹‹å","ä¹‹å‰","åŒæ—¶","å¦å¤–","å¹¶","ä½†","å´","è¢«","æŠŠ","è®©","ç»™",
        "å’Œ","ä¸","è·Ÿ","åŠ","è¿˜æœ‰","å°±","éƒ½","åœ¨","å¯¹","å¯¹äº","çš„","äº†","ç€","è¿‡","åˆ°","äº","ä»","ä»¥","ä¸º","å‘","è‡³","æ˜¯"
    ]
    for sw in stop_words:
        t = t.replace(sw, " ")
    # å»æ ‡ç‚¹
    t = re.sub(r"[ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š,.!?;:\"'ï¼ˆï¼‰()ï¼»ï¼½\[\]\-â€”â€¦Â·]", " ", t)
    # åŸºç¡€ä¸­æ–‡ç‰‡æ®µï¼ˆ>=2ï¼‰
    base = re.findall(r"[\u4e00-\u9fff]{2,}", t)
    # ç‰¹æ®Šç»„åˆï¼šç¬¬Xæ¬¡XXXX
    specials = re.findall(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ¬¡[\u4e00-\u9fff]{2,6}", text)
    # æ—¥æœŸä¸æ•°å­—
    dates = re.findall(r"\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥|\d{1,2}æœˆ\d{1,2}æ—¥|\d{4}-\d{1,2}-\d{1,2}", text)
    numbers = re.findall(r"\b\d+\b", text)

    generic = {"å»ºè®®","æ¨è","å¸®åŠ©","æå‡","æŠ€èƒ½","æœ‰æ•ˆ","å›¢é˜Ÿ","å‚ä¸åº¦","å–œæ¬¢","å¼€å§‹"}
    tokens: List[str] = specials + base + dates + numbers
    uniq: List[str] = []
    seen = set()
    for tok in tokens:
        tok2 = tok.strip()
        if len(tok2) < 2 or len(tok2) > 6:
            continue
        if tok2 in generic:
            continue
        if tok2 not in seen:
            uniq.append(tok2)
            seen.add(tok2)
    # æ’é™¤å¸¸è§ç–‘é—®å‹çŸ­è¯­
    blacklist_exact = {"æ˜¯ä»€ä¹ˆ","å¤šå°‘","å¤šå°‘å¤©","å“ªä¸ª","å“ªäº›","ä¹‹é—´","å…ˆ","å","ä¹‹å‰","ä¹‹å"}
    uniq2: List[str] = [u for u in uniq if u not in blacklist_exact]
    return uniq2[:12]


def generate_query_keywords_cn(question: str) -> List[str]:
    """å¢å¼ºç‰ˆå…³é”®è¯æå–ï¼Œç‰¹åˆ«å…³æ³¨æŠ€æœ¯æœ¯è¯­å’Œä¸“æœ‰åè¯"""
    if not question:
        return []

    # æå–ä¸“æœ‰åè¯ï¼ˆå¸¦å¼•å·çš„å†…å®¹ï¼‰
    quoted_terms = re.findall(r'["""]([^"""]+)["""]', question)

    # æå–æŠ€æœ¯æœ¯è¯­ï¼ˆä¸­è‹±æ–‡æ··åˆï¼‰
    tech_terms = re.findall(r'[A-Z][a-zA-Z]+\s+[A-Z][a-zA-Z]+|[A-Za-z]+[\u4e00-\u9fff]+|[\u4e00-\u9fff]+[A-Za-z]+', question)

    # æå–æ ¸å¿ƒåè¯çŸ­è¯­
    core_nouns = re.findall(r'[\u4e00-\u9fff]{2,5}ç³»ç»Ÿ|[\u4e00-\u9fff]{2,5}ç®¡ç†|[\u4e00-\u9fff]{2,5}åˆ†æ|[\u4e00-\u9fff]{2,5}å·¥ä½œåŠ|[\u4e00-\u9fff]{2,5}ç ”è®¨ä¼š', question)

    # åŸºç¡€ä¸­æ–‡ç‰‡æ®µ
    base_tokens = _extract_cn_tokens(question)

    # ç‰¹å®šé¢†åŸŸå…³é”®è¯å¢å¼º
    domain_keywords = []
    # GPSç›¸å…³
    if any(term in question for term in ["GPS", "å¯¼èˆª", "å®šä½ç³»ç»Ÿ", "ç³»ç»Ÿè¿è¡Œ"]):
        domain_keywords.extend(["GPS", "å¯¼èˆªç³»ç»Ÿ", "å®šä½", "ç³»ç»Ÿæ•…éšœ", "åŠŸèƒ½å¼‚å¸¸"])
    # æ´»åŠ¨ç›¸å…³
    if any(term in question for term in ["å·¥ä½œåŠ", "ç ”è®¨ä¼š", "ç½‘ç»œç ”è®¨ä¼š", "æ´»åŠ¨"]):
        domain_keywords.extend(["å·¥ä½œåŠ", "ç ”è®¨ä¼š", "å‚åŠ ", "å‚ä¸", "æ´»åŠ¨"])
    # æ—¶é—´é¡ºåºç›¸å…³
    if any(term in question for term in ["å…ˆ", "å", "ç¬¬ä¸€ä¸ª", "ä¹‹å‰", "é¦–å…ˆ"]):
        domain_keywords.extend(["å…ˆ", "å", "ä¹‹å‰", "ä¹‹å", "ç¬¬ä¸€æ¬¡", "é¦–å…ˆ"])
    # è®¾å¤‡ç›¸å…³
    if any(term in question for term in ["è®¾å¤‡", "æ‰‹æœº", "ç”µè„‘", "ç¬”è®°æœ¬ç”µè„‘"]):
        domain_keywords.extend(["è®¾å¤‡", "æ‰‹æœº", "ç”µè„‘", "ç¬”è®°æœ¬ç”µè„‘", "è´­ä¹°"])

    # åˆå¹¶å¹¶å»é‡
    all_tokens = quoted_terms + tech_terms + core_nouns + base_tokens + domain_keywords
    seen = set()
    final_tokens = []

    for token in all_tokens:
        token = token.strip()
        if len(token) >= 2 and token not in seen:
            final_tokens.append(token)
            seen.add(token)

    return final_tokens[:8]


def smart_context_selection(contexts: List[str], question: str, max_chars: int = 4000) -> str:
    """å¢å¼ºç‰ˆä¸Šä¸‹æ–‡é€‰æ‹©ï¼šç‰¹åˆ«ä¼˜åŒ–æŠ€æœ¯æœ¯è¯­å’Œç²¾ç¡®åŒ¹é…"""
    if not contexts:
        return ""

    # æ£€æµ‹æ˜¯å¦ä¸ºæ—¶é—´æ¨ç†é—®é¢˜
    is_temporal_question = any(keyword in question.lower() for keyword in
                              ['days', 'day', 'before', 'after', 'first', 'å…ˆå', 'é¡ºåº', 'é—´éš”', 'å¤šä¹…', 'å¤šå°‘å¤©'])

    # æå–æ—¶é—´å®ä½“ä»é—®é¢˜ä¸­
    question_time_entities = extract_time_entities(question)

    # æå–å…³é”®æŠ€æœ¯å®ä½“
    key_entities = []
    # GPSç›¸å…³
    if any(term in question for term in ["GPS", "å¯¼èˆª", "å®šä½ç³»ç»Ÿ", "ç³»ç»Ÿè¿è¡Œ"]):
        key_entities.extend(["GPS", "å¯¼èˆª", "å®šä½", "ç³»ç»Ÿ", "åŠŸèƒ½", "é—®é¢˜", "æ•…éšœ"])
    # æ´»åŠ¨ç›¸å…³
    if any(term in question for term in ["å·¥ä½œåŠ", "ç ”è®¨ä¼š", "ç½‘ç»œç ”è®¨ä¼š", "æ´»åŠ¨"]):
        key_entities.extend(["å·¥ä½œåŠ", "ç ”è®¨ä¼š", "å‚åŠ ", "å‚ä¸", "æ´»åŠ¨", "æ—¶é—´"])
    # æ—¶é—´é¡ºåºç›¸å…³
    if any(term in question for term in ["å…ˆ", "å", "ç¬¬ä¸€ä¸ª", "ä¹‹å‰", "é¦–å…ˆ"]):
        key_entities.extend(["å…ˆ", "å", "ä¹‹å‰", "ä¹‹å", "ç¬¬ä¸€æ¬¡", "é¦–å…ˆ"])

    # è‹±æ–‡å…³é”®è¯ï¼ˆå»åœç”¨è¯ï¼‰
    question_lower = question.lower()
    stop_words = {
        'what','when','where','who','why','how','did','do','does','is','are','was','were',
        'the','a','an','and','or','but','many','which','first'
    }
    eng_words = [w for w in set(re.findall(r'\b\w+\b', question_lower))
                if w not in stop_words and len(w) > 2]

    # ä¸­æ–‡ç‰‡æ®µä¸å€™é€‰é€‰é¡¹
    cn_tokens = generate_query_keywords_cn(question)
    options = extract_candidate_options(question)

    # æ—¶é—´æ¨ç†é—®é¢˜çš„ç‰¹æ®Šå¤„ç†
    if is_temporal_question:
        # ä¸ºæ—¶é—´é—®é¢˜æ·»åŠ æ—¶é—´ç›¸å…³å…³é”®è¯
        time_keywords = ['å¤©', 'æ—¥', 'æœˆ', 'å¹´', 'before', 'after', 'days', 'first', 'å…ˆå']
        eng_words = [w for w in eng_words if w not in ['days', 'first']]  # é¿å…é‡å¤
        cn_tokens.extend([kw for kw in time_keywords if kw not in cn_tokens])

        # é™åˆ¶å…³é”®è¯æ•°é‡ï¼Œä¼˜å…ˆæ—¶é—´ç›¸å…³
        tokens = time_keywords[:2] + key_entities[:3] + cn_tokens[:2] + eng_words[:1] + options[:1]
    else:
        # å¸¸è§„é—®é¢˜å¤„ç†ï¼Œä¼˜å…ˆå…³é”®æŠ€æœ¯å®ä½“
        tokens = key_entities[:4] + cn_tokens[:3] + options[:2] + eng_words[:1]

    # å»é‡
    seen = set()
    final_tokens: List[str] = []
    for t in tokens:
        t2 = t.strip()
        if t2 and t2 not in seen:
            final_tokens.append(t2)
            seen.add(t2)

    scored_contexts: List[tuple[float, str]] = []

    # å…³é”®æŠ€æœ¯å®ä½“æƒé‡æ˜ å°„
    key_entity_weights = {
        "GPS": 3.0, "å¯¼èˆª": 2.5, "ç³»ç»Ÿ": 2.0, "åŠŸèƒ½": 2.0, "é—®é¢˜": 2.0, "æ•…éšœ": 2.5,
        "å·¥ä½œåŠ": 2.5, "ç ”è®¨ä¼š": 2.5, "å‚åŠ ": 2.0, "å‚ä¸": 2.0,
        "å…ˆ": 2.0, "å": 2.0, "ä¹‹å‰": 2.0, "ä¹‹å": 2.0, "ç¬¬ä¸€æ¬¡": 2.5
    }

    # æ—¶é—´æ¨ç†é—®é¢˜çš„æƒé‡æ˜ å°„
    temporal_weight_map = {
        "å¤©": 2.0, "æ—¥": 2.0, "æœˆ": 1.8, "å¹´": 1.8, "days": 2.0,
        "before": 1.5, "after": 1.5, "first": 1.5, "å…ˆå": 1.5
    }

    # å¸¸è§„é—®é¢˜çš„æƒé‡æ˜ å°„
    normal_weight_map = {
        "é—®é¢˜": 2.0, "æ•…éšœ": 2.0, "å¼‚å¸¸": 1.8, "ä¸æ­£å¸¸": 1.8, "åäº†": 1.8,
        "ç³»ç»Ÿ": 1.3, "GPS": 1.5, "ä¿å…»": 1.4, "è®¾å¤‡": 1.2, "æ¨¡å—": 1.2, "åŠŸèƒ½": 1.1
    }

    # åˆå¹¶æƒé‡æ˜ å°„
    weight_map = {**normal_weight_map, **temporal_weight_map, **key_entity_weights}

    for i, context in enumerate(contexts):
        context_str = str(context)
        lines = re.split(r'[\r\n]+', context_str)
        hit_lines: List[str] = []
        kw_hits: float = 0.0
        time_entity_count = 0
        key_entity_hits = 0

        for line in lines:
            ln = line.strip()
            if not ln:
                continue

            has_keyword = False
            # å…³é”®è¯åŒ¹é…
            for tok in final_tokens:
                if tok and tok in ln:
                    w = weight_map.get(tok, 1.0)
                    hit_count = ln.count(tok)
                    kw_hits += hit_count * w
                    # å…³é”®æŠ€æœ¯å®ä½“é¢å¤–å¥–åŠ±
                    if tok in key_entity_weights:
                        key_entity_hits += hit_count
                    has_keyword = True

            # æ—¶é—´å®ä½“æ£€æµ‹ï¼ˆç‰¹åˆ«é’ˆå¯¹æ—¶é—´æ¨ç†é—®é¢˜ï¼‰
            if is_temporal_question:
                time_entities = extract_time_entities(ln)
                time_entity_count += len(time_entities)
                if time_entities:
                    has_keyword = True

            # ç²¾ç¡®åŒ¹é…å¥–åŠ±ï¼ˆå®Œæ•´é—®é¢˜å…³é”®è¯å‡ºç°åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼‰
            for q_word in question.split():
                if len(q_word) > 3 and q_word in ln:
                    kw_hits += 0.5  # ç²¾ç¡®åŒ¹é…å¥–åŠ±

            if has_keyword:
                # å¯¹äºåŒ…å«å…³é”®ä¿¡æ¯çš„è¡Œï¼Œä¿ç•™å®Œæ•´è¡Œ
                hit_lines.append(ln)

        snippet = "\n".join(hit_lines) if hit_lines else context_str.strip()

        # é™åˆ¶å•æ®µé•¿åº¦ï¼Œä½†å¯¹åŒ…å«å…³é”®ä¿¡æ¯çš„ä¸Šä¸‹æ–‡ç¨å¾®æ”¾å®½é™åˆ¶
        max_snippet_len = 600 if (key_entity_hits > 0 or time_entity_count > 0) else 500
        if len(snippet) > max_snippet_len:
            snippet = snippet[:max_snippet_len]

        # è¯„åˆ†é€»è¾‘
        has_number = 1 if re.search(r'\d', snippet) else 0
        has_date = 1 if (re.search(r'\b\d{4}-\d{1,2}-\d{1,2}\b', snippet) or
                        re.search(r'\d{1,2}æœˆ\d{1,2}æ—¥', snippet)) else 0

        # å…³é”®æŠ€æœ¯å®ä½“å¥–åŠ±
        key_entity_bonus = key_entity_hits * 1.0

        # æ—¶é—´æ¨ç†é—®é¢˜çš„ç‰¹æ®Šè¯„åˆ†
        if is_temporal_question:
            time_bonus = time_entity_count * 2.0  # æ—¶é—´å®ä½“å¥–åŠ±
            temporal_coherence = 3 if (has_date and time_entity_count >= 2) else 0
        else:
            time_bonus = 0
            temporal_coherence = 0

        length_bonus = 5 if 50 < len(snippet) < 1000 else (2 if len(snippet) >= 1000 else 0)
        pos_bonus = 3 if i < 3 else 0

        score = (kw_hits * 0.8 + (has_number + has_date) * 1.5 +
                length_bonus + pos_bonus + time_bonus + temporal_coherence + key_entity_bonus)

        scored_contexts.append((score, snippet))

    # é€‰æ‹©ç´¯è®¡è‡³æ€»å­—ç¬¦é¢„ç®—
    scored_contexts.sort(key=lambda x: x[0], reverse=True)
    selected: List[str] = []
    total_chars = 0

    for score, snippet in scored_contexts:
        if total_chars + len(snippet) <= max_chars:
            selected.append(snippet)
            total_chars += len(snippet)
        else:
            if not selected and len(snippet) > max_chars:
                selected.append(snippet[:max_chars])
            break

    final_context = "\n\n".join(selected)

    # å¯¹äºæ—¶é—´æ¨ç†é—®é¢˜ï¼Œæ·»åŠ æ—¶é—´è®¡ç®—æç¤º
    if is_temporal_question and question_time_entities:
        time_prompt = "\n\n[æ—¶é—´æ¨ç†æç¤ºï¼šè¯·ä»”ç»†åˆ†æä¸Šè¿°ä¸Šä¸‹æ–‡ä¸­çš„æ—¥æœŸå’Œæ—¶é—´å…³ç³»ï¼Œè®¡ç®—æ—¶é—´é—´éš”æˆ–ç¡®å®šäº‹ä»¶é¡ºåº]"
        if total_chars + len(time_prompt) <= max_chars:
            final_context += time_prompt

    return final_context


# é€šè¿‡åˆ«ååŒ¹é…è¿›è¡Œå®ä½“å…³é”®è¯æ£€ç´¢ï¼ˆå¤štokenåˆå¹¶ï¼‰
async def _search_entities_by_aliases(connector: Neo4jConnector, tokens: List[str], group_id: str | None, limit: int) -> List[Dict[str, Any]]:
    results: List[Dict[str, Any]] = []
    try:
        for tok in tokens:
            rows = await connector.execute_query(SEARCH_ENTITIES_BY_NAME, q=tok, group_id=group_id, limit=limit)
            if rows:
                results.extend(rows)
    except Exception:
        pass

    # æŒ‰ name å»é‡
    deduped: List[Dict[str, Any]] = []
    seen = set()
    for r in results:
        k = str(r.get("name", ""))
        if k and k not in seen:
            deduped.append(r)
            seen.add(k)
    return deduped


# é€šè¿‡å¯¹è¯/é™ˆè¿°ä¸­çš„entity_idsåæŸ¥å®ä½“åç§°
_FETCH_ENTITIES_BY_IDS = """
MATCH (e:ExtractedEntity)
WHERE e.id IN $ids AND ($group_id IS NULL OR e.group_id = $group_id)
RETURN e.id AS id, e.name AS name, e.group_id AS group_id, e.entity_type AS entity_type
"""

async def _fetch_entities_by_ids(connector: Neo4jConnector, ids: List[str], group_id: str | None) -> List[Dict[str, Any]]:
    if not ids:
        return []
    try:
        rows = await connector.execute_query(_FETCH_ENTITIES_BY_IDS, ids=list({i for i in ids if i}), group_id=group_id)
        return rows or []
    except Exception:
        return []


# å¢å¼ºçš„æ—¶é—´å®ä½“æ£€ç´¢
_TIME_ENTITY_SEARCH = """
MATCH (e:ExtractedEntity)
WHERE e.entity_type CONTAINS "TIME" OR e.entity_type CONTAINS "DATE" OR e.name =~ $date_pattern
AND ($group_id IS NULL OR e.group_id = $group_id)
RETURN e.id AS id, e.name AS name, e.group_id AS group_id, e.entity_type AS entity_type
LIMIT $limit
"""

async def _search_time_entities(connector: Neo4jConnector, group_id: str | None, limit: int = 5) -> List[Dict[str, Any]]:
    """ä¸“é—¨æœç´¢æ—¶é—´ç›¸å…³çš„å®ä½“"""
    try:
        date_pattern = r".*\d{4}.*|.*\d{1,2}æœˆ\d{1,2}æ—¥.*"
        rows = await connector.execute_query(_TIME_ENTITY_SEARCH,
                                           date_pattern=date_pattern,
                                           group_id=group_id,
                                           limit=limit)
        return rows or []
    except Exception:
        return []


# æŠ€æœ¯æœ¯è¯­ä¸“é—¨æ£€ç´¢
async def _search_tech_terms(connector: Neo4jConnector, question: str, group_id: str | None, limit: int = 3) -> List[Dict[str, Any]]:
    """ä¸“é—¨æœç´¢æŠ€æœ¯æœ¯è¯­ç›¸å…³çš„å®ä½“"""
    tech_entities = []
    try:
        # GPSç›¸å…³
        if any(term in question for term in ["GPS", "å¯¼èˆª", "å®šä½ç³»ç»Ÿ"]):
            gps_rows = await connector.execute_query(SEARCH_ENTITIES_BY_NAME, q="GPS", group_id=group_id, limit=limit)
            if gps_rows:
                tech_entities.extend(gps_rows)

        # æ´»åŠ¨ç›¸å…³
        if any(term in question for term in ["å·¥ä½œåŠ", "ç ”è®¨ä¼š", "ç½‘ç»œç ”è®¨ä¼š"]):
            workshop_rows = await connector.execute_query(SEARCH_ENTITIES_BY_NAME, q="å·¥ä½œåŠ", group_id=group_id, limit=limit)
            if workshop_rows:
                tech_entities.extend(workshop_rows)

        # æ—¶é—´é¡ºåºç›¸å…³
        if any(term in question for term in ["å…ˆ", "å", "ç¬¬ä¸€ä¸ª"]):
            time_rows = await connector.execute_query(SEARCH_ENTITIES_BY_NAME, q="ç¬¬ä¸€æ¬¡", group_id=group_id, limit=limit)
            if time_rows:
                tech_entities.extend(time_rows)

    except Exception:
        pass

    return tech_entities


# ä¸­è‹±ç›¸å¯¹æ—¶é—´è§£æï¼štoday/æ˜¨å¤©/ä¸Šå‘¨/3å¤©å ç­‰ç®€å•å½’ä¸€åŒ–ä¸ºæ—¥æœŸ
def _resolve_relative_times_cn_en(text: str, anchor: datetime) -> str:
    t = str(text) if text is not None else ""
    # è‹±æ–‡ today/yesterday/tomorrow
    t = re.sub(r"\btoday\b", anchor.date().isoformat(), t, flags=re.IGNORECASE)
    t = re.sub(r"\byesterday\b", (anchor - timedelta(days=1)).date().isoformat(), t, flags=re.IGNORECASE)
    t = re.sub(r"\btomorrow\b", (anchor + timedelta(days=1)).date().isoformat(), t, flags=re.IGNORECASE)

    # è‹±æ–‡ X days ago / in X days
    def _ago_repl(m: re.Match[str]) -> str:
        n = int(m.group(1))
        return (anchor - timedelta(days=n)).date().isoformat()
    def _in_repl(m: re.Match[str]) -> str:
        n = int(m.group(1))
        return (anchor + timedelta(days=n)).date().isoformat()
    t = re.sub(r"\b(\d+)\s+days\s+ago\b", _ago_repl, t, flags=re.IGNORECASE)
    t = re.sub(r"\bin\s+(\d+)\s+days\b", _in_repl, t, flags=re.IGNORECASE)
    t = re.sub(r"\blast\s+week\b", (anchor - timedelta(days=7)).date().isoformat(), t, flags=re.IGNORECASE)
    t = re.sub(r"\bnext\s+week\b", (anchor + timedelta(days=7)).date().isoformat(), t, flags=re.IGNORECASE)

    # ä¸­æ–‡ ä»Šå¤©/æ˜¨å¤©/æ˜å¤©
    t = re.sub(r"ä»Šå¤©", anchor.date().isoformat(), t)
    t = re.sub(r"æ˜¨æ—¥|æ˜¨å¤©", (anchor - timedelta(days=1)).date().isoformat(), t)
    t = re.sub(r"æ˜å¤©", (anchor + timedelta(days=1)).date().isoformat(), t)
    # ä¸­æ–‡ Xå¤©å‰ / Xå¤©å
    t = re.sub(r"(\d+)å¤©å‰", lambda m: (anchor - timedelta(days=int(m.group(1)))).date().isoformat(), t)
    t = re.sub(r"(\d+)å¤©å", lambda m: (anchor + timedelta(days=int(m.group(1)))).date().isoformat(), t)
    # ä¸­æ–‡ ä¸Šå‘¨ / ä¸‹å‘¨ï¼ˆè¿‘ä¼¼7å¤©ï¼‰
    t = re.sub(r"ä¸Šå‘¨", (anchor - timedelta(days=7)).date().isoformat(), t)
    t = re.sub(r"ä¸‹å‘¨", (anchor + timedelta(days=7)).date().isoformat(), t)
    # ä¸­æ–‡ æœˆæ—¥ï¼ˆæ— å¹´ä»½ï¼‰è¡¥å…¨å¹´ä»½
    def _md_repl(m: re.Match[str]) -> str:
        mon = int(m.group(1)); day = int(m.group(2))
        return f"{anchor.year}-{mon:02d}-{day:02d}"
    t = re.sub(r"(\d{1,2})æœˆ(\d{1,2})æ—¥", _md_repl, t)
    return t


async def run_longmemeval_test(
    sample_size: int = 3,
    group_id: str = "longmemeval_zh_bak_2",
    search_limit: int = 8,
    context_char_budget: int = 4000,
    llm_temperature: float = 0.0,
    llm_max_tokens: int = 16,
    search_type: str = "hybrid",
    data_path: str | None = None,
    start_index: int = 0,
) -> Dict[str, Any]:
    """LongMemEval è¯„ä¼°æµ‹è¯•ï¼šå¢å¼ºæŠ€æœ¯æœ¯è¯­æ£€ç´¢èƒ½åŠ›"""

    # æ•°æ®è·¯å¾„
    if not data_path:
        # å›ºå®šä½¿ç”¨ä¸­æ–‡æ•°æ®é›†ï¼šdata/longmemeval_oracle_zh.json
        zh_proj = os.path.join(PROJECT_ROOT, "data", "longmemeval_oracle_zh.json")
        zh_cwd = os.path.join(os.getcwd(), "data", "longmemeval_oracle_zh.json")
        if os.path.exists(zh_proj):
            data_path = zh_proj
        elif os.path.exists(zh_cwd):
            data_path = zh_cwd
        else:
            raise FileNotFoundError("æœªæ‰¾åˆ°æ•°æ®é›†: data/longmemeval_oracle_zh.jsonï¼Œè¯·ç¡®ä¿å…¶å­˜åœ¨äºé¡¹ç›®æ ¹ç›®å½•æˆ–å½“å‰å·¥ä½œç›®å½•çš„ data ç›®å½•ä¸‹ã€‚")

    qa_list: List[Dict[str, Any]] = load_dataset_any(data_path)
    # æ”¯æŒè¯„ä¼°å…¨éƒ¨æ ·æœ¬ï¼šå½“ sample_size <= 0 æ—¶ï¼Œå–ä» start_index åˆ°æœ«å°¾
    if sample_size is None or sample_size <= 0:
        items = qa_list[start_index:]
    else:
        items = qa_list[start_index:start_index + sample_size]

    # åˆå§‹åŒ–ç»„ä»¶ - ä½¿ç”¨å¼‚æ­¥LLMå®¢æˆ·ç«¯
    llm_client = get_llm_client(SELECTED_LLM_ID)
    connector = Neo4jConnector()
    cfg_dict = get_embedder_config(SELECTED_EMBEDDING_ID)
    embedder = OpenAIEmbedderClient(
        model_config=RedBearModelConfig.model_validate(cfg_dict)
    )

    # æŒ‡æ ‡æ”¶é›†
    latencies_llm: List[float] = []
    latencies_search: List[float] = []
    per_query_context_counts: List[int] = []
    per_query_context_avg_tokens: List[float] = []
    per_query_context_chars: List[int] = []

    type_correct: Dict[str, List[float]] = {}
    type_f1: Dict[str, List[float]] = {}
    type_jacc: Dict[str, List[float]] = {}

    samples: List[Dict[str, Any]] = []
    # ç»Ÿè®¡é‡å¤çš„ä¸Šä¸‹æ–‡é¢„è§ˆï¼ˆè·¨æ ·æœ¬ï¼‰ï¼Œä¾¿äºè¯Šæ–­"ç›¸åŒä¸Šä¸‹æ–‡"é—®é¢˜
    preview_counter: Dict[str, int] = {}

    try:
        for item in items:
            question = item.get("question", "")
            reference = item.get("answer", "")
            qtype = item.get("question_type") or item.get("type", "unknown")

            print(f"\n=== å¤„ç†é—®é¢˜: {question} ===")

            # æ£€æµ‹é—®é¢˜ç±»å‹
            is_temporal = any(keyword in question.lower() for keyword in
                             ['days', 'day', 'before', 'after', 'first', 'å…ˆå', 'é¡ºåº', 'é—´éš”', 'å¤šä¹…', 'å¤šå°‘å¤©'])

            # æ£€ç´¢
            t0 = time.time()
            contexts_all: List[str] = []
            dialogs, statements, entities = [], [], []

            try:
                if search_type == "embedding":
                    search_results = await search_graph_by_embedding(
                        connector=connector,
                        embedder_client=embedder,
                        query_text=question,
                        group_id=group_id,
                        limit=search_limit,
                        include=["dialogues", "statements", "entities"],
                    )
                    dialogs = search_results.get("dialogues", [])
                    statements = search_results.get("statements", [])
                    entities = search_results.get("entities", [])

                    for d in dialogs:
                        content = str(d.get("content", "")).strip()
                        if content:
                            contexts_all.append(content)
                    for s in statements:
                        stmt_text = str(s.get("statement", "")).strip()
                        if stmt_text:
                            contexts_all.append(stmt_text)
                    # å®ä½“æ‘˜è¦ï¼ˆæœ€å¤š3ä¸ªï¼‰
                    scored = [e for e in entities if e.get("score") is not None]
                    top_entities = sorted(scored, key=lambda x: x.get("score", 0), reverse=True)[:3] if scored else entities[:3]
                    if top_entities:
                        summary_lines = []
                        for e in top_entities:
                            name = str(e.get("name", "")).strip()
                            etype = str(e.get("entity_type", "")).strip()
                            score = e.get("score")
                            if name:
                                meta = []
                                if etype:
                                    meta.append(f"type={etype}")
                                if isinstance(score, (int, float)):
                                    meta.append(f"score={score:.3f}")
                                summary_lines.append(f"EntitySummary: {name}{(' [' + '; '.join(meta) + ']') if meta else ''}")
                        if summary_lines:
                            contexts_all.append("\n".join(summary_lines))

                elif search_type == "keyword":
                    search_results = await search_graph(
                        connector=connector,
                        q=question,
                        group_id=group_id,
                        limit=search_limit,
                    )
                    dialogs = search_results.get("dialogues", [])
                    statements = search_results.get("statements", [])
                    entities = search_results.get("entities", [])

                    for d in dialogs:
                        content = str(d.get("content", "")).strip()
                        if content:
                            contexts_all.append(content)
                    for s in statements:
                        stmt_text = str(s.get("statement", "")).strip()
                        if stmt_text:
                            contexts_all.append(stmt_text)
                    if entities:
                        entity_names = [str(e.get("name", "")).strip() for e in entities[:5] if e.get("name")]
                        if entity_names:
                            contexts_all.append(f"EntitySummary: {', '.join(entity_names)}")

                else:  # hybridï¼ˆå¢å¼ºç‰ˆï¼šç‰¹åˆ«ä¼˜åŒ–æŠ€æœ¯æœ¯è¯­æ£€ç´¢ï¼‰
                    emb_dialogs, emb_statements, emb_entities = [], [], []
                    kw_dialogs, kw_statements, kw_entities = [], [], []

                    # 1) åµŒå…¥æ£€ç´¢
                    try:
                        emb_res = await search_graph_by_embedding(
                            connector=connector,
                            embedder_client=embedder,
                            query_text=question,
                            group_id=group_id,
                            limit=search_limit,
                            include=["dialogues", "statements", "entities"],
                        )
                        if isinstance(emb_res, dict):
                            emb_dialogs = emb_res.get("dialogues", []) or []
                            emb_statements = emb_res.get("statements", []) or []
                            emb_entities = emb_res.get("entities", []) or []
                    except Exception as e:
                        print(f"âš ï¸ åµŒå…¥æ£€ç´¢å¤±è´¥ï¼Œå°†ç»§ç»­è¿›è¡Œå…³é”®è¯æ£€ç´¢: {e}")

                    # 2) å…³é”®è¯æ£€ç´¢ï¼ˆå¢å¼ºç‰ˆï¼‰
                    try:
                        kw_res = await search_graph(
                            connector=connector,
                            q=question,
                            group_id=group_id,
                            limit=search_limit,
                        )
                        if isinstance(kw_res, dict):
                            kw_dialogs = kw_res.get("dialogues", []) or []
                            kw_statements = kw_res.get("statements", []) or []
                            kw_entities = kw_res.get("entities", []) or []

                            # æŠ€æœ¯æœ¯è¯­ä¸“é—¨æ£€ç´¢
                            tech_entities = await _search_tech_terms(connector, question, group_id, search_limit//2)
                            if tech_entities:
                                kw_entities.extend(tech_entities)

                            # æ—¶é—´æ¨ç†é—®é¢˜çš„ç‰¹æ®Šå¤„ç†
                            if is_temporal:
                                # ä¸“é—¨æœç´¢æ—¶é—´å®ä½“
                                time_entities = await _search_time_entities(connector, group_id, search_limit//2)
                                if time_entities:
                                    kw_entities.extend(time_entities)
                                # æ·»åŠ æ—¶é—´ç›¸å…³å…³é”®è¯æ£€ç´¢
                                time_keywords = ['å¤©', 'æ—¥', 'æœˆ', 'å¹´', 'before', 'after', 'first']
                                for tk in time_keywords:
                                    try:
                                        time_res = await search_graph(
                                            connector=connector,
                                            q=tk,
                                            group_id=group_id,
                                            limit=2,
                                        )
                                        if isinstance(time_res, dict):
                                            kw_dialogs.extend(time_res.get("dialogues", []) or [])
                                            kw_statements.extend(time_res.get("statements", []) or [])
                                    except Exception:
                                        pass

                            # ä¸­æ–‡å…³é”®è¯æ‹†åˆ†ååšåˆ«ååŒ¹é…
                            cn_tokens = generate_query_keywords_cn(question)  # ä½¿ç”¨å¢å¼ºç‰ˆå…³é”®è¯æå–
                            alias_entities = await _search_entities_by_aliases(connector, cn_tokens, group_id, search_limit)
                            if alias_entities:
                                kw_entities.extend(alias_entities)

                            # ä»å¯¹è¯/é™ˆè¿°ä¸­çš„ entity_ids åæŸ¥å®ä½“
                            ids = []
                            try:
                                for d in kw_dialogs:
                                    ids.extend(d.get("entity_ids", []) or [])
                                for s in kw_statements:
                                    ids.extend(s.get("entity_ids", []) or [])
                            except Exception:
                                pass
                            if ids:
                                id_entities = await _fetch_entities_by_ids(connector, ids, group_id)
                                if id_entities:
                                    kw_entities.extend(id_entities)

                            # å¤šå…³é”®è¯æ£€ç´¢ï¼ˆä½¿ç”¨å¢å¼ºç‰ˆå…³é”®è¯ï¼‰
                            try:
                                eng_words = [w for w in set(re.findall(r"\b\w+\b", question.lower())) if len(w) > 2]
                                kw_list = generate_query_keywords_cn(question)[:4]  # ä½¿ç”¨æ›´å¤šå…³é”®è¯
                                for kw in kw_list:
                                    if not kw:
                                        continue
                                    sub_res = await search_graph(
                                        connector=connector,
                                        q=str(kw),
                                        group_id=group_id,
                                        limit=max(3, search_limit // 2),
                                    )
                                    if isinstance(sub_res, dict):
                                        kw_dialogs.extend(sub_res.get("dialogues", []) or [])
                                        kw_statements.extend(sub_res.get("statements", []) or [])
                                        kw_entities.extend(sub_res.get("entities", []) or [])
                            except Exception:
                                pass

                            # é€‰é¡¹å‚ä¸å…³é”®è¯æ£€ç´¢
                            try:
                                opt_list = extract_candidate_options(question)[:2]
                                for opt in opt_list:
                                    if not opt:
                                        continue
                                    opt_res = await search_graph(
                                        connector=connector,
                                        q=str(opt),
                                        group_id=group_id,
                                        limit=max(3, search_limit // 2),
                                    )
                                    if isinstance(opt_res, dict):
                                        kw_dialogs.extend(opt_res.get("dialogues", []) or [])
                                        kw_statements.extend(opt_res.get("statements", []) or [])
                                        kw_entities.extend(opt_res.get("entities", []) or [])
                            except Exception:
                                pass
                    except Exception as e:
                        print(f"âŒ å…³é”®è¯æ£€ç´¢å¤±è´¥: {e}")

                    # 3) åˆå¹¶ã€æ’åºå¹¶å»é‡
                    all_dialogs = emb_dialogs + kw_dialogs
                    all_statements = emb_statements + kw_statements
                    all_entities = emb_entities + kw_entities

                    def dedup(items: List[Dict[str, Any]], key_field: str = "uuid") -> List[Dict[str, Any]]:
                        seen = set()
                        out = []
                        for it in items:
                            key = str(it.get(key_field, "")) + str(it.get("content", "") + str(it.get("statement", "")))
                            if key not in seen:
                                out.append(it)
                                seen.add(key)
                        return out

                    # å…³é”®æŠ€æœ¯å®ä½“ä¼˜å…ˆæ’åº
                    def enhanced_score(item: Dict[str, Any]) -> float:
                        score_val = item.get("score", 0.0)
                        base_score = float(score_val) if score_val is not None else 0.0
                        content = str(item.get("content", "") + str(item.get("statement", "")))

                        # å…³é”®æŠ€æœ¯å®ä½“å¥–åŠ±
                        key_entities = []
                        if any(term in question for term in ["GPS", "å¯¼èˆª", "ç³»ç»Ÿ"]):
                            key_entities.extend(["GPS", "å¯¼èˆª", "ç³»ç»Ÿ", "åŠŸèƒ½"])
                        if any(term in question for term in ["å·¥ä½œåŠ", "ç ”è®¨ä¼š", "æ´»åŠ¨"]):
                            key_entities.extend(["å·¥ä½œåŠ", "ç ”è®¨ä¼š", "å‚åŠ "])

                        key_bonus = 0
                        for key_ent in key_entities:
                            if key_ent in content:
                                key_bonus += 1.0

                        # æ—¶é—´å®ä½“å¥–åŠ±
                        time_bonus = 0
                        if is_temporal:
                            time_entities = extract_time_entities(content)
                            time_bonus = len(time_entities) * 0.5

                        return base_score + key_bonus + time_bonus

                    dialogs = dedup(sorted(all_dialogs, key=enhanced_score, reverse=True))
                    statements = dedup(sorted(all_statements, key=enhanced_score, reverse=True))
                    entities = dedup(all_entities, key_field="name")

                    # 4) æ„å»ºä¸Šä¸‹æ–‡
                    for d in dialogs:
                        content = str(d.get("content", "")).strip()
                        if content:
                            contexts_all.append(content)
                    for s in statements:
                        stmt_text = str(s.get("statement", "")).strip()
                        if stmt_text:
                            contexts_all.append(stmt_text)
                    # å®ä½“æ‘˜è¦
                    try:
                        scored = [e for e in entities if e.get("score") is not None]
                        top_entities = sorted(scored, key=lambda x: x.get("score", 0), reverse=True)[:3] if scored else entities[:3]
                        if top_entities:
                            summary_lines = []
                            for e in top_entities:
                                name = str(e.get("name", "")).strip()
                                etype = str(e.get("entity_type", "")).strip()
                                score = e.get("score")
                                if name:
                                    meta = []
                                    if etype:
                                        meta.append(f"type={etype}")
                                    if isinstance(score, (int, float)):
                                        meta.append(f"score={score:.3f}")
                                    summary_lines.append(f"EntitySummary: {name}{(' [' + '; '.join(meta) + ']') if meta else ''}")
                            if summary_lines:
                                contexts_all.append("\n".join(summary_lines))
                    except Exception:
                        pass

                # å…¨å±€å›é€€
                if not contexts_all and search_type in ("embedding", "hybrid"):
                    try:
                        print("ğŸ” æ£€ç´¢ä¸ºç©ºï¼Œå›é€€åˆ°å…³é”®è¯æ£€ç´¢...")
                        kw_fallback = await search_graph(
                            connector=connector,
                            q=question,
                            group_id=group_id,
                            limit=max(search_limit, 5),
                        )
                        fb_dialogs = kw_fallback.get("dialogues", []) or []
                        fb_statements = kw_fallback.get("statements", []) or []
                        fb_entities = kw_fallback.get("entities", []) or []

                        for d in fb_dialogs:
                            content = str(d.get("content", "")).strip()
                            if content:
                                contexts_all.append(content)
                        for s in fb_statements:
                            stmt_text = str(s.get("statement", "")).strip()
                            if stmt_text:
                                contexts_all.append(stmt_text)
                        if fb_entities:
                            entity_names = [str(e.get("name", "")).strip() for e in fb_entities[:5] if e.get("name")]
                            if entity_names:
                                contexts_all.append(f"EntitySummary: {', '.join(entity_names)}")

                        dialogs = fb_dialogs if fb_dialogs else dialogs
                        statements = fb_statements if fb_statements else statements
                        entities = fb_entities if fb_entities else entities
                        print(f"â†©ï¸ å›é€€åˆ°å…³é”®è¯æ£€ç´¢: {len(fb_dialogs)} å¯¹è¯, {len(fb_statements)} æ¡é™ˆè¿°, {len(fb_entities)} ä¸ªå®ä½“")
                    except Exception as fe:
                        print(f"âŒ å…³é”®è¯å›é€€å¤±è´¥: {fe}")

                ent_count = len(entities) if isinstance(entities, list) else 0
                print(f"âœ… {search_type}æ£€ç´¢æˆåŠŸ: {len(dialogs)} å¯¹è¯, {len(statements)} æ¡é™ˆè¿°, {ent_count} ä¸ªå®ä½“")
                if is_temporal:
                    print("â° æ£€æµ‹ä¸ºæ—¶é—´æ¨ç†é—®é¢˜ï¼Œå·²å¯ç”¨æ—¶é—´ä¼˜åŒ–æ£€ç´¢")

            except Exception as e:
                print(f"âŒ {search_type}æ£€ç´¢å¤±è´¥: {e}")
                contexts_all = []

            t1 = time.time()
            latencies_search.append((t1 - t0) * 1000)

            # æ™ºèƒ½ä¸Šä¸‹æ–‡é€‰æ‹©
            context_text = ""
            if contexts_all:
                context_text = smart_context_selection(contexts_all, question, max_chars=context_char_budget)
                # ç›¸å¯¹æ—¶é—´è§£æ
                try:
                    context_text = _resolve_relative_times_cn_en(context_text, anchor=datetime.now())
                except Exception:
                    pass
                # è¯Šæ–­ä¿¡æ¯
                try:
                    cn_diag = generate_query_keywords_cn(question)[:4]  # æ˜¾ç¤ºæ›´å¤šå…³é”®è¯
                    opts = extract_candidate_options(question)[:2]
                    qlw = [w for w in set(re.findall(r'\b\w+\b', question.lower())) if len(w) > 2][:1]
                    diag_tokens: List[str] = []
                    for t in cn_diag + opts + qlw:
                        if t and t not in diag_tokens:
                            diag_tokens.append(t)
                    print(f"ğŸ” å…³é”®è¯/é€‰é¡¹: {', '.join(diag_tokens)}")
                    preview = context_text[:200].replace('\n', ' ')
                    print(f"ğŸ” ä¸Šä¸‹æ–‡é¢„è§ˆ: {preview}...")
                    key_preview = preview.strip()
                    if key_preview:
                        preview_counter[key_preview] = preview_counter.get(key_preview, 0) + 1
                except Exception:
                    pass
            else:
                print("âŒ æ²¡æœ‰æ£€ç´¢åˆ°æœ‰æ•ˆä¸Šä¸‹æ–‡")
                context_text = "No relevant context found."

            # è®°å½•ä¸Šä¸‹æ–‡è¯Šæ–­ä¿¡æ¯
            per_query_context_counts.append(len(contexts_all))
            per_query_context_avg_tokens.append(avg_context_tokens([context_text]))
            per_query_context_chars.append(len(context_text))

            # LLM æ¨ç†ï¼ˆå¢å¼ºæŠ€æœ¯æœ¯è¯­æç¤ºï¼‰
            options = extract_candidate_options(question)
            if len(options) >= 2:
                opt_lines = "\n".join(f"- {o}" for o in options)
                # æŠ€æœ¯æœ¯è¯­é—®é¢˜çš„ç‰¹æ®Šæç¤º
                if any(term in question for term in ["GPS", "ç³»ç»Ÿ", "åŠŸèƒ½", "å·¥ä½œåŠ", "ç ”è®¨ä¼š"]):
                    system_prompt = (
                        "You are a QA assistant specializing in technical and activity-related questions. "
                        "Pay special attention to technical terms like GPS, systems, functions, workshops, and seminars. "
                        "Return ONLY one string: exactly one option from the provided candidates. If the context is insufficient, respond with 'Unknown'. "
                        "Focus on matching technical details and activity sequences accurately."
                    )
                elif is_temporal:
                    system_prompt = (
                        "You are a QA assistant specializing in temporal reasoning. Analyze the dates and time relationships in the context carefully. "
                        "Return ONLY one string: exactly one option from the provided candidates. If the context is insufficient, respond with 'Unknown'. "
                        "Pay special attention to date sequences and time intervals."
                    )
                else:
                    system_prompt = (
                        "You are a QA assistant. Respond in the same language as the question. Return ONLY one string: exactly one option from the provided candidates. "
                        "If the context is insufficient, respond with 'Unknown'. If the context expresses a synonym or paraphrase of a candidate, return the closest candidate. "
                        "Do not include explanations."
                    )

                messages = [
                    {"role": "system", "content": system_prompt},
                    {
                        "role": "user",
                        "content": (
                            f"Question: {question}\n\nCandidates:\n{opt_lines}\n\nContext:\n{context_text}\n\nReturn EXACTLY one candidate string (or 'Unknown')."
                        ),
                    },
                ]
            else:
                # æŠ€æœ¯æœ¯è¯­é—®é¢˜çš„ç‰¹æ®Šæç¤º
                if any(term in question for term in ["GPS", "ç³»ç»Ÿ", "åŠŸèƒ½", "å·¥ä½œåŠ", "ç ”è®¨ä¼š"]):
                    system_prompt = (
                        "You are a QA assistant specializing in technical and activity-related questions. "
                        "Pay special attention to technical terms like GPS, systems, functions, workshops, and seminars. "
                        "If the context contains the answer, return a concise answer phrase focusing on technical details. "
                        "If the answer cannot be determined from the context, respond with 'Unknown'. Return ONLY the final answer string, no explanations."
                    )
                elif is_temporal:
                    system_prompt = (
                        "You are a QA assistant specializing in temporal reasoning. Analyze the dates and time relationships in the context carefully. "
                        "If the context contains the answer, return a concise answer phrase focusing on temporal information. "
                        "If the answer cannot be determined from the context, respond with 'Unknown'. Return ONLY the final answer string, no explanations."
                    )
                else:
                    system_prompt = (
                        "You are a QA assistant. Respond in the same language as the question. If the context contains the answer, return a concise answer phrase. "
                        "If the answer cannot be determined from the context, respond with 'Unknown'. Return ONLY the final answer string, no explanations."
                    )

                messages = [
                    {"role": "system", "content": system_prompt},
                    {
                        "role": "user",
                        "content": f"Question: {question}\n\nContext:\n{context_text}\n\nReturn ONLY the answer (or 'Unknown').",
                    },
                ]

            t2 = time.time()
            # ä½¿ç”¨å¼‚æ­¥è°ƒç”¨
            resp = await llm_client.chat(messages=messages)
            t3 = time.time()
            latencies_llm.append((t3 - t2) * 1000)

            # å…¼å®¹ä¸åŒçš„å“åº”æ ¼å¼
            pred_raw = resp.content.strip() if hasattr(resp, 'content') else (resp["choices"][0]["message"]["content"].strip() if isinstance(resp, dict) else "Unknown")

            # é€‰é¡¹é¢˜è¾“å‡ºè§„èŒƒåŒ–
            pred = pred_raw
            if len(options) >= 2 and not pred_raw.lower().startswith("unknown"):
                def _basic_norm(s: str) -> str:
                    s = s.lower().strip()
                    return re.sub(r"[^\w\s]", " ", s)
                def _jaccard(a: str, b: str) -> float:
                    ta = set(t for t in _basic_norm(a).split() if t)
                    tb = set(t for t in _basic_norm(b).split() if t)
                    if not ta and not tb:
                        return 1.0
                    if not ta or not tb:
                        return 0.0
                    return len(ta & tb) / len(ta | tb)
                best = None
                best_score = -1.0
                for o in options:
                    score = _jaccard(pred_raw, o)
                    if score > best_score:
                        best = o
                        best_score = score
                if best is not None and best_score > 0.0:
                    pred = best

            # æŒ‡æ ‡
            flag = exact_match(pred, reference)
            f1_val = common_f1(str(pred), str(reference))
            j_val = jaccard(str(pred), str(reference))

            type_correct.setdefault(qtype, []).append(flag)
            type_f1.setdefault(qtype, []).append(f1_val)
            type_jacc.setdefault(qtype, []).append(j_val)

            samples.append({
                "question": question,
                "prediction": pred,
                "answer": reference,
                "question_type": qtype,
                "is_temporal": is_temporal,
                "question_id": item.get("question_id"),
                "options": options,
                "context_count": len(contexts_all),
                "context_chars": len(context_text),
                "retrieved_dialogue_count": len(dialogs),
                "retrieved_statement_count": len(statements),
                "metrics": {
                    "exact_match": bool(flag),
                    "f1": f1_val,
                    "jaccard": j_val
                },
                "timing": {
                    "search_ms": (t1 - t0) * 1000,
                    "llm_ms": (t3 - t2) * 1000
                }
            })

            print(f"ğŸ¤– LLM å›ç­”: {pred}")
            print(f"âœ… æ­£ç¡®ç­”æ¡ˆ: {reference}")
            print(f"ğŸ“ˆ å½“å‰æŒ‡æ ‡ - Exact Match: {flag}, F1: {f1_val:.3f}, Jaccard: {j_val:.3f}")

        # èšåˆç»“æœ
        type_acc = {t: (sum(v) / max(len(v), 1)) for t, v in type_correct.items()}
        f1_by_type = {t: (sum(v) / max(len(v), 1)) for t, v in type_f1.items()}
        jacc_by_type = {t: (sum(v) / max(len(v), 1)) for t, v in type_jacc.items()}

        result = {
            "dataset": "longmemeval",
            "items": len(items),
            "accuracy_by_type": type_acc,
            "f1_by_type": f1_by_type,
            "jaccard_by_type": jacc_by_type,
            "samples": samples,
            "latency": {
                "search": latency_stats(latencies_search),
                "llm": latency_stats(latencies_llm),
            },
            "context": {
                "avg_tokens": statistics.mean(per_query_context_avg_tokens) if per_query_context_avg_tokens else 0.0,
                "avg_chars": statistics.mean(per_query_context_chars) if per_query_context_chars else 0.0,
                "count_avg": statistics.mean(per_query_context_counts) if per_query_context_counts else 0.0,
            },
            "params": {
                "group_id": group_id,
                "search_limit": search_limit,
                "context_char_budget": context_char_budget,
                "search_type": search_type,
                "llm_id": SELECTED_LLM_ID,
                "embedding_id": SELECTED_EMBEDDING_ID,
                "sample_size": sample_size,
                "start_index": start_index,
            },
            "timestamp": datetime.now().isoformat()
        }

        # è®¡ç®—æ±‡æ€»æŒ‡æ ‡
        try:
            total_items = max(len(samples), 1)
            correct_count = sum(1 for s in samples if s.get("metrics", {}).get("exact_match"))
            score_accuracy = (correct_count / total_items) * 100.0

            total_latencies_ms = []
            for s in samples:
                t = s.get("timing", {})
                total_latencies_ms.append(float(t.get("search_ms", 0.0)) + float(t.get("llm_ms", 0.0)))
            total_lat_stats = latency_stats(total_latencies_ms) if total_latencies_ms else {"p50": 0.0, "iqr": 0.0}
            latency_median_s = total_lat_stats.get("p50", 0.0) / 1000.0
            latency_iqr_s = total_lat_stats.get("iqr", 0.0) / 1000.0

            avg_ctx_tokens = statistics.mean(per_query_context_avg_tokens) if per_query_context_avg_tokens else 0.0
            avg_ctx_tokens_k = avg_ctx_tokens / 1000.0

            result["metric_summary"] = {
                "score_accuracy": score_accuracy,
                "latency_median_s": latency_median_s,
                "latency_iqr_s": latency_iqr_s,
                "avg_context_tokens_k": avg_ctx_tokens_k,
            }
        except Exception:
            result["metric_summary"] = {
                "score_accuracy": 0.0,
                "latency_median_s": 0.0,
                "latency_iqr_s": 0.0,
                "avg_context_tokens_k": 0.0,
            }

        # è¯Šæ–­ä¿¡æ¯
        try:
            dups = sorted([(k, c) for k, c in preview_counter.items() if c > 1], key=lambda x: -x[1])[:5]
            result["diagnostics"] = {
                "duplicate_previews_top": [{"count": c, "preview": k[:120]} for k, c in dups],
                "unique_preview_count": len(preview_counter),
            }
        except Exception:
            pass

        return result

    finally:
        await connector.close()


def main():
    load_dotenv()
    parser = argparse.ArgumentParser(description="LongMemEval è¯„ä¼°æµ‹è¯•è„šæœ¬ï¼ˆå¢å¼ºæŠ€æœ¯æœ¯è¯­æ£€ç´¢ç‰ˆï¼‰")
    parser.add_argument("--sample-size", type=int, default=3, help="æ ·æœ¬æ•°é‡ï¼ˆ<=0 è¡¨ç¤ºå…¨éƒ¨ï¼‰")
    parser.add_argument("--all", action="store_true", help="è¯„ä¼°å…¨éƒ¨æ ·æœ¬ï¼ˆè¦†ç›– --sample-sizeï¼‰")
    parser.add_argument("--start-index", type=int, default=0, help="èµ·å§‹æ ·æœ¬ç´¢å¼•")
    parser.add_argument("--group-id", type=str, default="longmemeval_zh_bak_3", help="å›¾æ•°æ®åº“ Group ID")
    parser.add_argument("--search-limit", type=int, default=8, help="æ£€ç´¢æ¡æ•°ä¸Šé™")
    parser.add_argument("--context-char-budget", type=int, default=4000, help="ä¸Šä¸‹æ–‡å­—ç¬¦é¢„ç®—")
    parser.add_argument("--llm-temperature", type=float, default=0.0, help="LLM æ¸©åº¦")
    parser.add_argument("--llm-max-tokens", type=int, default=16, help="LLM æœ€å¤§è¾“å‡º token")
    parser.add_argument("--search-type", type=str, default="hybrid", choices=["embedding","keyword","hybrid"], help="æ£€ç´¢ç±»å‹")
    parser.add_argument("--data-path", type=str, default=None, help="æ•°æ®é›†è·¯å¾„")
    args = parser.parse_args()

    sample_size = 0 if args.all else args.sample_size

    result = asyncio.run(
        run_longmemeval_test(
            sample_size=sample_size,
            group_id=args.group_id,
            search_limit=args.search_limit,
            context_char_budget=args.context_char_budget,
            llm_temperature=args.llm_temperature,
            llm_max_tokens=args.llm_max_tokens,
            search_type=args.search_type,
            data_path=args.data_path,
            start_index=args.start_index,
        )
    )

    # æ‰“å°ç»“æœ
    print("\n" + "="*50)
    print("ğŸ“Š LongMemEval æµ‹è¯•ç»“æœ:")
    print(f"   æ ·æœ¬æ•°é‡: {result['items']}")

    if result['accuracy_by_type']:
        print("\nğŸ“ˆ æŒ‰é—®é¢˜ç±»å‹ç»†åˆ†:")
        for qtype, acc in result['accuracy_by_type'].items():
            print(f"   {qtype}:")
            print(f"     Score (Accuracy): {acc:.3f}")

    print(f"\nğŸ“Š æŒ‡æ ‡æ€»è§ˆ:")
    ms = result.get('metric_summary', {})
    print(f"   Score (Accuracy): {ms.get('score_accuracy', 0.0):.1f}%")
    print(f"   Latency (s): median {ms.get('latency_median_s', 0.0):.3f}s")
    print(f"   Latency IQR (s): {ms.get('latency_iqr_s', 0.0):.3f}s")
    print(f"   Avg Context Tokens (k): {ms.get('avg_context_tokens_k', 0.0):.3f}k")

    print(f"\nâ±ï¸  ç»†åˆ†æ€§èƒ½æŒ‡æ ‡:")
    print(f"   æ£€ç´¢å»¶è¿Ÿ(å‡å€¼): {result['latency']['search']['mean']:.1f}ms")
    print(f"   LLMå»¶è¿Ÿ(å‡å€¼): {result['latency']['llm']['mean']:.1f}ms")
    print(f"   ä¸Šä¸‹æ–‡é•¿åº¦(å‡å€¼): {result['context']['avg_chars']:.0f} å­—ç¬¦")


    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    try:
        out_dir = os.path.join(PROJECT_ROOT, "evaluation", "longmemeval", "results")
        os.makedirs(out_dir, exist_ok=True)
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        out_path = os.path.join(out_dir, f"longmemeval_{result['params']['search_type']}_{ts}.json")
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {out_path}")
    except Exception as e:
        print(f"âš ï¸ ç»“æœä¿å­˜å¤±è´¥: {e}")


if __name__ == "__main__":
    main()
