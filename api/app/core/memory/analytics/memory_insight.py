"""
This module provides the MemoryInsight class for analyzing user memory data.

This script can be executed directly to generate a memory insight report for a test user.
"""

import asyncio
import os
import sys
import json
from collections import Counter
from datetime import datetime

# To run this script directly, we need to add the src directory to the Python path
# to resolve the inconsistent imports in other modules.
src_path = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if src_path not in sys.path:
    sys.path.insert(0, src_path)

from pydantic import BaseModel, Field

from app.repositories.neo4j.neo4j_connector import Neo4jConnector
from app.core.memory.utils.llm.llm_utils import get_llm_client
from app.core.memory.analytics.hot_memory_tags import get_hot_memory_tags
from app.core.memory.utils.config.definitions import SELECTED_GROUP_ID, SELECTED_LLM_ID

# 定义用于LLM结构化输出的Pydantic模型
class TagClassification(BaseModel):
    """
    Represents the classification of a tag into a specific domain.
    """

    domain: str = Field(
        ...,
        description="The domain the tag belongs to, chosen from the predefined list.",
        examples=["教育", "学习", "工作", "旅行", "家庭", "运动", "社交", "娱乐", "健康", "其他"],
    )

class InsightReport(BaseModel):
    """
    Represents the final insight report generated by the LLM.
    """

    report: str = Field(
        ...,
        description="A comprehensive insight report in Chinese, summarizing the user's memory patterns.",
    )


class MemoryInsight:
    """
    Provides insights into user memories by analyzing various aspects of their data.
    """

    def __init__(self, user_id: str):
        self.user_id = user_id
        self.neo4j_connector = Neo4jConnector()
        from app.core.memory.utils.config import definitions as config_defs
        self.llm_client = get_llm_client(config_defs.SELECTED_LLM_ID)

    async def close(self):
        """关闭数据库连接。"""
        await self.neo4j_connector.close()

    async def get_domain_distribution(self) -> dict[str, float]:
        """
        Calculates the distribution of memory domains based on hot tags.
        """
        hot_tags = await get_hot_memory_tags(self.user_id)
        if not hot_tags:
            return {}

        domain_counts = Counter()
        for tag, _ in hot_tags:
            prompt = f"""请将以下标签归类到最合适的领域中。

可选领域及其关键词：
- 教育：学校、课程、考试、培训、教学、学科、教师、学生、班级、作业、成绩、毕业、入学、校园、大学、中学、小学、教材、学位等
- 学习：自学、阅读、书籍、技能提升、知识积累、笔记、复习、练习、研究、历史知识、科学知识、文化知识、学术讨论、知识问答等
- 工作：职业、项目、会议、同事、业务、公司、办公、任务、客户、合同、职场、工作计划等
- 旅行：旅游、景点、出行、度假、酒店、机票、导游、风景、旅行计划等
- 家庭：亲人、父母、子女、配偶、家事、家庭活动、亲情、家庭聚会等
- 运动：健身、体育、锻炼、跑步、游泳、球类、瑜伽、运动计划等
- 社交：朋友、聚会、社交活动、派对、聊天、交友、社交网络等
- 娱乐：游戏、电影、音乐、休闲、综艺、动漫、小说、娱乐活动等
- 健康：医疗、养生、心理健康、体检、药物、疾病、保健、健康管理等
- 其他：确实无法归入以上任何类别的内容

标签: {tag}

分析步骤：
1. 仔细理解标签的核心含义和使用场景
2. 对比各个领域的关键词，找到最匹配的领域
3. 特别注意：
   - 历史、科学、文化等知识性内容应归类为"学习"
   - 学校、课程、考试等正式教育场景应归类为"教育"
   - 只有在标签完全不属于上述9个具体领域时，才选择"其他"
4. 如果标签与某个领域有任何相关性，就选择该领域，不要选"其他"

请直接返回最合适的领域名称。"""
            messages = [
                {"role": "system", "content": "你是一个专业的标签分类助手。你必须仔细分析标签的实际含义和使用场景，优先选择9个具体领域之一。'其他'类别只用于完全无法归类的极少数情况。特别注意：历史、科学、文化等知识性对话应归类为'学习'领域；学校、课程、考试等正式教育场景应归类为'教育'领域。"},
                {"role": "user", "content": prompt}
            ]
            # 直接调用并等待结果
            classification = await self.llm_client.response_structured(
                messages=messages,
                response_model=TagClassification,
            )
            if classification and hasattr(classification, 'domain') and classification.domain:
                domain_counts[classification.domain] += 1

        total_tags = sum(domain_counts.values())
        if total_tags == 0:
            return {}

        domain_distribution = {
            domain: count / total_tags for domain, count in domain_counts.items()
        }
        return dict(
            sorted(domain_distribution.items(), key=lambda item: item[1], reverse=True)
        )

    async def get_active_periods(self) -> list[int]:
        """
        Identifies the top 2 most active months for the user.
        Only returns months if there is valid and diverse time data.
        
        This method checks if the time data represents real user memory timestamps
        rather than auto-generated system timestamps by verifying:
        1. Time data exists and is parseable
        2. Time data is distributed across multiple months (not concentrated in 1-2 months)
        """
        query = f"""
        MATCH (d:Dialogue)
        WHERE d.group_id = '{self.user_id}' AND d.created_at IS NOT NULL AND d.created_at <> ''
        RETURN d.created_at AS creation_time
        """
        records = await self.neo4j_connector.execute_query(query)

        if not records:
            return []

        month_counts = Counter()
        valid_dates_count = 0
        for record in records:
            creation_time_str = record.get("creation_time")
            if not creation_time_str:
                continue
            try:
                # 尝试解析时间字符串
                dt_object = datetime.fromisoformat(creation_time_str.replace("Z", "+00:00"))
                month_counts[dt_object.month] += 1
                valid_dates_count += 1
            except (ValueError, TypeError, AttributeError):
                # 如果解析失败，跳过这条记录
                continue

        # 如果没有有效的时间数据，返回空列表
        if not month_counts or valid_dates_count == 0:
            return []

        # 检查时间分布是否过于集中（可能是批量导入的数据）
        # 如果超过80%的数据集中在1-2个月，认为这是系统时间戳而非真实时间
        unique_months = len(month_counts)
        if unique_months <= 2:
            # 只有1-2个月有数据，很可能是批量导入
            most_common_count = month_counts.most_common(1)[0][1]
            if most_common_count / valid_dates_count > 0.8:
                # 超过80%集中在一个月，认为是系统时间戳
                return []
        
        # 如果时间分布较为分散（3个月以上），认为是真实时间数据
        if unique_months >= 3:
            most_common_months = month_counts.most_common(2)
            return [month for month, _ in most_common_months]
        
        # 2个月的情况，检查是否分布均匀
        if unique_months == 2:
            counts = list(month_counts.values())
            # 如果两个月的数据量相差不大（比例在0.3-3之间），认为是真实数据
            ratio = min(counts) / max(counts)
            if ratio > 0.3:
                most_common_months = month_counts.most_common(2)
                return [month for month, _ in most_common_months]
        
        # 其他情况返回空列表
        return []

    async def get_social_connections(self) -> dict | None:
        """
        Finds the user with whom the most memories are shared.
        """
        query = f"""
        MATCH (d1:Dialogue {{group_id: '{self.user_id}'}})<-[:MENTIONS]-(s:Statement)-[:MENTIONS]->(d2:Dialogue)
        WHERE d1 <> d2
        RETURN d2.group_id AS other_user_id, COUNT(s) AS common_statements
        ORDER BY common_statements DESC
        LIMIT 1
        """
        records = await self.neo4j_connector.execute_query(query)
        if not records:
            return None

        most_connected_user = records[0]["other_user_id"]
        common_memories_count = records[0]["common_statements"]

        time_range_query = f"""
        MATCH (d:Dialogue)
        WHERE d.group_id IN ['{self.user_id}', '{most_connected_user}']
        RETURN min(d.created_at) AS start_time, max(d.created_at) AS end_time
        """
        time_records = await self.neo4j_connector.execute_query(time_range_query)
        start_year, end_year = "N/A", "N/A"
        if time_records and time_records[0]["start_time"]:
            start_year = datetime.fromisoformat(time_records[0]["start_time"].replace("Z", "+00:00")).year
            end_year = datetime.fromisoformat(time_records[0]["end_time"].replace("Z", "+00:00")).year

        return {
            "user_id": most_connected_user,
            "common_memories_count": common_memories_count,
            "time_range": f"{start_year}-{end_year}",
        }

    async def generate_insight_report(self) -> str:
        """
        Generates the final insight report in natural language.
        """
        domain_dist, active_periods, social_conn = await asyncio.gather(
            self.get_domain_distribution(),
            self.get_active_periods(),
            self.get_social_connections(),
        )

        prompt_parts = []

        if domain_dist:
            top_domains = ", ".join([f"{k}({v:.0%})" for k, v in list(domain_dist.items())[:3]])
            prompt_parts.append(f"- 核心领域: 用户的记忆主要集中在 {top_domains}。")

        if active_periods:
            months_str = " 和 ".join(map(str, active_periods))
            prompt_parts.append(f"- 活跃时段: 用户在每年的 {months_str} 月最为活跃。")

        if social_conn:
            prompt_parts.append(
                f"- 社交关联: 与用户\"{social_conn['user_id']}\"拥有最多共同记忆({social_conn['common_memories_count']}条)，时间范围主要在 {social_conn['time_range']}。"
            )

        if not prompt_parts:
            return "暂无足够数据生成洞察报告。"

        system_prompt = '''你是一位资深的个人记忆分析师。你的任务是根据我提供的要点，为用户生成一段简洁、自然、个性化的记忆洞察报告。

重要规则：
1. 报告需要将所有要点流畅地串联成一个段落
2. 语言风格要亲切、易于理解，就像和朋友聊天一样
3. 不要添加任何额外的解释或标题，直接输出报告内容
4. 只使用我提供的要点，不要编造或推测任何信息
5. 如果某个维度没有数据（如没有活跃时段信息），就不要在报告中提及该维度

例如，如果输入是：
- 核心领域: 用户的记忆主要集中在 旅行(38%), 工作(24%), 家庭(21%)。
- 活跃时段: 用户在每年的 4 和 10 月最为活跃。
- 社交关联: 与用户"张明"拥有最多共同记忆(47条)，时间范围主要在 2017-2020。

你的输出应该是：
"您的记忆集中在旅行(38%)、工作(24%)和家庭(21%)三大领域。每年4月和10月是您最活跃的记录期，可能与春秋季旅行计划相关。您与'张明'共同拥有最多记忆(47条)，主要集中在2017-2020年间。"

如果输入只有：
- 核心领域: 用户的记忆主要集中在 教育(65%), 学习(25%)。

你的输出应该是：
"您的记忆主要集中在教育(65%)和学习(25%)两大领域，显示出您对知识和成长的持续关注。"'''

        user_prompt = "\n".join(prompt_parts)
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        response = await self.llm_client.chat(messages=messages)

        return response.content

    async def close(self):
        """
        Closes the database connection.
        """
        await self.neo4j_connector.close()


async def main():
    """
    Initializes and runs the memory insight analysis for a test user.
    """
    # 默认从 runtime.json selections.group_id 读取
    test_user_id = SELECTED_GROUP_ID
    print(f"正在为用户 {test_user_id} 生成记忆洞察报告...\n")

    insight = None
    try:
        insight = MemoryInsight(user_id=test_user_id)
        report = await insight.generate_insight_report()
        print("--- 记忆洞察报告 ---")
        print(report)
        print("---------------------")

        # 将结果写入统一的 User-Dashboard.json，使用全局配置路径
        try:
            from app.core.config import settings
            settings.ensure_memory_output_dir()
            output_dir = settings.MEMORY_OUTPUT_DIR
            try:
                os.makedirs(output_dir, exist_ok=True)
            except Exception:
                pass
            dashboard_path = os.path.join(output_dir, "User-Dashboard.json")
            existing = {}
            if os.path.exists(dashboard_path):
                with open(dashboard_path, "r", encoding="utf-8") as rf:
                    existing = json.load(rf)
            existing["memory_insight"] = {
                "group_id": test_user_id,
                "report": report
            }
            with open(dashboard_path, "w", encoding="utf-8") as wf:
                json.dump(existing, wf, ensure_ascii=False, indent=2)
            print(f"已写入 {dashboard_path} -> memory_insight")
        except Exception as e:
            print(f"写入 User-Dashboard.json 失败: {e}")
    except Exception as e:
        print(f"生成报告时出错: {e}")
    finally:
        if insight:
            await insight.close()


if __name__ == "__main__":
    # This setup allows running the async main function
    if sys.platform.startswith('win') and sys.version_info >= (3, 8):
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())
