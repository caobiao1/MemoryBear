"""
èƒå–å¼•æ“ - æµæ°´çº¿ç¼–æ’å™¨

è¯¥æ¨¡å—æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æµæ°´çº¿ç¼–æ’å™¨ï¼Œç”¨äºåè°ƒæ•´ä¸ªçŸ¥è¯†æå–æµç¨‹ã€‚
å®ƒæ•´åˆäº†æ•°æ®é¢„å¤„ç†ã€çŸ¥è¯†æå–ã€å»é‡æ¶ˆæ­§ç­‰æ¨¡å—ï¼Œæä¾›ç»Ÿä¸€çš„æ‰§è¡Œæ¥å£ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. åè°ƒæ•°æ®é¢„å¤„ç†ã€åˆ†å—ã€é™ˆè¿°å¥æå–ã€ä¸‰å…ƒç»„æå–ã€æ—¶é—´ä¿¡æ¯æå–ç­‰æ­¥éª¤
2. ç®¡ç†åµŒå…¥å‘é‡ç”Ÿæˆ
3. æ‰§è¡Œä¸¤é˜¶æ®µå»é‡å’Œæ¶ˆæ­§
4. å°†æå–ç»“æœè½¬æ¢ä¸ºå›¾æ•°æ®åº“èŠ‚ç‚¹å’Œè¾¹
5. æä¾›é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
6. æ”¯æŒè¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸å†™å…¥æ•°æ®åº“ï¼‰

ä½œè€…ï¼š
æ—¥æœŸï¼š2025-11-21
"""

import asyncio
import logging
import os
from typing import List, Dict, Any, Tuple, Optional, Callable, Awaitable
from datetime import datetime

from app.core.memory.models.message_models import DialogData
from app.core.memory.models.graph_models import (
    DialogueNode,
    ChunkNode,
    StatementNode,
    ExtractedEntityNode,
    StatementChunkEdge,
    StatementEntityEdge,
    EntityEntityEdge,
)
from app.core.memory.utils.data.ontology import TemporalInfo
from app.core.memory.models.variate_config import (
    ExtractionPipelineConfig,
    StatementExtractionConfig,
)
from app.core.memory.llm_tools.openai_client import LLMClient
from app.core.memory.llm_tools.openai_embedder import OpenAIEmbedderClient
from app.repositories.neo4j.neo4j_connector import Neo4jConnector

# å¯¼å…¥å„ä¸ªæå–æ¨¡å—
from app.core.memory.storage_services.extraction_engine.knowledge_extraction.statement_extraction import (
    StatementExtractor,
)
from app.core.memory.storage_services.extraction_engine.knowledge_extraction.triplet_extraction import (
    TripletExtractor,
)
from app.core.memory.storage_services.extraction_engine.knowledge_extraction.temporal_extraction import (
    TemporalExtractor,
)
from app.core.memory.storage_services.extraction_engine.knowledge_extraction.embedding_generation import (
    embedding_generation,
    embedding_generation_all,
    generate_entity_embeddings_from_triplets,
)
from app.core.memory.storage_services.extraction_engine.deduplication.two_stage_dedup import (
    dedup_layers_and_merge_and_return,
)
from app.core.memory.storage_services.extraction_engine.pipeline_help import (
    _write_extracted_result_summary,
    export_test_input_doc,
)

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class ExtractionOrchestrator:
    """
    çŸ¥è¯†æå–æµæ°´çº¿ç¼–æ’å™¨

    è¯¥ç±»è´Ÿè´£åè°ƒæ•´ä¸ªçŸ¥è¯†æå–æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
    1. é™ˆè¿°å¥æå–
    2. ä¸‰å…ƒç»„æå–
    3. æ—¶é—´ä¿¡æ¯æå–
    4. åµŒå…¥å‘é‡ç”Ÿæˆ
    5. æ•°æ®èµ‹å€¼åˆ°è¯­å¥
    6. èŠ‚ç‚¹å’Œè¾¹çš„åˆ›å»º
    7. ä¸¤é˜¶æ®µå»é‡å’Œæ¶ˆæ­§
    8. ç»“æœæ±‡æ€»å’Œè¾“å‡º

    Attributes:
        llm_client: LLM å®¢æˆ·ç«¯ï¼Œç”¨äºè°ƒç”¨å¤§è¯­è¨€æ¨¡å‹
        embedder_client: åµŒå…¥æ¨¡å‹å®¢æˆ·ç«¯ï¼Œç”¨äºç”Ÿæˆå‘é‡åµŒå…¥
        connector: Neo4j è¿æ¥å™¨ï¼Œç”¨äºæ•°æ®åº“æ“ä½œ
        config: æµæ°´çº¿é…ç½®
    """

    def __init__(
        self,
        llm_client: LLMClient,
        embedder_client: OpenAIEmbedderClient,
        connector: Neo4jConnector,
        config: Optional[ExtractionPipelineConfig] = None,
        progress_callback: Optional[Callable[[str, str, Optional[Dict[str, Any]]], Awaitable[None]]] = None,
    ):
        """
        åˆå§‹åŒ–æµæ°´çº¿ç¼–æ’å™¨

        Args:
            llm_client: LLM å®¢æˆ·ç«¯
            embedder_client: åµŒå…¥æ¨¡å‹å®¢æˆ·ç«¯
            connector: Neo4j è¿æ¥å™¨
            config: æµæ°´çº¿é…ç½®ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
                - æ¥å— (stage: str, message: str, data: Optional[Dict[str, Any]]) å¹¶è¿”å› Awaitable[None]
                - åœ¨ç®¡çº¿å…³é”®ç‚¹è°ƒç”¨ä»¥æŠ¥å‘Šè¿›åº¦å’Œç»“æœæ•°æ®
        """
        self.llm_client = llm_client
        self.embedder_client = embedder_client
        self.connector = connector
        self.config = config or ExtractionPipelineConfig()
        self.is_pilot_run = False  # é»˜è®¤éè¯•è¿è¡Œæ¨¡å¼
        self.progress_callback = progress_callback  # ä¿å­˜è¿›åº¦å›è°ƒå‡½æ•°
        
        # ä¿å­˜å»é‡æ¶ˆæ­§çš„è¯¦ç»†è®°å½•ï¼ˆå†…å­˜ä¸­çš„æ•°æ®ç»“æ„ï¼‰
        self.dedup_merge_records: List[Dict[str, Any]] = []  # å®ä½“åˆå¹¶è®°å½•
        self.dedup_disamb_records: List[Dict[str, Any]] = []  # å®ä½“æ¶ˆæ­§è®°å½•
        self.id_redirect_map: Dict[str, str] = {}  # IDé‡å®šå‘æ˜ å°„

        # åˆå§‹åŒ–å„ä¸ªæå–å™¨
        self.statement_extractor = StatementExtractor(
            llm_client=llm_client,
            config=self.config.statement_extraction,
        )
        self.triplet_extractor = TripletExtractor(llm_client=llm_client)
        self.temporal_extractor = TemporalExtractor(llm_client=llm_client)

        logger.info("ExtractionOrchestrator åˆå§‹åŒ–å®Œæˆ")

    async def run(
        self,
        dialog_data_list: List[DialogData],
        is_pilot_run: bool = False,
    ) -> Tuple[
        Tuple[List[DialogueNode], List[ChunkNode], List[StatementNode]],
        Tuple[List[ExtractedEntityNode], List[StatementEntityEdge], List[EntityEntityEdge]],
        Tuple[List[ExtractedEntityNode], List[StatementEntityEdge], List[EntityEntityEdge]],
    ]:
        """
        è¿è¡Œå®Œæ•´çš„çŸ¥è¯†æå–æµæ°´çº¿ï¼ˆä¼˜åŒ–ç‰ˆï¼šå¹¶è¡Œæ‰§è¡Œï¼‰

        è¯¥æ–¹æ³•åè°ƒæ‰€æœ‰æå–æ­¥éª¤ï¼Œä¼˜åŒ–æ‰§è¡Œé¡ºåºï¼š
        1. é™ˆè¿°å¥æå–
        2. å¹¶è¡Œæ‰§è¡Œï¼šä¸‰å…ƒç»„æå– + æ—¶é—´ä¿¡æ¯æå– + é™ˆè¿°å¥/åˆ†å—åµŒå…¥ç”Ÿæˆ
        3. å®ä½“åµŒå…¥ç”Ÿæˆï¼ˆä¾èµ–ä¸‰å…ƒç»„ï¼‰
        4. æ•°æ®èµ‹å€¼
        5. èŠ‚ç‚¹å’Œè¾¹åˆ›å»º
        6. ä¸¤é˜¶æ®µå»é‡
        7. ç»“æœæ±‡æ€»

        Args:
            dialog_data_list: å·²åˆ†å—çš„å¯¹è¯æ•°æ®åˆ—è¡¨
            is_pilot_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸å†™å…¥æ•°æ®åº“ï¼‰

        Returns:
            åŒ…å«ä¸‰ä¸ªå…ƒç»„çš„å…ƒç»„ï¼š
            - ç¬¬ä¸€ä¸ªå…ƒç»„ï¼š(å¯¹è¯èŠ‚ç‚¹åˆ—è¡¨, åˆ†å—èŠ‚ç‚¹åˆ—è¡¨, é™ˆè¿°å¥èŠ‚ç‚¹åˆ—è¡¨)
            - ç¬¬äºŒä¸ªå…ƒç»„ï¼šå»é‡å‰çš„ (å®ä½“èŠ‚ç‚¹åˆ—è¡¨, é™ˆè¿°å¥-å®ä½“è¾¹åˆ—è¡¨, å®ä½“-å®ä½“è¾¹åˆ—è¡¨)
            - ç¬¬ä¸‰ä¸ªå…ƒç»„ï¼šå»é‡åçš„ (å®ä½“èŠ‚ç‚¹åˆ—è¡¨, é™ˆè¿°å¥-å®ä½“è¾¹åˆ—è¡¨, å®ä½“-å®ä½“è¾¹åˆ—è¡¨)
        """
        try:
            # è®¾ç½®è¯•è¿è¡Œæ¨¡å¼æ ‡å¿—
            self.is_pilot_run = is_pilot_run
            mode_str = "è¯•è¿è¡Œæ¨¡å¼" if is_pilot_run else "æ­£å¼æ¨¡å¼"
            logger.info(f"å¼€å§‹è¿è¡ŒçŸ¥è¯†æå–æµæ°´çº¿ï¼ˆä¼˜åŒ–ç‰ˆ - {mode_str}ï¼‰ï¼Œå…± {len(dialog_data_list)} ä¸ªå¯¹è¯")

            # æ­¥éª¤ 1: é™ˆè¿°å¥æå–
            logger.info("æ­¥éª¤ 1/6: é™ˆè¿°å¥æå–ï¼ˆå…¨å±€åˆ†å—çº§å¹¶è¡Œï¼‰")
            dialog_data_list = await self._extract_statements(dialog_data_list)
            
            # æ”¶é›†é™ˆè¿°å¥å†…å®¹å’Œç»Ÿè®¡æ•°é‡
            all_statements_list = []
            for dialog in dialog_data_list:
                for chunk in dialog.chunks:
                    all_statements_list.extend(chunk.statements)
            total_statements = len(all_statements_list)

            # ğŸ”¥ é™ˆè¿°å¥æå–å®Œæˆåï¼Œç«‹å³å‘é€çŸ¥è¯†æŠ½å–å®Œæˆæ¶ˆæ¯
            if self.progress_callback:
                extraction_stats = {
                    "statements_count": total_statements,
                    "entities_count": 0,  # æš‚æ—¶ä¸º0ï¼Œåç»­ä¼šæ›´æ–°
                    "triplets_count": 0,  # æš‚æ—¶ä¸º0ï¼Œåç»­ä¼šæ›´æ–°
                    "temporal_ranges_count": 0,  # æš‚æ—¶ä¸º0ï¼Œåç»­ä¼šæ›´æ–°
                }
                await self.progress_callback("knowledge_extraction_complete", "çŸ¥è¯†æŠ½å–å®Œæˆ", extraction_stats)
                
                # ğŸ”¥ ç«‹å³å‘é€ä¸‹ä¸€é˜¶æ®µçš„å¼€å§‹æ¶ˆæ¯ï¼Œè®©å‰ç«¯çŸ¥é“è¿›å…¥äº†åˆ›å»ºèŠ‚ç‚¹å’Œè¾¹é˜¶æ®µ
                await self.progress_callback("creating_nodes_edges", "æ­£åœ¨åˆ›å»ºèŠ‚ç‚¹å’Œè¾¹...")

            # æ­¥éª¤ 2: å¹¶è¡Œæ‰§è¡Œä¸‰å…ƒç»„æå–ã€æ—¶é—´ä¿¡æ¯æå–å’ŒåŸºç¡€åµŒå…¥ç”Ÿæˆï¼ˆåå°é™é»˜æ‰§è¡Œï¼‰
            logger.info("æ­¥éª¤ 2/6: å¹¶è¡Œæ‰§è¡Œä¸‰å…ƒç»„æå–ã€æ—¶é—´ä¿¡æ¯æå–å’ŒåµŒå…¥ç”Ÿæˆï¼ˆåå°é™é»˜æ‰§è¡Œï¼‰")
            (
                triplet_maps,
                temporal_maps,
                statement_embedding_maps,
                chunk_embedding_maps,
                dialog_embeddings,
            ) = await self._parallel_extract_and_embed(dialog_data_list)
            
            # æ”¶é›†å®ä½“å’Œä¸‰å…ƒç»„å†…å®¹ï¼Œå¹¶ç»Ÿè®¡æ•°é‡
            all_entities_list = []
            all_triplets_list = []
            for triplet_map in triplet_maps:
                for triplet_info in triplet_map.values():
                    if triplet_info:
                        all_entities_list.extend(triplet_info.entities)
                        all_triplets_list.extend(triplet_info.triplets)
            
            total_entities = len(all_entities_list)
            total_triplets = len(all_triplets_list)
            total_temporal = sum(len(temporal_map) for temporal_map in temporal_maps)

            # æ­¥éª¤ 3: ç”Ÿæˆå®ä½“åµŒå…¥ï¼ˆä¾èµ–ä¸‰å…ƒç»„æå–ç»“æœï¼‰
            logger.info("æ­¥éª¤ 3/6: ç”Ÿæˆå®ä½“åµŒå…¥")
            triplet_maps = await self._generate_entity_embeddings(triplet_maps)

            # æ­¥éª¤ 4: å°†æå–çš„æ•°æ®èµ‹å€¼åˆ°è¯­å¥
            logger.info("æ­¥éª¤ 4/6: æ•°æ®èµ‹å€¼")
            dialog_data_list = await self._assign_extracted_data(
                dialog_data_list,
                temporal_maps,
                triplet_maps,
                statement_embedding_maps,
                chunk_embedding_maps,
                dialog_embeddings,
            )

            # æ­¥éª¤ 5: åˆ›å»ºèŠ‚ç‚¹å’Œè¾¹
            logger.info("æ­¥éª¤ 5/6: åˆ›å»ºèŠ‚ç‚¹å’Œè¾¹")
            
            # æ³¨æ„ï¼šcreating_nodes_edges æ¶ˆæ¯å·²åœ¨çŸ¥è¯†æŠ½å–å®Œæˆåç«‹å³å‘é€
            
            (
                dialogue_nodes,
                chunk_nodes,
                statement_nodes,
                entity_nodes,
                statement_chunk_edges,
                statement_entity_edges,
                entity_entity_edges,
            ) = await self._create_nodes_and_edges(dialog_data_list)

            # å¯¼å‡ºå»é‡å‰çš„æµ‹è¯•è¾“å…¥æ–‡æ¡£ï¼ˆè¯•è¿è¡Œå’Œæ­£å¼æ¨¡å¼éƒ½éœ€è¦ï¼Œç”¨äºç”Ÿæˆç»“æœæ±‡æ€»ï¼‰
            export_test_input_doc(entity_nodes, statement_entity_edges, entity_entity_edges)

            # æ­¥éª¤ 6: ä¸¤é˜¶æ®µå»é‡å’Œæ¶ˆæ­§
            if is_pilot_run:
                logger.info("æ­¥éª¤ 6/6: å»é‡å’Œæ¶ˆæ­§ï¼ˆè¯•è¿è¡Œæ¨¡å¼ï¼šä»…ç¬¬ä¸€å±‚å»é‡ï¼‰")
            else:
                logger.info("æ­¥éª¤ 6/6: ä¸¤é˜¶æ®µå»é‡å’Œæ¶ˆæ­§")
            
            # æ³¨æ„ï¼šdeduplication æ¶ˆæ¯å·²åœ¨åˆ›å»ºèŠ‚ç‚¹å’Œè¾¹å®Œæˆåç«‹å³å‘é€
            
            result = await self._run_dedup_and_write_summary(
                dialogue_nodes,
                chunk_nodes,
                statement_nodes,
                entity_nodes,
                statement_chunk_edges,
                statement_entity_edges,
                entity_entity_edges,
                dialog_data_list,
            )



            logger.info(f"çŸ¥è¯†æå–æµæ°´çº¿è¿è¡Œå®Œæˆï¼ˆ{mode_str}ï¼‰")
            return result

        except Exception as e:
            logger.error(f"çŸ¥è¯†æå–æµæ°´çº¿è¿è¡Œå¤±è´¥: {e}", exc_info=True)
            raise

    async def _extract_statements(
        self, dialog_data_list: List[DialogData]
    ) -> List[DialogData]:
        """
        ä»å¯¹è¯ä¸­æå–é™ˆè¿°å¥ï¼ˆæµå¼è¾“å‡ºç‰ˆæœ¬ï¼šè¾¹æå–è¾¹å‘é€è¿›åº¦ï¼‰

        Args:
            dialog_data_list: å¯¹è¯æ•°æ®åˆ—è¡¨

        Returns:
            æ›´æ–°åçš„å¯¹è¯æ•°æ®åˆ—è¡¨ï¼ˆåŒ…å«æå–çš„é™ˆè¿°å¥ï¼‰
        """
        logger.info("å¼€å§‹é™ˆè¿°å¥æå–ï¼ˆå…¨å±€åˆ†å—çº§å¹¶è¡Œ + æµå¼è¾“å‡ºï¼‰")

        # æ”¶é›†æ‰€æœ‰åˆ†å—åŠå…¶å…ƒæ•°æ®
        all_chunks = []
        chunk_metadata = []  # (dialog_idx, chunk_idx)
        
        for d_idx, dialog in enumerate(dialog_data_list):
            dialogue_content = dialog.content if self.config.statement_extraction.include_dialogue_context else None
            for c_idx, chunk in enumerate(dialog.chunks):
                all_chunks.append((chunk, dialog.group_id, dialogue_content))
                chunk_metadata.append((d_idx, c_idx))

        logger.info(f"æ”¶é›†åˆ° {len(all_chunks)} ä¸ªåˆ†å—ï¼Œå¼€å§‹å…¨å±€å¹¶è¡Œæå–")
        
        # ç”¨äºè·Ÿè¸ªå·²å®Œæˆçš„åˆ†å—æ•°é‡
        completed_chunks = 0
        total_chunks = len(all_chunks)

        # å…¨å±€å¹¶è¡Œå¤„ç†æ‰€æœ‰åˆ†å—
        async def extract_for_chunk(chunk_data, chunk_index):
            nonlocal completed_chunks
            chunk, group_id, dialogue_content = chunk_data
            try:
                statements = await self.statement_extractor._extract_statements(chunk, group_id, dialogue_content)
                
                #  æµå¼è¾“å‡ºï¼šæ¯æå–å®Œä¸€ä¸ªåˆ†å—çš„é™ˆè¿°å¥ï¼Œç«‹å³å‘é€è¿›åº¦
                # æ³¨æ„ï¼šåªåœ¨è¯•è¿è¡Œæ¨¡å¼ä¸‹å‘é€é™ˆè¿°å¥è¯¦æƒ…ï¼Œæ­£å¼æ¨¡å¼ä¸å‘é€
                completed_chunks += 1
                if self.progress_callback and statements and self.is_pilot_run:
                    # å‘é€å‰3ä¸ªé™ˆè¿°å¥ä½œä¸ºç¤ºä¾‹
                    for idx, stmt in enumerate(statements[:3]):
                        stmt_result = {
                            "extraction_type": "statement",
                            "statement": stmt.statement,
                            "statement_id": stmt.id,
                            "chunk_progress": f"{completed_chunks}/{total_chunks}",
                            "statement_index_in_chunk": idx + 1
                        }
                        await self.progress_callback(
                            "knowledge_extraction_result", 
                            f"é™ˆè¿°å¥æå–ä¸­ ({completed_chunks}/{total_chunks})", 
                            stmt_result
                        )
                
                return statements
            except Exception as e:
                logger.error(f"åˆ†å— {chunk.id} é™ˆè¿°å¥æå–å¤±è´¥: {e}")
                completed_chunks += 1
                return []

        tasks = [extract_for_chunk(chunk_data, i) for i, chunk_data in enumerate(all_chunks)]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å°†ç»“æœåˆ†é…å›å¯¹è¯
        for i, result in enumerate(results):
            d_idx, c_idx = chunk_metadata[i]
            if isinstance(result, Exception):
                logger.error(f"åˆ†å—å¤„ç†å¼‚å¸¸: {result}")
                dialog_data_list[d_idx].chunks[c_idx].statements = []
            elif isinstance(result, list):
                dialog_data_list[d_idx].chunks[c_idx].statements = result
            else:
                dialog_data_list[d_idx].chunks[c_idx].statements = []

        # ç»Ÿè®¡å¹¶ä¿å­˜ï¼ˆè¯•è¿è¡Œå’Œæ­£å¼æ¨¡å¼éƒ½éœ€è¦ä¿å­˜ï¼Œç”¨äºç”Ÿæˆç»“æœæ±‡æ€»ï¼‰
        all_statements = []
        for dialog in dialog_data_list:
            for chunk in dialog.chunks:
                if chunk.statements:
                    all_statements.extend(chunk.statements)

        # ä¿å­˜é™ˆè¿°å¥åˆ°æ–‡ä»¶ï¼ˆè¯•è¿è¡Œå’Œæ­£å¼æ¨¡å¼éƒ½éœ€è¦ï¼‰
        self.statement_extractor.save_statements(all_statements)
        
        logger.info(f"é™ˆè¿°å¥æå–å®Œæˆï¼Œå…±æå– {len(all_statements)} æ¡é™ˆè¿°å¥")

        return dialog_data_list

    async def _extract_triplets(
        self, dialog_data_list: List[DialogData]
    ) -> List[Dict[str, Any]]:
        """
        ä»å¯¹è¯ä¸­æå–ä¸‰å…ƒç»„ï¼ˆæµå¼è¾“å‡ºç‰ˆæœ¬ï¼šè¾¹æå–è¾¹å‘é€è¿›åº¦ï¼‰

        Args:
            dialog_data_list: å¯¹è¯æ•°æ®åˆ—è¡¨

        Returns:
            ä¸‰å…ƒç»„æ˜ å°„åˆ—è¡¨ï¼Œæ¯ä¸ªå¯¹è¯å¯¹åº”ä¸€ä¸ªå­—å…¸
        """
        logger.info("å¼€å§‹ä¸‰å…ƒç»„æå–ï¼ˆå…¨å±€é™ˆè¿°å¥çº§å¹¶è¡Œ + æµå¼è¾“å‡ºï¼‰")

        # æ”¶é›†æ‰€æœ‰é™ˆè¿°å¥åŠå…¶å…ƒæ•°æ®
        all_statements = []
        statement_metadata = []  # (dialog_idx, statement_id, chunk_content)
        
        for d_idx, dialog in enumerate(dialog_data_list):
            for chunk in dialog.chunks:
                for statement in chunk.statements:
                    all_statements.append((statement, chunk.content))
                    statement_metadata.append((d_idx, statement.id))

        logger.info(f"æ”¶é›†åˆ° {len(all_statements)} ä¸ªé™ˆè¿°å¥ï¼Œå¼€å§‹å…¨å±€å¹¶è¡Œæå–ä¸‰å…ƒç»„")
        
        # ç”¨äºè·Ÿè¸ªå·²å®Œæˆçš„é™ˆè¿°å¥æ•°é‡
        completed_statements = 0
        total_statements = len(all_statements)

        # å…¨å±€å¹¶è¡Œå¤„ç†æ‰€æœ‰é™ˆè¿°å¥
        async def extract_for_statement(stmt_data, stmt_index):
            nonlocal completed_statements
            statement, chunk_content = stmt_data
            try:
                triplet_info = await self.triplet_extractor._extract_triplets(statement, chunk_content)
                
                # æ³¨æ„ï¼šä¸å†å‘é€ä¸‰å…ƒç»„æå–çš„æµå¼è¾“å‡º
                # ä¸‰å…ƒç»„æå–åœ¨åå°æ‰§è¡Œï¼Œä½†ä¸å‘å‰ç«¯å‘é€è¯¦ç»†ä¿¡æ¯
                completed_statements += 1
                
                return triplet_info
            except Exception as e:
                logger.error(f"é™ˆè¿°å¥ {statement.id} ä¸‰å…ƒç»„æå–å¤±è´¥: {e}")
                completed_statements += 1
                from app.core.memory.models.triplet_models import TripletExtractionResponse
                return TripletExtractionResponse(triplets=[], entities=[])

        tasks = [extract_for_statement(stmt_data, i) for i, stmt_data in enumerate(all_statements)]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å°†ç»“æœç»„ç»‡æˆå¯¹è¯çº§åˆ«çš„æ˜ å°„
        triplet_maps = [{} for _ in dialog_data_list]
        all_responses = []
        
        for i, result in enumerate(results):
            d_idx, stmt_id = statement_metadata[i]
            if isinstance(result, Exception):
                logger.error(f"é™ˆè¿°å¥å¤„ç†å¼‚å¸¸: {result}")
                from app.core.memory.models.triplet_models import TripletExtractionResponse
                triplet_maps[d_idx][stmt_id] = TripletExtractionResponse(triplets=[], entities=[])
            else:
                triplet_maps[d_idx][stmt_id] = result
                all_responses.append(result)

        # ç»Ÿè®¡æå–ç»“æœ
        total_triplets = sum(len(m) for m in triplet_maps)
        logger.info(f"ä¸‰å…ƒç»„æå–å®Œæˆï¼Œå…±æå– {total_triplets} ä¸ªä¸‰å…ƒç»„")

        # ä¿å­˜ä¸‰å…ƒç»„åˆ°æ–‡ä»¶ï¼ˆè¯•è¿è¡Œå’Œæ­£å¼æ¨¡å¼éƒ½éœ€è¦ï¼Œç”¨äºç”Ÿæˆç»“æœæ±‡æ€»ï¼‰
        if all_responses:
            try:
                self.triplet_extractor.save_triplets(all_responses)
                logger.info("ä¸‰å…ƒç»„æ•°æ®å·²ä¿å­˜åˆ°æ–‡ä»¶")
            except Exception as e:
                logger.error(f"ä¿å­˜ä¸‰å…ƒç»„åˆ°æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)

        return triplet_maps

    async def _extract_temporal(
        self, dialog_data_list: List[DialogData]
    ) -> List[Dict[str, Any]]:
        """
        ä»å¯¹è¯ä¸­æå–æ—¶é—´ä¿¡æ¯ï¼ˆæµå¼è¾“å‡ºç‰ˆæœ¬ï¼šè¾¹æå–è¾¹å‘é€è¿›åº¦ï¼‰

        Args:
            dialog_data_list: å¯¹è¯æ•°æ®åˆ—è¡¨

        Returns:
            æ—¶é—´ä¿¡æ¯æ˜ å°„åˆ—è¡¨ï¼Œæ¯ä¸ªå¯¹è¯å¯¹åº”ä¸€ä¸ªå­—å…¸
        """
        # è¯•è¿è¡Œæ¨¡å¼ï¼šè·³è¿‡æ—¶é—´æå–ä»¥èŠ‚çœæ—¶é—´
        if self.is_pilot_run:
            logger.info("è¯•è¿è¡Œæ¨¡å¼ï¼šè·³è¿‡æ—¶é—´ä¿¡æ¯æå–ï¼ˆèŠ‚çœçº¦ 10-15 ç§’ï¼‰")
            # ä¸ºæ‰€æœ‰é™ˆè¿°å¥è¿”å›ç©ºçš„æ—¶é—´èŒƒå›´
            from app.core.memory.models.message_models import TemporalValidityRange
            temporal_maps = []
            for dialog in dialog_data_list:
                temporal_map = {}
                for chunk in dialog.chunks:
                    for statement in chunk.statements:
                        temporal_map[statement.id] = TemporalValidityRange(valid_at=None, invalid_at=None)
                temporal_maps.append(temporal_map)
            return temporal_maps
        
        logger.info("å¼€å§‹æ—¶é—´ä¿¡æ¯æå–ï¼ˆå…¨å±€é™ˆè¿°å¥çº§å¹¶è¡Œ + æµå¼è¾“å‡ºï¼‰")

        # æ”¶é›†æ‰€æœ‰éœ€è¦æå–æ—¶é—´çš„é™ˆè¿°å¥
        all_statements = []
        statement_metadata = []  # (dialog_idx, statement_id, ref_dates)
        
        for d_idx, dialog in enumerate(dialog_data_list):
            # è·å–å‚è€ƒæ—¥æœŸ
            ref_dates = {}
            if hasattr(dialog, 'metadata') and dialog.metadata:
                if 'conversation_date' in dialog.metadata:
                    ref_dates['conversation_date'] = dialog.metadata['conversation_date']
                if 'publication_date' in dialog.metadata:
                    ref_dates['publication_date'] = dialog.metadata['publication_date']
            
            if not ref_dates:
                from datetime import datetime
                ref_dates = {"today": datetime.now().strftime("%Y-%m-%d")}
            
            for chunk in dialog.chunks:
                for statement in chunk.statements:
                    # è·³è¿‡ ATEMPORAL ç±»å‹çš„é™ˆè¿°å¥
                    from app.core.memory.utils.data.ontology import TemporalInfo
                    if statement.temporal_info != TemporalInfo.ATEMPORAL:
                        all_statements.append((statement, ref_dates))
                        statement_metadata.append((d_idx, statement.id))

        logger.info(f"æ”¶é›†åˆ° {len(all_statements)} ä¸ªéœ€è¦æ—¶é—´æå–çš„é™ˆè¿°å¥ï¼Œå¼€å§‹å…¨å±€å¹¶è¡Œæå–")
        
        # ç”¨äºè·Ÿè¸ªå·²å®Œæˆçš„æ—¶é—´æå–æ•°é‡
        completed_temporal = 0
        total_temporal_statements = len(all_statements)

        # å…¨å±€å¹¶è¡Œå¤„ç†æ‰€æœ‰é™ˆè¿°å¥
        async def extract_for_statement(stmt_data, stmt_index):
            nonlocal completed_temporal
            statement, ref_dates = stmt_data
            try:
                temporal_range = await self.temporal_extractor._extract_temporal_ranges(statement, ref_dates)
                
                # æ³¨æ„ï¼šä¸å†å‘é€æ—¶é—´æå–çš„æµå¼è¾“å‡º
                # æ—¶é—´æå–åœ¨åå°æ‰§è¡Œï¼Œä½†ä¸å‘å‰ç«¯å‘é€è¯¦ç»†ä¿¡æ¯
                completed_temporal += 1
                
                return temporal_range
            except Exception as e:
                logger.error(f"é™ˆè¿°å¥ {statement.id} æ—¶é—´ä¿¡æ¯æå–å¤±è´¥: {e}")
                completed_temporal += 1
                from app.core.memory.models.message_models import TemporalValidityRange
                return TemporalValidityRange(valid_at=None, invalid_at=None)

        tasks = [extract_for_statement(stmt_data, i) for i, stmt_data in enumerate(all_statements)]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å°†ç»“æœç»„ç»‡æˆå¯¹è¯çº§åˆ«çš„æ˜ å°„
        temporal_maps = [{} for _ in dialog_data_list]
        
        for i, result in enumerate(results):
            d_idx, stmt_id = statement_metadata[i]
            if isinstance(result, Exception):
                logger.error(f"é™ˆè¿°å¥å¤„ç†å¼‚å¸¸: {result}")
                from app.core.memory.models.message_models import TemporalValidityRange
                temporal_maps[d_idx][stmt_id] = TemporalValidityRange(valid_at=None, invalid_at=None)
            else:
                temporal_maps[d_idx][stmt_id] = result

        # ä¸º ATEMPORAL é™ˆè¿°å¥æ·»åŠ ç©ºçš„æ—¶é—´èŒƒå›´
        from app.core.memory.utils.data.ontology import TemporalInfo
        from app.core.memory.models.message_models import TemporalValidityRange
        for d_idx, dialog in enumerate(dialog_data_list):
            for chunk in dialog.chunks:
                for statement in chunk.statements:
                    if statement.temporal_info == TemporalInfo.ATEMPORAL and statement.id not in temporal_maps[d_idx]:
                        temporal_maps[d_idx][statement.id] = TemporalValidityRange(valid_at=None, invalid_at=None)

        # ç»Ÿè®¡æå–ç»“æœ
        total_temporal = sum(len(m) for m in temporal_maps)
        logger.info(f"æ—¶é—´ä¿¡æ¯æå–å®Œæˆï¼Œå…±æå– {total_temporal} ä¸ªæ—¶é—´èŒƒå›´")

        return temporal_maps

    async def _parallel_extract_and_embed(
        self, dialog_data_list: List[DialogData]
    ) -> Tuple[
        List[Dict[str, Any]],
        List[Dict[str, Any]],
        List[Dict[str, List[float]]],
        List[Dict[str, List[float]]],
        List[List[float]],
    ]:
        """
        å¹¶è¡Œæ‰§è¡Œä¸‰å…ƒç»„æå–ã€æ—¶é—´ä¿¡æ¯æå–å’ŒåŸºç¡€åµŒå…¥ç”Ÿæˆ

        è¿™ä¸‰ä¸ªä»»åŠ¡éƒ½ä¾èµ–é™ˆè¿°å¥æå–çš„ç»“æœï¼Œä½†å½¼æ­¤ç‹¬ç«‹ï¼Œå¯ä»¥å¹¶è¡Œæ‰§è¡Œï¼š
        - ä¸‰å…ƒç»„æå–ï¼šä»é™ˆè¿°å¥ä¸­æå–å®ä½“å’Œå…³ç³»
        - æ—¶é—´ä¿¡æ¯æå–ï¼šä»é™ˆè¿°å¥ä¸­æå–æ—¶é—´èŒƒå›´
        - åµŒå…¥ç”Ÿæˆï¼šä¸ºé™ˆè¿°å¥ã€åˆ†å—å’Œå¯¹è¯ç”Ÿæˆå‘é‡ï¼ˆä¸ä¾èµ–ä¸‰å…ƒç»„ï¼‰

        Args:
            dialog_data_list: å¯¹è¯æ•°æ®åˆ—è¡¨

        Returns:
            äº”ä¸ªåˆ—è¡¨çš„å…ƒç»„ï¼š
            - ä¸‰å…ƒç»„æ˜ å°„åˆ—è¡¨
            - æ—¶é—´ä¿¡æ¯æ˜ å°„åˆ—è¡¨
            - é™ˆè¿°å¥åµŒå…¥æ˜ å°„åˆ—è¡¨
            - åˆ†å—åµŒå…¥æ˜ å°„åˆ—è¡¨
            - å¯¹è¯åµŒå…¥åˆ—è¡¨
        """
        logger.info("å¹¶è¡Œæ‰§è¡Œï¼šä¸‰å…ƒç»„æå– + æ—¶é—´ä¿¡æ¯æå– + åŸºç¡€åµŒå…¥ç”Ÿæˆ")

        # åˆ›å»ºä¸‰ä¸ªå¹¶è¡Œä»»åŠ¡
        triplet_task = self._extract_triplets(dialog_data_list)
        temporal_task = self._extract_temporal(dialog_data_list)
        embedding_task = self._generate_basic_embeddings(dialog_data_list)

        # å¹¶è¡Œæ‰§è¡Œ
        results = await asyncio.gather(
            triplet_task,
            temporal_task,
            embedding_task,
            return_exceptions=True
        )

        # è§£åŒ…ç»“æœ
        triplet_maps = results[0] if not isinstance(results[0], Exception) else [{} for _ in dialog_data_list]
        temporal_maps = results[1] if not isinstance(results[1], Exception) else [{} for _ in dialog_data_list]
        
        if isinstance(results[2], Exception):
            logger.error(f"åŸºç¡€åµŒå…¥ç”Ÿæˆå¤±è´¥: {results[2]}")
            statement_embedding_maps = [{} for _ in dialog_data_list]
            chunk_embedding_maps = [{} for _ in dialog_data_list]
            dialog_embeddings = [[] for _ in dialog_data_list]
        else:
            statement_embedding_maps, chunk_embedding_maps, dialog_embeddings = results[2]

        logger.info("å¹¶è¡Œä»»åŠ¡æ‰§è¡Œå®Œæˆ")
        return (
            triplet_maps,
            temporal_maps,
            statement_embedding_maps,
            chunk_embedding_maps,
            dialog_embeddings,
        )

    async def _generate_basic_embeddings(
        self, dialog_data_list: List[DialogData]
    ) -> Tuple[List[Dict[str, List[float]]], List[Dict[str, List[float]]], List[List[float]]]:
        """
        ç”ŸæˆåŸºç¡€åµŒå…¥å‘é‡ï¼ˆé™ˆè¿°å¥ã€åˆ†å—ã€å¯¹è¯ï¼‰

        è¿™äº›åµŒå…¥ä¸ä¾èµ–ä¸‰å…ƒç»„æå–ç»“æœï¼Œå¯ä»¥æå‰ç”Ÿæˆ
        åœ¨è¯•è¿è¡Œæ¨¡å¼ä¸‹ï¼Œè·³è¿‡åµŒå…¥ç”Ÿæˆä»¥èŠ‚çœæ—¶é—´

        Args:
            dialog_data_list: å¯¹è¯æ•°æ®åˆ—è¡¨

        Returns:
            ä¸‰ä¸ªåˆ—è¡¨çš„å…ƒç»„ï¼š
            - é™ˆè¿°å¥åµŒå…¥æ˜ å°„åˆ—è¡¨
            - åˆ†å—åµŒå…¥æ˜ å°„åˆ—è¡¨
            - å¯¹è¯åµŒå…¥åˆ—è¡¨
        """
        # è¯•è¿è¡Œæ¨¡å¼ï¼šè·³è¿‡åµŒå…¥ç”Ÿæˆ
        if self.is_pilot_run:
            logger.info("è¯•è¿è¡Œæ¨¡å¼ï¼šè·³è¿‡åŸºç¡€åµŒå…¥ç”Ÿæˆï¼ˆèŠ‚çœçº¦ 20 ç§’ï¼‰")
            return (
                [{} for _ in dialog_data_list],
                [{} for _ in dialog_data_list],
                [[] for _ in dialog_data_list],
            )

        logger.info("å¼€å§‹ç”ŸæˆåŸºç¡€åµŒå…¥å‘é‡ï¼ˆé™ˆè¿°å¥ã€åˆ†å—ã€å¯¹è¯ï¼‰")

        try:
            # ä» runtime.json è·å–åµŒå…¥æ¨¡å‹é…ç½®ID
            from app.core.memory.utils.config import definitions as config_defs
            embedding_id = config_defs.SELECTED_EMBEDDING_ID
            
            if not embedding_id:
                logger.error("æœªåœ¨ runtime.json ä¸­é…ç½® embedding æ¨¡å‹ ID")
                raise ValueError("æœªé…ç½®åµŒå…¥æ¨¡å‹ID")
            
            # åªç”Ÿæˆé™ˆè¿°å¥ã€åˆ†å—å’Œå¯¹è¯çš„åµŒå…¥ï¼ˆä¸åŒ…æ‹¬å®ä½“ï¼‰
            statement_embedding_maps, chunk_embedding_maps, dialog_embeddings = await embedding_generation(
                dialog_data_list, embedding_id
            )

            # ç»Ÿè®¡ç”Ÿæˆç»“æœ
            total_statement_embeddings = sum(len(m) for m in statement_embedding_maps)
            total_chunk_embeddings = sum(len(m) for m in chunk_embedding_maps)
            logger.info(
                f"åŸºç¡€åµŒå…¥ç”Ÿæˆå®Œæˆï¼š{total_statement_embeddings} ä¸ªé™ˆè¿°å¥åµŒå…¥ï¼Œ"
                f"{total_chunk_embeddings} ä¸ªåˆ†å—åµŒå…¥ï¼Œ{len(dialog_embeddings)} ä¸ªå¯¹è¯åµŒå…¥"
            )

            return statement_embedding_maps, chunk_embedding_maps, dialog_embeddings

        except Exception as e:
            logger.error(f"åŸºç¡€åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            # è¿”å›ç©ºç»“æœ
            return (
                [{} for _ in dialog_data_list],
                [{} for _ in dialog_data_list],
                [[] for _ in dialog_data_list],
            )

    async def _generate_entity_embeddings(
        self, triplet_maps: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆå®ä½“åµŒå…¥å‘é‡

        åœ¨è¯•è¿è¡Œæ¨¡å¼ä¸‹ï¼Œè·³è¿‡å®ä½“åµŒå…¥ç”Ÿæˆä»¥èŠ‚çœæ—¶é—´

        Args:
            triplet_maps: ä¸‰å…ƒç»„æ˜ å°„åˆ—è¡¨

        Returns:
            æ›´æ–°åçš„ä¸‰å…ƒç»„æ˜ å°„åˆ—è¡¨ï¼ˆåŒ…å«å®ä½“åµŒå…¥ï¼‰
        """
        # è¯•è¿è¡Œæ¨¡å¼ï¼šè·³è¿‡å®ä½“åµŒå…¥ç”Ÿæˆ
        if self.is_pilot_run:
            logger.info("è¯•è¿è¡Œæ¨¡å¼ï¼šè·³è¿‡å®ä½“åµŒå…¥ç”Ÿæˆï¼ˆèŠ‚çœçº¦ 5-8 ç§’ï¼‰")
            return triplet_maps

        logger.info("å¼€å§‹ç”Ÿæˆå®ä½“åµŒå…¥å‘é‡")

        try:
            # ä» runtime.json è·å–åµŒå…¥æ¨¡å‹é…ç½®ID
            from app.core.memory.utils.config import definitions as config_defs
            embedding_id = config_defs.SELECTED_EMBEDDING_ID
            
            if not embedding_id:
                logger.error("æœªåœ¨ runtime.json ä¸­é…ç½® embedding æ¨¡å‹ ID")
                return triplet_maps
            
            # ç”Ÿæˆå®ä½“åµŒå…¥
            updated_triplet_maps = await generate_entity_embeddings_from_triplets(
                triplet_maps, embedding_id
            )

            logger.info("å®ä½“åµŒå…¥ç”Ÿæˆå®Œæˆ")
            return updated_triplet_maps

        except Exception as e:
            logger.error(f"å®ä½“åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            return triplet_maps



    async def _assign_extracted_data(
        self,
        dialog_data_list: List[DialogData],
        temporal_maps: List[Dict[str, Any]],
        triplet_maps: List[Dict[str, Any]],
        statement_embedding_maps: List[Dict[str, List[float]]],
        chunk_embedding_maps: List[Dict[str, List[float]]],
        dialog_embeddings: List[List[float]],
    ) -> List[DialogData]:
        """
        å°†æå–çš„æ•°æ®èµ‹å€¼åˆ°è¯­å¥

        Args:
            dialog_data_list: å¯¹è¯æ•°æ®åˆ—è¡¨
            temporal_maps: æ—¶é—´ä¿¡æ¯æ˜ å°„åˆ—è¡¨
            triplet_maps: ä¸‰å…ƒç»„æ˜ å°„åˆ—è¡¨
            statement_embedding_maps: é™ˆè¿°å¥åµŒå…¥æ˜ å°„åˆ—è¡¨
            chunk_embedding_maps: åˆ†å—åµŒå…¥æ˜ å°„åˆ—è¡¨
            dialog_embeddings: å¯¹è¯åµŒå…¥åˆ—è¡¨

        Returns:
            æ›´æ–°åçš„å¯¹è¯æ•°æ®åˆ—è¡¨
        """
        logger.info("å¼€å§‹å°†æå–æ•°æ®èµ‹å€¼åˆ°è¯­å¥")

        # ç¡®ä¿åˆ—è¡¨é•¿åº¦åŒ¹é…
        expected_length = len(dialog_data_list)
        if (
            len(temporal_maps) != expected_length
            or len(triplet_maps) != expected_length
            or len(statement_embedding_maps) != expected_length
            or len(chunk_embedding_maps) != expected_length
            or len(dialog_embeddings) != expected_length
        ):
            logger.warning(
                f"æ•°æ®å¤§å°ä¸åŒ¹é… - å¯¹è¯: {len(dialog_data_list)}, "
                f"æ—¶é—´æ˜ å°„: {len(temporal_maps)}, ä¸‰å…ƒç»„æ˜ å°„: {len(triplet_maps)}, "
                f"é™ˆè¿°å¥åµŒå…¥: {len(statement_embedding_maps)}, "
                f"åˆ†å—åµŒå…¥: {len(chunk_embedding_maps)}, "
                f"å¯¹è¯åµŒå…¥: {len(dialog_embeddings)}"
            )

        total_statements = 0
        assigned_temporal = 0
        assigned_triplets = 0
        assigned_statement_embeddings = 0
        assigned_chunk_embeddings = 0
        assigned_dialog_embeddings = 0

        # å¤„ç†æ¯ä¸ªå¯¹è¯
        for i, dialog_data in enumerate(dialog_data_list):
            # æ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±çš„æ•°æ®
            if i >= len(temporal_maps) or i >= len(triplet_maps):
                logger.warning(f"å¯¹è¯ {dialog_data.id} ç¼ºå°‘æå–æ•°æ®ï¼Œè·³è¿‡èµ‹å€¼")
                continue

            temporal_map = temporal_maps[i]
            triplet_map = triplet_maps[i]
            statement_embedding_map = statement_embedding_maps[i] if i < len(statement_embedding_maps) else {}
            chunk_embedding_map = chunk_embedding_maps[i] if i < len(chunk_embedding_maps) else {}
            dialog_embedding = dialog_embeddings[i] if i < len(dialog_embeddings) else []

            # èµ‹å€¼å¯¹è¯åµŒå…¥
            if dialog_embedding:
                dialog_data.dialog_embedding = dialog_embedding
                assigned_dialog_embeddings += 1

            # å¤„ç†æ¯ä¸ªåˆ†å—
            for chunk in dialog_data.chunks:
                # èµ‹å€¼åˆ†å—åµŒå…¥
                if chunk.id in chunk_embedding_map:
                    chunk.chunk_embedding = chunk_embedding_map[chunk.id]
                    assigned_chunk_embeddings += 1

                # å¤„ç†æ¯ä¸ªé™ˆè¿°å¥
                for statement in chunk.statements:
                    total_statements += 1

                    # èµ‹å€¼æ—¶é—´ä¿¡æ¯
                    if statement.id in temporal_map:
                        statement.temporal_validity = temporal_map[statement.id]
                        assigned_temporal += 1

                    # èµ‹å€¼ä¸‰å…ƒç»„
                    if statement.id in triplet_map:
                        statement.triplet_extraction_info = triplet_map[statement.id]
                        assigned_triplets += 1

                    # èµ‹å€¼é™ˆè¿°å¥åµŒå…¥
                    if statement.id in statement_embedding_map:
                        statement.statement_embedding = statement_embedding_map[statement.id]
                        assigned_statement_embeddings += 1

        logger.info(
            f"æ•°æ®èµ‹å€¼å®Œæˆ - æ€»é™ˆè¿°å¥: {total_statements}, "
            f"æ—¶é—´ä¿¡æ¯: {assigned_temporal}, ä¸‰å…ƒç»„: {assigned_triplets}, "
            f"é™ˆè¿°å¥åµŒå…¥: {assigned_statement_embeddings}, "
            f"åˆ†å—åµŒå…¥: {assigned_chunk_embeddings}, "
            f"å¯¹è¯åµŒå…¥: {assigned_dialog_embeddings}"
        )

        return dialog_data_list

    async def _create_nodes_and_edges(
        self, dialog_data_list: List[DialogData]
    ) -> Tuple[
        List[DialogueNode],
        List[ChunkNode],
        List[StatementNode],
        List[ExtractedEntityNode],
        List[StatementChunkEdge],
        List[StatementEntityEdge],
        List[EntityEntityEdge],
    ]:
        """
        åˆ›å»ºå›¾æ•°æ®åº“èŠ‚ç‚¹å’Œè¾¹

        å°†å¯¹è¯æ•°æ®è½¬æ¢ä¸ºå›¾æ•°æ®åº“çš„èŠ‚ç‚¹å’Œè¾¹ç»“æ„

        Args:
            dialog_data_list: å¯¹è¯æ•°æ®åˆ—è¡¨

        Returns:
            åŒ…å«æ‰€æœ‰èŠ‚ç‚¹å’Œè¾¹çš„å…ƒç»„
        """
        logger.info("å¼€å§‹åˆ›å»ºèŠ‚ç‚¹å’Œè¾¹")
        
        # æ³¨æ„ï¼šå¼€å§‹æ¶ˆæ¯å·²åœ¨ run æ–¹æ³•ä¸­å‘é€ï¼Œè¿™é‡Œä¸å†é‡å¤å‘é€

        dialogue_nodes = []
        chunk_nodes = []
        statement_nodes = []
        entity_nodes = []
        statement_chunk_edges = []
        statement_entity_edges = []
        entity_entity_edges = []

        # ç”¨äºå»é‡çš„é›†åˆ
        entity_id_set = set()
        
        # ç”¨äºè·Ÿè¸ªè¿›åº¦
        total_dialogs = len(dialog_data_list)
        processed_dialogs = 0

        for dialog_data in dialog_data_list:
            processed_dialogs += 1
            # åˆ›å»ºå¯¹è¯èŠ‚ç‚¹
            dialogue_node = DialogueNode(
                id=dialog_data.id,
                name=f"Dialog_{dialog_data.id}",  # æ·»åŠ å¿…éœ€çš„ name å­—æ®µ
                ref_id=dialog_data.ref_id,
                group_id=dialog_data.group_id,
                user_id=dialog_data.user_id,
                apply_id=dialog_data.apply_id,
                run_id=dialog_data.run_id,  # ä½¿ç”¨ dialog_data çš„ run_id
                content=dialog_data.context.content if dialog_data.context else "",
                dialog_embedding=dialog_data.dialog_embedding if hasattr(dialog_data, 'dialog_embedding') else None,
                created_at=dialog_data.created_at,
                expired_at=dialog_data.expired_at,
                metadata=dialog_data.metadata,
                config_id=dialog_data.config_id if hasattr(dialog_data, 'config_id') else None,
            )
            dialogue_nodes.append(dialogue_node)

            # å¤„ç†æ¯ä¸ªåˆ†å—
            for chunk_idx, chunk in enumerate(dialog_data.chunks):
                # åˆ›å»ºåˆ†å—èŠ‚ç‚¹
                chunk_node = ChunkNode(
                    id=chunk.id,
                    name=f"Chunk_{chunk.id}",  # æ·»åŠ å¿…éœ€çš„ name å­—æ®µ
                    dialog_id=dialog_data.id,
                    group_id=dialog_data.group_id,
                    user_id=dialog_data.user_id,
                    apply_id=dialog_data.apply_id,
                    run_id=dialog_data.run_id,  # ä½¿ç”¨ dialog_data çš„ run_id
                    content=chunk.content,
                    chunk_embedding=chunk.chunk_embedding,
                    sequence_number=chunk_idx,  # æ·»åŠ å¿…éœ€çš„ sequence_number å­—æ®µ
                    created_at=dialog_data.created_at,
                    expired_at=dialog_data.expired_at,
                    metadata=chunk.metadata,
                )
                chunk_nodes.append(chunk_node)

                # å¤„ç†æ¯ä¸ªé™ˆè¿°å¥
                for statement in chunk.statements:
                    # åˆ›å»ºé™ˆè¿°å¥èŠ‚ç‚¹
                    statement_node = StatementNode(
                        id=statement.id,
                        name=f"Statement_{statement.id}",  # æ·»åŠ å¿…éœ€çš„ name å­—æ®µ
                        chunk_id=chunk.id,
                        stmt_type=getattr(statement, 'stmt_type', 'general'),  # æ·»åŠ å¿…éœ€çš„ stmt_type å­—æ®µ
                        temporal_info=getattr(statement, 'temporal_info', TemporalInfo.ATEMPORAL),  # æ·»åŠ å¿…éœ€çš„ temporal_info å­—æ®µ
                        connect_strength=statement.connect_strength if statement.connect_strength is not None else 'Strong',  # æ·»åŠ å¿…éœ€çš„ connect_strength å­—æ®µ
                        group_id=dialog_data.group_id,
                        user_id=dialog_data.user_id,
                        apply_id=dialog_data.apply_id,
                        run_id=dialog_data.run_id,  # ä½¿ç”¨ dialog_data çš„ run_id
                        statement=statement.statement,
                        statement_embedding=statement.statement_embedding,
                        valid_at=statement.temporal_validity.valid_at if hasattr(statement, 'temporal_validity') and statement.temporal_validity else None,
                        invalid_at=statement.temporal_validity.invalid_at if hasattr(statement, 'temporal_validity') and statement.temporal_validity else None,
                        created_at=dialog_data.created_at,
                        expired_at=dialog_data.expired_at,
                        config_id=dialog_data.config_id if hasattr(dialog_data, 'config_id') else None,
                    )
                    statement_nodes.append(statement_node)

                    # åˆ›å»ºé™ˆè¿°å¥-åˆ†å—è¾¹
                    statement_chunk_edge = StatementChunkEdge(
                        source=statement.id,
                        target=chunk.id,
                        group_id=dialog_data.group_id,
                        user_id=dialog_data.user_id,
                        apply_id=dialog_data.apply_id,
                        run_id=dialog_data.run_id,  # ä½¿ç”¨ dialog_data çš„ run_id
                        created_at=dialog_data.created_at,
                    )
                    statement_chunk_edges.append(statement_chunk_edge)

                    # å¤„ç†ä¸‰å…ƒç»„ä¿¡æ¯
                    if statement.triplet_extraction_info:
                        triplet_info = statement.triplet_extraction_info

                        # åˆ›å»ºå®ä½“ç´¢å¼•åˆ°IDçš„æ˜ å°„
                        entity_idx_to_id = {}
                        
                        # åˆ›å»ºå®ä½“èŠ‚ç‚¹
                        for entity_idx, entity in enumerate(triplet_info.entities):
                            # æ˜ å°„å®ä½“ç´¢å¼•åˆ°å®ä½“ID
                            entity_idx_to_id[entity.entity_idx] = entity.id
                            
                            if entity.id not in entity_id_set:
                                entity_connect_strength = getattr(entity, 'connect_strength', 'Strong')
                                entity_node = ExtractedEntityNode(
                                    id=entity.id,
                                    name=getattr(entity, 'name', f"Entity_{entity.id}"),  # ä½¿ç”¨ name è€Œä¸æ˜¯ entity_name
                                    entity_idx=entity.entity_idx,  # ä½¿ç”¨å®ä½“è‡ªå·±çš„ entity_idx
                                    statement_id=statement.id,  # æ·»åŠ å¿…éœ€çš„ statement_id å­—æ®µ
                                    entity_type=getattr(entity, 'type', 'unknown'),  # ä½¿ç”¨ type è€Œä¸æ˜¯ entity_type
                                    description=getattr(entity, 'description', ''),  # æ·»åŠ å¿…éœ€çš„ description å­—æ®µ
                                    fact_summary=getattr(entity, 'fact_summary', ''),  # æ·»åŠ å¿…éœ€çš„ fact_summary å­—æ®µ
                                    connect_strength=entity_connect_strength if entity_connect_strength is not None else 'Strong',  # æ·»åŠ å¿…éœ€çš„ connect_strength å­—æ®µ
                                    aliases=getattr(entity, 'aliases', []) or [],  # ä¼ é€’ä»ä¸‰å…ƒç»„æå–é˜¶æ®µè·å–çš„aliases
                                    name_embedding=getattr(entity, 'name_embedding', None),
                                    group_id=dialog_data.group_id,
                                    user_id=dialog_data.user_id,
                                    apply_id=dialog_data.apply_id,
                                    run_id=dialog_data.run_id,  # ä½¿ç”¨ dialog_data çš„ run_id
                                    created_at=dialog_data.created_at,
                                    expired_at=dialog_data.expired_at,
                                    config_id=dialog_data.config_id if hasattr(dialog_data, 'config_id') else None,
                                )
                                entity_nodes.append(entity_node)
                                entity_id_set.add(entity.id)

                            # åˆ›å»ºé™ˆè¿°å¥-å®ä½“è¾¹
                            entity_connect_strength = getattr(entity, 'connect_strength', 'Strong')
                            statement_entity_edge = StatementEntityEdge(
                                source=statement.id,
                                target=entity.id,
                                connect_strength=entity_connect_strength if entity_connect_strength is not None else 'Strong',
                                group_id=dialog_data.group_id,
                                user_id=dialog_data.user_id,
                                apply_id=dialog_data.apply_id,
                                run_id=dialog_data.run_id,  # ä½¿ç”¨ dialog_data çš„ run_id
                                created_at=dialog_data.created_at,
                            )
                            statement_entity_edges.append(statement_entity_edge)

                        # åˆ›å»ºå®ä½“-å®ä½“è¾¹ï¼ˆä»ä¸‰å…ƒç»„ï¼‰
                        for triplet in triplet_info.triplets:
                            # å°†ä¸‰å…ƒç»„ä¸­çš„æ•´æ•°ç´¢å¼•æ˜ å°„åˆ°å®ä½“ID
                            subject_entity_id = entity_idx_to_id.get(triplet.subject_id)
                            object_entity_id = entity_idx_to_id.get(triplet.object_id)
                            
                            # åªæœ‰å½“ä¸¤ä¸ªå®ä½“IDéƒ½å­˜åœ¨æ—¶æ‰åˆ›å»ºè¾¹
                            if subject_entity_id and object_entity_id:
                                entity_entity_edge = EntityEntityEdge(
                                    source=subject_entity_id,
                                    target=object_entity_id,
                                    relation_type=triplet.predicate,
                                    statement=statement.statement,
                                    source_statement_id=statement.id,
                                    group_id=dialog_data.group_id,
                                    user_id=dialog_data.user_id,
                                    apply_id=dialog_data.apply_id,
                                    run_id=dialog_data.run_id,  # ä½¿ç”¨ dialog_data çš„ run_id
                                    created_at=dialog_data.created_at,
                                    expired_at=dialog_data.expired_at,
                                )
                                entity_entity_edges.append(entity_entity_edge)
                                
                                #  æµå¼è¾“å‡ºï¼šæ¯åˆ›å»ºä¸€ä¸ªå…³ç³»è¾¹ï¼Œç«‹å³å‘é€è¿›åº¦ï¼ˆé™åˆ¶å‘é€æ•°é‡ï¼‰
                                if self.progress_callback and len(entity_entity_edges) <= 10:
                                    # è·å–å®ä½“åç§°
                                    source_name = triplet.subject_name
                                    target_name = triplet.object_name
                                    relationship_result = {
                                        "result_type": "relationship_creation",
                                        "relationship_index": len(entity_entity_edges),
                                        "source_entity": source_name,
                                        "relation_type": triplet.predicate,
                                        "target_entity": target_name,
                                        "relationship_text": f"{source_name} -[{triplet.predicate}]-> {target_name}",
                                        "dialog_progress": f"{processed_dialogs}/{total_dialogs}"
                                    }
                                    await self.progress_callback(
                                        "creating_nodes_edges_result", 
                                        f"å…³ç³»åˆ›å»ºä¸­ ({processed_dialogs}/{total_dialogs})", 
                                        relationship_result
                                    )
                            else:
                                logger.warning(
                                    f"è·³è¿‡ä¸‰å…ƒç»„ - æ— æ³•æ‰¾åˆ°å®ä½“ID: subject_id={triplet.subject_id}, "
                                    f"object_id={triplet.object_id}, statement_id={statement.id}"
                                )

        logger.info(
            f"èŠ‚ç‚¹å’Œè¾¹åˆ›å»ºå®Œæˆ - å¯¹è¯èŠ‚ç‚¹: {len(dialogue_nodes)}, "
            f"åˆ†å—èŠ‚ç‚¹: {len(chunk_nodes)}, é™ˆè¿°å¥èŠ‚ç‚¹: {len(statement_nodes)}, "
            f"å®ä½“èŠ‚ç‚¹: {len(entity_nodes)}, é™ˆè¿°å¥-åˆ†å—è¾¹: {len(statement_chunk_edges)}, "
            f"é™ˆè¿°å¥-å®ä½“è¾¹: {len(statement_entity_edges)}, "
            f"å®ä½“-å®ä½“è¾¹: {len(entity_entity_edges)}"
        )
        
        # è¿›åº¦å›è°ƒï¼šåˆ›å»ºèŠ‚ç‚¹å’Œè¾¹å®Œæˆï¼Œä¼ é€’ç»“æœç»Ÿè®¡
        # æ³¨æ„ï¼šå…·ä½“çš„å…³ç³»åˆ›å»ºç»“æœå·²ç»åœ¨åˆ›å»ºè¿‡ç¨‹ä¸­å®æ—¶å‘é€äº†
        if self.progress_callback:
            nodes_edges_stats = {
                "dialogue_nodes_count": len(dialogue_nodes),
                "chunk_nodes_count": len(chunk_nodes),
                "statement_nodes_count": len(statement_nodes),
                "entity_nodes_count": len(entity_nodes),
                "statement_chunk_edges_count": len(statement_chunk_edges),
                "statement_entity_edges_count": len(statement_entity_edges),
                "entity_entity_edges_count": len(entity_entity_edges),
            }
            await self.progress_callback("creating_nodes_edges_complete", "åˆ›å»ºèŠ‚ç‚¹å’Œè¾¹å®Œæˆ", nodes_edges_stats)

        return (
            dialogue_nodes,
            chunk_nodes,
            statement_nodes,
            entity_nodes,
            statement_chunk_edges,
            statement_entity_edges,
            entity_entity_edges,
        )

    async def _run_dedup_and_write_summary(
        self,
        dialogue_nodes: List[DialogueNode],
        chunk_nodes: List[ChunkNode],
        statement_nodes: List[StatementNode],
        entity_nodes: List[ExtractedEntityNode],
        statement_chunk_edges: List[StatementChunkEdge],
        statement_entity_edges: List[StatementEntityEdge],
        entity_entity_edges: List[EntityEntityEdge],
        dialog_data_list: List[DialogData],
    ) -> Tuple[
        Tuple[List[DialogueNode], List[ChunkNode], List[StatementNode]],
        Tuple[List[ExtractedEntityNode], List[StatementEntityEdge], List[EntityEntityEdge]],
        Tuple[List[ExtractedEntityNode], List[StatementEntityEdge], List[EntityEntityEdge]],
    ]:
        """
        æ‰§è¡Œä¸¤é˜¶æ®µå»é‡å¹¶å†™å…¥æ±‡æ€»

        Args:
            dialogue_nodes: å¯¹è¯èŠ‚ç‚¹åˆ—è¡¨
            chunk_nodes: åˆ†å—èŠ‚ç‚¹åˆ—è¡¨
            statement_nodes: é™ˆè¿°å¥èŠ‚ç‚¹åˆ—è¡¨
            entity_nodes: å®ä½“èŠ‚ç‚¹åˆ—è¡¨
            statement_chunk_edges: é™ˆè¿°å¥-åˆ†å—è¾¹åˆ—è¡¨
            statement_entity_edges: é™ˆè¿°å¥-å®ä½“è¾¹åˆ—è¡¨
            entity_entity_edges: å®ä½“-å®ä½“è¾¹åˆ—è¡¨
            dialog_data_list: å¯¹è¯æ•°æ®åˆ—è¡¨

        Returns:
            åŒ…å«ä¸‰ä¸ªå…ƒç»„çš„å…ƒç»„ï¼š
            - ç¬¬ä¸€ä¸ªå…ƒç»„ï¼š(å¯¹è¯èŠ‚ç‚¹åˆ—è¡¨, åˆ†å—èŠ‚ç‚¹åˆ—è¡¨, é™ˆè¿°å¥èŠ‚ç‚¹åˆ—è¡¨)
            - ç¬¬äºŒä¸ªå…ƒç»„ï¼šå»é‡å‰çš„ (å®ä½“èŠ‚ç‚¹åˆ—è¡¨, é™ˆè¿°å¥-å®ä½“è¾¹åˆ—è¡¨, å®ä½“-å®ä½“è¾¹åˆ—è¡¨)
            - ç¬¬ä¸‰ä¸ªå…ƒç»„ï¼šå»é‡åçš„ (å®ä½“èŠ‚ç‚¹åˆ—è¡¨, é™ˆè¿°å¥-å®ä½“è¾¹åˆ—è¡¨, å®ä½“-å®ä½“è¾¹åˆ—è¡¨)
        """
        logger.info("å¼€å§‹ä¸¤é˜¶æ®µå®ä½“å»é‡å’Œæ¶ˆæ­§")
        
        # è¿›åº¦å›è°ƒï¼šå‘é€å»é‡æ¶ˆæ­§å¼€å§‹æ¶ˆæ¯
        if self.progress_callback:
            await self.progress_callback("deduplication", "æ­£åœ¨å»é‡æ¶ˆæ­§...")
        
        logger.info(
            f"å»é‡å‰: {len(entity_nodes)} ä¸ªå®ä½“èŠ‚ç‚¹, "
            f"{len(statement_entity_edges)} æ¡é™ˆè¿°å¥-å®ä½“è¾¹, "
            f"{len(entity_entity_edges)} æ¡å®ä½“-å®ä½“è¾¹"
        )

        try:
            # åœ¨è¯•è¿è¡Œæ¨¡å¼ä¸‹ï¼Œè·³è¿‡ç¬¬äºŒå±‚å»é‡ï¼ˆä¸æŸ¥è¯¢æ•°æ®åº“ï¼‰
            if self.is_pilot_run:
                logger.info("è¯•è¿è¡Œæ¨¡å¼ï¼šä»…æ‰§è¡Œç¬¬ä¸€å±‚å»é‡ï¼Œè·³è¿‡ç¬¬äºŒå±‚æ•°æ®åº“å»é‡")
                # åªæ‰§è¡Œç¬¬ä¸€å±‚å»é‡
                from app.core.memory.storage_services.extraction_engine.deduplication.deduped_and_disamb import deduplicate_entities_and_edges
                
                dedup_entity_nodes, dedup_statement_entity_edges, dedup_entity_entity_edges, dedup_details = await deduplicate_entities_and_edges(
                    entity_nodes,
                    statement_entity_edges,
                    entity_entity_edges,
                    report_stage="ç¬¬ä¸€å±‚å»é‡æ¶ˆæ­§ï¼ˆè¯•è¿è¡Œï¼‰",
                    report_append=False,
                    dedup_config=self.config.deduplication,
                )
                
                # ä¿å­˜å»é‡æ¶ˆæ­§çš„è¯¦ç»†è®°å½•åˆ°å®ä¾‹å˜é‡
                self._save_dedup_details(dedup_details, entity_nodes, dedup_entity_nodes)
                
                result_tuple = (
                    dialogue_nodes,
                    chunk_nodes,
                    statement_nodes,
                    dedup_entity_nodes,
                    statement_chunk_edges,
                    dedup_statement_entity_edges,
                    dedup_entity_entity_edges,
                )
                
                final_entity_nodes = dedup_entity_nodes
                final_statement_entity_edges = dedup_statement_entity_edges
                final_entity_entity_edges = dedup_entity_entity_edges
            else:
                # æ­£å¼æ¨¡å¼ï¼šæ‰§è¡Œå®Œæ•´çš„ä¸¤é˜¶æ®µå»é‡
                result_tuple = await dedup_layers_and_merge_and_return(
                    dialogue_nodes,
                    chunk_nodes,
                    statement_nodes,
                    entity_nodes,
                    statement_chunk_edges,
                    statement_entity_edges,
                    entity_entity_edges,
                    dialog_data_list,
                    self.config,
                    self.connector,
                )

                # è§£åŒ…è¿”å›å€¼
                (
                    _,
                    _,
                    _,
                    final_entity_nodes,
                    _,
                    final_statement_entity_edges,
                    final_entity_entity_edges,
                    dedup_details,
                ) = result_tuple
                
                # ä¿å­˜å»é‡æ¶ˆæ­§çš„è¯¦ç»†è®°å½•åˆ°å®ä¾‹å˜é‡
                self._save_dedup_details(dedup_details, entity_nodes, final_entity_nodes)

            logger.info(
                f"å»é‡å: {len(final_entity_nodes)} ä¸ªå®ä½“èŠ‚ç‚¹, "
                f"{len(final_statement_entity_edges)} æ¡é™ˆè¿°å¥-å®ä½“è¾¹, "
                f"{len(final_entity_entity_edges)} æ¡å®ä½“-å®ä½“è¾¹"
            )
            logger.info(
                f"å»é‡æ•ˆæœ: å®ä½“å‡å°‘ {len(entity_nodes) - len(final_entity_nodes)}, "
                f"é™ˆè¿°å¥-å®ä½“è¾¹å‡å°‘ {len(statement_entity_edges) - len(final_statement_entity_edges)}, "
                f"å®ä½“-å®ä½“è¾¹å‡å°‘ {len(entity_entity_edges) - len(final_entity_entity_edges)}"
            )
            
            #  æµå¼è¾“å‡ºï¼šå®æ—¶è¾“å‡ºå»é‡æ¶ˆæ­§çš„å…·ä½“ç»“æœ
            if self.progress_callback:
                # åˆ†æå®ä½“åˆå¹¶æƒ…å†µï¼ˆä½¿ç”¨å†…å­˜ä¸­çš„è®°å½•ï¼‰
                merge_info = await self._analyze_entity_merges(entity_nodes, final_entity_nodes)
                
                # é€ä¸ªè¾“å‡ºå»é‡åˆå¹¶çš„å®ä½“ç¤ºä¾‹
                for i, merge_detail in enumerate(merge_info[:5]):  # è¾“å‡ºå‰5ä¸ªå»é‡ç»“æœ
                    dedup_result = {
                        "result_type": "entity_merge",
                        "merged_entity_name": merge_detail["main_entity_name"],
                        "merged_count": merge_detail["merged_count"],
                        "merge_progress": f"{i + 1}/{min(len(merge_info), 5)}",
                        "message": f"{merge_detail['main_entity_name']}åˆå¹¶{merge_detail['merged_count']}ä¸ªï¼šç›¸ä¼¼å®ä½“å·²åˆå¹¶"
                    }
                    await self.progress_callback("dedup_disambiguation_result", "å®ä½“å»é‡ä¸­", dedup_result)
                
                # åˆ†æå®ä½“æ¶ˆæ­§æƒ…å†µï¼ˆä½¿ç”¨å†…å­˜ä¸­çš„è®°å½•ï¼‰
                disamb_info = await self._analyze_entity_disambiguation(entity_nodes, final_entity_nodes)
                
                # é€ä¸ªè¾“å‡ºå®ä½“æ¶ˆæ­§çš„ç»“æœ
                for i, disamb_detail in enumerate(disamb_info[:5]):  # è¾“å‡ºå‰5ä¸ªæ¶ˆæ­§ç»“æœ
                    disamb_result = {
                        "result_type": "entity_disambiguation",
                        "disambiguated_entity_name": disamb_detail["entity_name"],
                        "disambiguation_type": disamb_detail["disamb_type"],
                        "confidence": disamb_detail.get("confidence", "unknown"),
                        "reason": disamb_detail.get("reason", ""),
                        "disamb_progress": f"{i + 1}/{min(len(disamb_info), 5)}",
                        "message": f"{disamb_detail['entity_name']}æ¶ˆæ­§å®Œæˆï¼š{disamb_detail['disamb_type']}"
                    }
                    await self.progress_callback("dedup_disambiguation_result", "å®ä½“æ¶ˆæ­§ä¸­", disamb_result)
                
                # è¿›åº¦å›è°ƒï¼šå»é‡æ¶ˆæ­§å®Œæˆï¼Œä¼ é€’å»é‡å’Œæ¶ˆæ­§çš„å…·ä½“æ•ˆæœ
                await self._send_dedup_progress_callback(
                    len(entity_nodes), len(final_entity_nodes),
                    len(statement_entity_edges), len(final_statement_entity_edges),
                    len(entity_entity_edges), len(final_entity_entity_edges)
                )
 

            # å†™å…¥æå–ç»“æœæ±‡æ€»ï¼ˆè¯•è¿è¡Œå’Œæ­£å¼æ¨¡å¼éƒ½éœ€è¦ç”Ÿæˆï¼‰
            try:
                from app.core.config import settings
                settings.ensure_memory_output_dir()
                _write_extracted_result_summary(
                    chunk_nodes=chunk_nodes,
                    pipeline_output_dir=settings.MEMORY_OUTPUT_DIR,
                )
                mode_str = "è¯•è¿è¡Œ" if self.is_pilot_run else "æ­£å¼"
                logger.info(f"æå–ç»“æœæ±‡æ€»å·²å†™å…¥ï¼ˆ{mode_str}æ¨¡å¼ï¼‰")
            except Exception as e:
                logger.warning(f"å†™å…¥æå–ç»“æœæ±‡æ€»å¤±è´¥: {e}")

            return result_tuple

        except Exception as e:
            logger.error(f"ä¸¤é˜¶æ®µå»é‡å¤±è´¥: {e}", exc_info=True)
            raise

    def _save_dedup_details(
        self,
        dedup_details: Dict[str, Any],
        original_entities: List[ExtractedEntityNode],
        final_entities: List[ExtractedEntityNode]
    ):
        """
        ä¿å­˜å»é‡æ¶ˆæ­§çš„è¯¦ç»†è®°å½•åˆ°å®ä¾‹å˜é‡ï¼ˆåŸºäºå†…å­˜æ•°æ®ç»“æ„ï¼‰
        
        Args:
            dedup_details: å»é‡å‡½æ•°è¿”å›çš„è¯¦ç»†è®°å½•
            original_entities: å»é‡å‰çš„å®ä½“åˆ—è¡¨
            final_entities: å»é‡åçš„å®ä½“åˆ—è¡¨
        """
        try:
            # ä¿å­˜IDé‡å®šå‘æ˜ å°„
            self.id_redirect_map = dedup_details.get("id_redirect", {})
            
            # å¤„ç†ç²¾ç¡®åŒ¹é…çš„åˆå¹¶è®°å½•
            exact_merge_map = dedup_details.get("exact_merge_map", {})
            for key, info in exact_merge_map.items():
                merged_ids = info.get("merged_ids", set())
                if merged_ids:
                    self.dedup_merge_records.append({
                        "type": "ç²¾ç¡®åŒ¹é…",
                        "canonical_id": info.get("canonical_id"),
                        "entity_name": info.get("name"),
                        "entity_type": info.get("entity_type"),
                        "merged_count": len(merged_ids),
                        "merged_ids": list(merged_ids)
                    })
            
            # å¤„ç†æ¨¡ç³ŠåŒ¹é…çš„åˆå¹¶è®°å½•
            fuzzy_merge_records = dedup_details.get("fuzzy_merge_records", [])
            for record in fuzzy_merge_records:
                # è§£ææ¨¡ç³ŠåŒ¹é…è®°å½•å­—ç¬¦ä¸²
                # æ ¼å¼: "[æ¨¡ç³Š] è§„èŒƒå®ä½“ id (group|name|type) <- åˆå¹¶å®ä½“ id (group|name|type) | s_name=0.xxx, ..."
                try:
                    import re
                    match = re.search(r"è§„èŒƒå®ä½“ (\S+) \(([^|]+)\|([^|]+)\|([^)]+)\) <- åˆå¹¶å®ä½“ (\S+)", record)
                    if match:
                        self.dedup_merge_records.append({
                            "type": "æ¨¡ç³ŠåŒ¹é…",
                            "canonical_id": match.group(1),
                            "entity_name": match.group(3),
                            "entity_type": match.group(4),
                            "merged_count": 1,
                            "merged_ids": [match.group(5)]
                        })
                except Exception as e:
                    logger.debug(f"è§£ææ¨¡ç³ŠåŒ¹é…è®°å½•å¤±è´¥: {record}, é”™è¯¯: {e}")
            
            # å¤„ç†LLMå»é‡çš„åˆå¹¶è®°å½•
            llm_decision_records = dedup_details.get("llm_decision_records", [])
            for record in llm_decision_records:
                if "[LLMå»é‡]" in str(record):
                    try:
                        import re
                        # æ ¼å¼: "[LLMå»é‡] åŒåç±»å‹ç›¸ä¼¼ name1ï¼ˆtype1ï¼‰|name2ï¼ˆtype2ï¼‰ | conf=0.xx | reason=..."
                        match = re.search(r"åŒåç±»å‹ç›¸ä¼¼ ([^ï¼ˆ]+)ï¼ˆ([^ï¼‰]+)ï¼‰\|([^ï¼ˆ]+)ï¼ˆ([^ï¼‰]+)ï¼‰", record)
                        if match:
                            self.dedup_merge_records.append({
                                "type": "LLMå»é‡",
                                "entity_name": match.group(1),
                                "entity_type": f"{match.group(2)}|{match.group(4)}",
                                "merged_count": 1,
                                "merged_ids": []
                            })
                    except Exception as e:
                        logger.debug(f"è§£æLLMå»é‡è®°å½•å¤±è´¥: {record}, é”™è¯¯: {e}")
            
            # å¤„ç†æ¶ˆæ­§è®°å½•
            disamb_records = dedup_details.get("disamb_records", [])
            for record in disamb_records:
                if "[DISAMBé˜»æ–­]" in str(record):
                    try:
                        import re
                        # æ ¼å¼: "[DISAMBé˜»æ–­] name1ï¼ˆtype1ï¼‰|name2ï¼ˆtype2ï¼‰ | conf=0.xx | reason=..."
                        content = str(record).replace("[DISAMBé˜»æ–­]", "").strip()
                        match = re.search(r"([^ï¼ˆ]+)ï¼ˆ([^ï¼‰]+)ï¼‰\|([^ï¼ˆ]+)ï¼ˆ([^ï¼‰]+)ï¼‰", content)
                        if match:
                            entity1_name = match.group(1).strip()
                            entity1_type = match.group(2)
                            entity2_name = match.group(3).strip()
                            entity2_type = match.group(4)
                            
                            # æå–ç½®ä¿¡åº¦å’ŒåŸå› 
                            conf_match = re.search(r"conf=([0-9.]+)", str(record))
                            confidence = conf_match.group(1) if conf_match else "unknown"
                            
                            reason_match = re.search(r"reason=([^|]+)", str(record))
                            reason = reason_match.group(1).strip() if reason_match else ""
                            
                            self.dedup_disamb_records.append({
                                "entity_name": entity1_name,
                                "disamb_type": f"æ¶ˆæ­§é˜»æ–­ï¼š{entity1_type} vs {entity2_type}",
                                "confidence": confidence,
                                "reason": reason[:100] + "..." if len(reason) > 100 else reason
                            })
                    except Exception as e:
                        logger.debug(f"è§£ææ¶ˆæ­§è®°å½•å¤±è´¥: {record}, é”™è¯¯: {e}")
            
            logger.info(f"ä¿å­˜å»é‡æ¶ˆæ­§è®°å½•ï¼š{len(self.dedup_merge_records)} ä¸ªåˆå¹¶è®°å½•ï¼Œ{len(self.dedup_disamb_records)} ä¸ªæ¶ˆæ­§è®°å½•")
            
        except Exception as e:
            logger.error(f"ä¿å­˜å»é‡æ¶ˆæ­§è¯¦æƒ…å¤±è´¥: {e}", exc_info=True)

    async def _analyze_entity_merges(
        self,
        original_entities: List[ExtractedEntityNode],
        final_entities: List[ExtractedEntityNode]
    ) -> List[Dict[str, Any]]:
        """
        åˆ†æå®ä½“åˆå¹¶æƒ…å†µï¼Œç›´æ¥ä½¿ç”¨å†…å­˜ä¸­çš„åˆå¹¶è®°å½•ï¼ˆä¸å†è§£ææ—¥å¿—æ–‡ä»¶ï¼‰
        
        Args:
            original_entities: å»é‡å‰çš„å®ä½“åˆ—è¡¨
            final_entities: å»é‡åçš„å®ä½“åˆ—è¡¨
            
        Returns:
            åˆå¹¶è¯¦æƒ…åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ä¸»å®ä½“åç§°å’Œåˆå¹¶æ•°é‡
        """
        try:
            # ç›´æ¥ä½¿ç”¨ä¿å­˜çš„åˆå¹¶è®°å½•
            if self.dedup_merge_records:
                # æŒ‰åˆå¹¶æ•°é‡æ’åºï¼Œè¿”å›å‰å‡ ä¸ª
                sorted_records = sorted(
                    self.dedup_merge_records,
                    key=lambda x: x.get("merged_count", 0),
                    reverse=True
                )
                
                merge_info = []
                for record in sorted_records:
                    merge_info.append({
                        "main_entity_name": record.get("entity_name", "æœªçŸ¥å®ä½“"),
                        "merged_count": record.get("merged_count", 1)
                    })
                
                return merge_info
            
            # å¦‚æœæ²¡æœ‰ä¿å­˜çš„è®°å½•ï¼Œè¿”å›ç©ºåˆ—è¡¨
            logger.info("æœªæ‰¾åˆ°å®ä½“åˆå¹¶è®°å½•")
            return []
            
        except Exception as e:
            logger.error(f"åˆ†æå®ä½“åˆå¹¶æƒ…å†µå¤±è´¥: {e}", exc_info=True)
            return []

    async def _analyze_entity_disambiguation(
        self,
        original_entities: List[ExtractedEntityNode],
        final_entities: List[ExtractedEntityNode]
    ) -> List[Dict[str, Any]]:
        """
        åˆ†æå®ä½“æ¶ˆæ­§æƒ…å†µï¼Œç›´æ¥ä½¿ç”¨å†…å­˜ä¸­çš„æ¶ˆæ­§è®°å½•ï¼ˆä¸å†è§£ææ—¥å¿—æ–‡ä»¶ï¼‰
        
        Args:
            original_entities: å»é‡å‰çš„å®ä½“åˆ—è¡¨
            final_entities: å»é‡åçš„å®ä½“åˆ—è¡¨
            
        Returns:
            æ¶ˆæ­§è¯¦æƒ…åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«å®ä½“åç§°å’Œæ¶ˆæ­§ç±»å‹
        """
        try:
            # ç›´æ¥ä½¿ç”¨ä¿å­˜çš„æ¶ˆæ­§è®°å½•
            if self.dedup_disamb_records:
                return self.dedup_disamb_records
            
            # å¦‚æœæ²¡æœ‰ä¿å­˜çš„è®°å½•ï¼Œè¿”å›ç©ºåˆ—è¡¨
            logger.info("æœªæ‰¾åˆ°å®ä½“æ¶ˆæ­§è®°å½•")
            return []
            
        except Exception as e:
            logger.error(f"åˆ†æå®ä½“æ¶ˆæ­§æƒ…å†µå¤±è´¥: {e}", exc_info=True)
            return []

    def _get_entity_type_display_name(self, entity_type: str) -> str:
        """
        è·å–å®ä½“ç±»å‹çš„ä¸­æ–‡æ˜¾ç¤ºåç§°
        
        Args:
            entity_type: è‹±æ–‡å®ä½“ç±»å‹
            
        Returns:
            ä¸­æ–‡æ˜¾ç¤ºåç§°
        """
        type_mapping = {
            "Person": "äººç‰©å®ä½“èŠ‚ç‚¹",
            "Organization": "ç»„ç»‡å®ä½“èŠ‚ç‚¹", 
            "ORG": "ç»„ç»‡å®ä½“èŠ‚ç‚¹",
            "Location": "åœ°ç‚¹å®ä½“èŠ‚ç‚¹",
            "LOC": "åœ°ç‚¹å®ä½“èŠ‚ç‚¹",
            "Event": "äº‹ä»¶å®ä½“èŠ‚ç‚¹",
            "Concept": "æ¦‚å¿µå®ä½“èŠ‚ç‚¹",
            "Time": "æ—¶é—´å®ä½“èŠ‚ç‚¹",
            "Position": "èŒä½å®ä½“èŠ‚ç‚¹",
            "WorkRole": "èŒä¸šå®ä½“èŠ‚ç‚¹",
            "System": "ç³»ç»Ÿå®ä½“èŠ‚ç‚¹",
            "Policy": "æ”¿ç­–å®ä½“èŠ‚ç‚¹",
            "HistoricalPeriod": "å†å²æ—¶æœŸå®ä½“èŠ‚ç‚¹",
            "HistoricalState": "å†å²å›½å®¶å®ä½“èŠ‚ç‚¹",
            "HistoricalEvent": "å†å²äº‹ä»¶å®ä½“èŠ‚ç‚¹",
            "EconomicFactor": "ç»æµå› ç´ å®ä½“èŠ‚ç‚¹",
            "Condition": "æ¡ä»¶å®ä½“èŠ‚ç‚¹",
            "Numeric": "æ•°å€¼å®ä½“èŠ‚ç‚¹"
        }
        return type_mapping.get(entity_type, f"{entity_type}å®ä½“èŠ‚ç‚¹")

    async def _output_relationship_creation_results(
        self, 
        entity_entity_edges: List[EntityEntityEdge], 
        entity_nodes: List[ExtractedEntityNode]
    ):
        """
        è¾“å‡ºå…³ç³»åˆ›å»ºç»“æœ
        
        Args:
            entity_entity_edges: å®ä½“-å®ä½“è¾¹åˆ—è¡¨
            entity_nodes: å®ä½“èŠ‚ç‚¹åˆ—è¡¨
        """
        try:
            # åˆ›å»ºå®ä½“IDåˆ°åç§°çš„æ˜ å°„
            entity_id_to_name = {node.id: node.name for node in entity_nodes}
            
            # è¾“å‡ºå…³ç³»åˆ›å»ºç»“æœ
            for i, edge in enumerate(entity_entity_edges[:10]):  # åªè¾“å‡ºå‰10ä¸ªå…³ç³»
                source_name = entity_id_to_name.get(edge.source, f"Entity_{edge.source}")
                target_name = entity_id_to_name.get(edge.target, f"Entity_{edge.target}")
                relation_type = edge.relation_type
                
                relationship_result = {
                    "result_type": "relationship_creation",
                    "relationship_index": i + 1,
                    "source_entity": source_name,
                    "relation_type": relation_type,
                    "target_entity": target_name,
                    "relationship_text": f"{source_name} -[{relation_type}]-> {target_name}"
                }
                
                await self.progress_callback("creating_nodes_edges_result", "å…³ç³»åˆ›å»º", relationship_result)
                
        except Exception as e:
            logger.error(f"è¾“å‡ºå…³ç³»åˆ›å»ºç»“æœå¤±è´¥: {e}", exc_info=True)

    async def _send_dedup_progress_callback(
        self,
        original_entities: int,
        final_entities: int,
        original_stmt_edges: int,
        final_stmt_edges: int,
        original_ent_edges: int,
        final_ent_edges: int,
    ):
        """
        å‘é€å»é‡æ¶ˆæ­§å®Œæˆçš„è¿›åº¦å›è°ƒï¼Œä¼ é€’å…·ä½“çš„å»é‡å’Œæ¶ˆæ­§æ•ˆæœ
        
        Args:
            original_entities: å»é‡å‰å®ä½“æ•°é‡
            final_entities: å»é‡åå®ä½“æ•°é‡
            original_stmt_edges: å»é‡å‰é™ˆè¿°å¥-å®ä½“è¾¹æ•°é‡
            final_stmt_edges: å»é‡åé™ˆè¿°å¥-å®ä½“è¾¹æ•°é‡
            original_ent_edges: å»é‡å‰å®ä½“-å®ä½“è¾¹æ•°é‡
            final_ent_edges: å»é‡åå®ä½“-å®ä½“è¾¹æ•°é‡
        """
        try:
            # è§£æå»é‡æ¶ˆæ­§æŠ¥å‘Šæ–‡ä»¶ï¼Œè·å–å…·ä½“çš„å»é‡å’Œæ¶ˆæ­§æ•ˆæœ
            dedup_details = await self._parse_dedup_report()
            
            # è®¡ç®—å»é‡æ•ˆæœç»Ÿè®¡
            entities_reduced = original_entities - final_entities
            stmt_edges_reduced = original_stmt_edges - final_stmt_edges
            ent_edges_reduced = original_ent_edges - final_ent_edges
            
            # æ„å»ºè¿›åº¦å›è°ƒæ•°æ®
            dedup_stats = {
                "entities": {
                    "original_count": original_entities,
                    "final_count": final_entities,
                    "reduced_count": entities_reduced,
                    "reduction_rate": round(entities_reduced / original_entities * 100, 1) if original_entities > 0 else 0,
                },
                "statement_entity_edges": {
                    "original_count": original_stmt_edges,
                    "final_count": final_stmt_edges,
                    "reduced_count": stmt_edges_reduced,
                },
                "entity_entity_edges": {
                    "original_count": original_ent_edges,
                    "final_count": final_ent_edges,
                    "reduced_count": ent_edges_reduced,
                },
                "dedup_examples": dedup_details.get("dedup_examples", []),
                "disamb_examples": dedup_details.get("disamb_examples", []),
                "summary": {
                    "total_merges": dedup_details.get("total_merges", 0),
                    "total_disambiguations": dedup_details.get("total_disambiguations", 0),
                }
            }
            
            await self.progress_callback("dedup_disambiguation_complete", "å»é‡æ¶ˆæ­§å®Œæˆ", dedup_stats)
            
        except Exception as e:
            logger.error(f"å‘é€å»é‡æ¶ˆæ­§è¿›åº¦å›è°ƒå¤±è´¥: {e}", exc_info=True)
            # å³ä½¿è§£æå¤±è´¥ï¼Œä¹Ÿå‘é€åŸºæœ¬çš„ç»Ÿè®¡ä¿¡æ¯
            try:
                basic_stats = {
                    "entities": {
                        "original_count": original_entities,
                        "final_count": final_entities,
                        "reduced_count": original_entities - final_entities,
                    },
                    "summary": f"å®ä½“å»é‡åˆå¹¶{original_entities - final_entities}ä¸ª"
                }
                await self.progress_callback("dedup_disambiguation_complete", "å»é‡æ¶ˆæ­§å®Œæˆ", basic_stats)
            except Exception as e2:
                logger.error(f"å‘é€åŸºæœ¬å»é‡ç»Ÿè®¡å¤±è´¥: {e2}", exc_info=True)

    async def _parse_dedup_report(self) -> Dict[str, Any]:
        """
        è·å–å»é‡æ¶ˆæ­§æŠ¥å‘Šï¼Œç›´æ¥ä½¿ç”¨å†…å­˜ä¸­çš„è®°å½•ï¼ˆä¸å†è§£ææ—¥å¿—æ–‡ä»¶ï¼‰
        
        Returns:
            åŒ…å«å»é‡å’Œæ¶ˆæ­§è¯¦ç»†ä¿¡æ¯çš„å­—å…¸
        """
        try:
            # ç›´æ¥ä½¿ç”¨ä¿å­˜çš„è®°å½•æ„å»ºæŠ¥å‘Š
            dedup_examples = []
            disamb_examples = []
            total_merges = 0
            total_disambiguations = 0
            
            # å¤„ç†åˆå¹¶è®°å½•
            for record in self.dedup_merge_records:
                merge_count = record.get("merged_count", 0)
                total_merges += merge_count
                
                dedup_examples.append({
                    "type": record.get("type", "æœªçŸ¥"),
                    "entity_name": record.get("entity_name", "æœªçŸ¥å®ä½“"),
                    "entity_type": record.get("entity_type", "æœªçŸ¥ç±»å‹"),
                    "merge_count": merge_count,
                    "description": f"{record.get('entity_name', 'æœªçŸ¥å®ä½“')}å®ä½“å»é‡åˆå¹¶{merge_count}ä¸ª"
                })
            
            # å¤„ç†æ¶ˆæ­§è®°å½•
            for record in self.dedup_disamb_records:
                total_disambiguations += 1
                
                # ä»æ¶ˆæ­§ç±»å‹ä¸­æå–å®ä½“ç±»å‹ä¿¡æ¯
                disamb_type = record.get("disamb_type", "")
                entity_name = record.get("entity_name", "æœªçŸ¥å®ä½“")
                
                disamb_examples.append({
                    "entity1_name": entity_name,
                    "entity1_type": disamb_type.split("vs")[0].replace("æ¶ˆæ­§é˜»æ–­ï¼š", "").strip() if "vs" in disamb_type else "æœªçŸ¥",
                    "entity2_name": entity_name,
                    "entity2_type": disamb_type.split("vs")[1].strip() if "vs" in disamb_type else "æœªçŸ¥",
                    "description": f"{entity_name}ï¼Œæ¶ˆæ­§åŒºåˆ†æˆåŠŸ"
                })
            
            return {
                "dedup_examples": dedup_examples[:5],  # åªè¿”å›å‰5ä¸ªç¤ºä¾‹
                "disamb_examples": disamb_examples[:5],  # åªè¿”å›å‰5ä¸ªç¤ºä¾‹
                "total_merges": total_merges,
                "total_disambiguations": total_disambiguations,
            }
            
        except Exception as e:
            logger.error(f"è·å–å»é‡æŠ¥å‘Šå¤±è´¥: {e}", exc_info=True)
            return {"dedup_examples": [], "disamb_examples": [], "total_merges": 0, "total_disambiguations": 0}


# ============================================================================
# æ•°æ®åŠ è½½å’Œé¢„å¤„ç†å‡½æ•°
# ============================================================================
# ä»¥ä¸‹å‡½æ•°ä» extraction_pipeline.py è¿ç§»è€Œæ¥ï¼Œç”¨äºæ•°æ®åŠ è½½å’Œé¢„å¤„ç†


async def get_chunked_dialogs(
    chunker_strategy: str = "RecursiveChunker",
    group_id: str = "group_1",
    indices: Optional[List[int]] = None,
) -> List[DialogData]:
    """ä»æµ‹è¯•æ•°æ®ç”Ÿæˆåˆ†å—å¯¹è¯
    
    Args:
        chunker_strategy: åˆ†å—ç­–ç•¥ï¼ˆé»˜è®¤: RecursiveChunkerï¼‰
        group_id: ç»„ID
        indices: è¦å¤„ç†çš„æ•°æ®ç´¢å¼•åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        åŒ…å«åˆ†å—çš„ DialogData å¯¹è±¡åˆ—è¡¨
    """
    import json
    import re
    import os
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    testdata_path = os.path.join(os.path.dirname(__file__), "../../data", "testdata.json")
    with open(testdata_path, "r", encoding="utf-8") as f:
        test_data = [json.loads(line) for line in f]

    dialog_data_list = []

    if indices is not None:
        # é€‰æ‹©ç‰¹å®šç´¢å¼•
        selected_data = [test_data[i] for i in indices if 0 <= i < len(test_data)]
    else:
        # é»˜è®¤ä½¿ç”¨æ‰€æœ‰æ•°æ®
        selected_data = test_data
        
    for data in selected_data:
        # è§£æå¯¹è¯ä¸Šä¸‹æ–‡
        context_text = data["context"]

        # ä»contextæ–‡æœ¬ä¸­è§£ææ—¥æœŸ
        conv_date: Optional[str] = None
        m = re.search(r"(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥", context_text)
        if m:
            y, mo, d = int(m.group(1)), int(m.group(2)), int(m.group(3))
            conv_date = f"{y:04d}-{mo:02d}-{d:02d}"
        else:
            m = re.search(r"(\d{4})[-/](\d{1,2})[-/](\d{1,2})", context_text)
            if m:
                y, mo, d = int(m.group(1)), int(m.group(2)), int(m.group(3))
                conv_date = f"{y:04d}-{mo:02d}-{d:02d}"
                
        dialog_metadata: Dict[str, Any] = {}
        if conv_date:
            dialog_metadata["conversation_date"] = conv_date
            dialog_metadata["publication_date"] = conv_date

        # åˆ†å‰²å¯¹è¯ä¸ºæ¶ˆæ¯
        lines = context_text.split("\n")
        messages = []

        # è§£æå¯¹è¯è¡Œ
        for raw_line in lines:
            line = raw_line.strip()
            match = re.match(r'^[""]?(ç”¨æˆ·|AI)\s*[ï¼š:]\s*(.*)$', line)
            if match:
                role = match.group(1)
                msg = match.group(2).strip().rstrip('""')
                from app.core.memory.models.message_models import ConversationMessage
                messages.append(ConversationMessage(role=role, msg=msg))

        # åˆ›å»º DialogData
        from app.core.memory.models.message_models import ConversationContext
        conversation_context = ConversationContext(msgs=messages)
        dialog_data = DialogData(
            context=conversation_context,
            ref_id=data['id'],
            group_id=group_id,
            metadata=dialog_metadata,
        )
        
        # åˆ›å»ºåˆ†å—å™¨å¹¶å¤„ç†å¯¹è¯
        from app.core.memory.storage_services.extraction_engine.knowledge_extraction.chunk_extraction import DialogueChunker
        chunker = DialogueChunker(chunker_strategy)
        extracted_chunks = await chunker.process_dialogue(dialog_data)
        dialog_data.chunks = extracted_chunks

        dialog_data_list.append(dialog_data)

    # ä¿å­˜è¾“å‡º
    def serialize_datetime(obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        raise TypeError(
            f"Object of type {obj.__class__.__name__} is not JSON serializable"
        )

    combined_output = [dd.model_dump() for dd in dialog_data_list]
    from app.core.config import settings
    settings.ensure_memory_output_dir()
    output_path = settings.get_memory_output_path("chunker_test_output.txt")
    
    import json
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(
            combined_output, f, ensure_ascii=False, indent=4, default=serialize_datetime
        )

    return dialog_data_list


def preprocess_data(
    input_path: Optional[str] = None, 
    output_path: Optional[str] = None,
    skip_cleaning: bool = True,
    indices: Optional[List[int]] = None
) -> List[DialogData]:
    """æ•°æ®é¢„å¤„ç†
    
    Args:
        input_path: åŸå§‹æ•°æ®è·¯å¾„
        output_path: é¢„å¤„ç†åæ•°æ®ä¿å­˜è·¯å¾„
        skip_cleaning: æ˜¯å¦è·³è¿‡æ•°æ®æ¸…æ´—æ­¥éª¤ï¼ˆé»˜è®¤Falseï¼‰
        indices: è¦å¤„ç†çš„æ•°æ®ç´¢å¼•åˆ—è¡¨
        
    Returns:
        ç»è¿‡æ¸…æ´—è½¬æ¢åçš„ DialogData åˆ—è¡¨
    """
    print("\n=== æ•°æ®é¢„å¤„ç† ===")
    from app.core.memory.storage_services.extraction_engine.data_preprocessing.data_preprocessor import DataPreprocessor
    preprocessor = DataPreprocessor()
    try:
        cleaned_data = preprocessor.preprocess(input_path=input_path, output_path=output_path, skip_cleaning=skip_cleaning, indices=indices)
        print(f"æ•°æ®é¢„å¤„ç†å®Œæˆï¼å…±å¤„ç†äº† {len(cleaned_data)} æ¡å¯¹è¯æ•°æ®")
        return cleaned_data
    except Exception as e:
        print(f"æ•°æ®é¢„å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise


async def get_chunked_dialogs_from_preprocessed(
    data: List[DialogData],
    chunker_strategy: str = "RecursiveChunker",
    llm_client: Optional[Any] = None,
) -> List[DialogData]:
    """ä»é¢„å¤„ç†åçš„æ•°æ®ä¸­ç”Ÿæˆåˆ†å—
    
    Args:
        data: é¢„å¤„ç†åçš„ DialogData åˆ—è¡¨
        chunker_strategy: åˆ†å—ç­–ç•¥
        llm_client: LLM å®¢æˆ·ç«¯ï¼ˆç”¨äº LLMChunkerï¼‰
        
    Returns:
        å¸¦ chunks çš„ DialogData åˆ—è¡¨
    """
    print(f"\n=== æ‰¹é‡å¯¹è¯åˆ†å—å¤„ç† (ä½¿ç”¨ {chunker_strategy}) ===")
    if not data:
        raise ValueError("é¢„å¤„ç†æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†å—")
        
    all_chunked_dialogs: List[DialogData] = []
    from app.core.memory.storage_services.extraction_engine.knowledge_extraction.chunk_extraction import DialogueChunker
    
    for dialog_data in data:
        chunker = DialogueChunker(chunker_strategy, llm_client=llm_client)
        chunks = await chunker.process_dialogue(dialog_data)
        dialog_data.chunks = chunks
        all_chunked_dialogs.append(dialog_data)
        
    return all_chunked_dialogs


async def get_chunked_dialogs_with_preprocessing(
    chunker_strategy: str = "RecursiveChunker",
    group_id: str = "default",
    user_id: str = "default",
    apply_id: str = "default",
    indices: Optional[List[int]] = None,
    input_data_path: Optional[str] = None,
    llm_client: Optional[Any] = None,
    skip_cleaning: bool = True,
) -> List[DialogData]:
    """åŒ…å«æ•°æ®é¢„å¤„ç†æ­¥éª¤çš„å®Œæ•´åˆ†å—æµç¨‹
    
    Args:
        chunker_strategy: åˆ†å—ç­–ç•¥
        group_id: ç»„ID
        user_id: ç”¨æˆ·ID
        apply_id: åº”ç”¨ID
        indices: è¦å¤„ç†çš„æ•°æ®ç´¢å¼•åˆ—è¡¨
        input_data_path: è¾“å…¥æ•°æ®è·¯å¾„
        llm_client: LLM å®¢æˆ·ç«¯
        skip_cleaning: æ˜¯å¦è·³è¿‡æ•°æ®æ¸…æ´—æ­¥éª¤ï¼ˆé»˜è®¤Falseï¼‰
        
    Returns:
        å¸¦ chunks çš„ DialogData åˆ—è¡¨
    """
    import os
    print("\n=== å®Œæ•´æ•°æ®å¤„ç†æµç¨‹ï¼ˆåŒ…å«é¢„å¤„ç†ï¼‰===")

    if input_data_path is None:
        input_data_path = os.path.join(
            os.path.dirname(__file__), "../../data", "testdata.json"
        )
        
    # æ­¥éª¤1: æ•°æ®é¢„å¤„ç†ï¼ˆåŒ…å«ç´¢å¼•ç­›é€‰ï¼‰
    from app.core.config import settings
    settings.ensure_memory_output_dir()
    preprocessed_data = preprocess_data(
        input_path=input_data_path,
        output_path=settings.get_memory_output_path("preprocessed_data.json"),
        skip_cleaning=skip_cleaning,
        indices=indices,
    )
            
    # è®¾ç½® group_id, user_id, apply_id
    for dd in preprocessed_data:
        dd.group_id = group_id
        dd.user_id = user_id
        dd.apply_id = apply_id
        
    # æ­¥éª¤2: è¯­ä¹‰å‰ªæ
    try:
        from app.core.memory.storage_services.extraction_engine.data_preprocessing.data_pruning import SemanticPruner
        pruner = SemanticPruner(llm_client=llm_client)
        
        # è®°å½•å•å¯¹è¯åœºæ™¯ä¸‹å‰ªæå‰çš„æ¶ˆæ¯æ•°é‡
        single_dialog_original_msgs = None
        if len(preprocessed_data) == 1 and preprocessed_data[0].context:
            single_dialog_original_msgs = len(preprocessed_data[0].context.msgs)

        preprocessed_data = await pruner.prune_dataset(preprocessed_data)
        
        # å•å¯¹è¯ï¼šæ‰“å°æ¸…æ´—ä¸å‰ªæä¿¡æ¯
        if len(preprocessed_data) == 1 and single_dialog_original_msgs is not None:
            remaining_msgs = len(preprocessed_data[0].context.msgs) if preprocessed_data[0].context else 0
            deleted_msgs = max(0, single_dialog_original_msgs - remaining_msgs)
            print(
                f"è¯­ä¹‰å‰ªæå®Œæˆï¼å‰©ä½™ 1 æ¡å¯¹è¯ï¼åŸå§‹æ¶ˆæ¯æ•°ï¼š{single_dialog_original_msgs}ï¼Œ"
                f"ä¿ç•™æ¶ˆæ¯æ•°ï¼š{remaining_msgs}ï¼Œåˆ é™¤ {deleted_msgs} æ¡ã€‚"
            )
        else:
            print(f"è¯­ä¹‰å‰ªæå®Œæˆï¼å‰©ä½™ {len(preprocessed_data)} æ¡å¯¹è¯")
            
        # ä¿å­˜å‰ªæåçš„æ•°æ®
        try:
            from app.core.memory.storage_services.extraction_engine.data_preprocessing.data_preprocessor import DataPreprocessor
            pruned_output_path = settings.get_memory_output_path("pruned_data.json")
            dp = DataPreprocessor(output_file_path=pruned_output_path)
            dp.save_data(preprocessed_data, output_path=pruned_output_path)
        except Exception as se:
            print(f"ä¿å­˜å‰ªæç»“æœå¤±è´¥ï¼š{se}")
    except Exception as e:
        print(f"è¯­ä¹‰å‰ªæè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè·³è¿‡å‰ªæ: {e}")
        
    # æ­¥éª¤3: å¯¹è¯åˆ†å—
    return await get_chunked_dialogs_from_preprocessed(
        preprocessed_data,
        chunker_strategy=chunker_strategy,
        llm_client=llm_client,
    )
